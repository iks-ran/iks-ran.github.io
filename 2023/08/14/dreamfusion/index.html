

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/pageicon.png">
  <link rel="icon" href="/img/pageicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="iks-ran">
  <meta name="keywords" content="">
  
    <meta name="description" content="DreamFusionMotivation Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data. 3D assetsare currently designed by hand in model">
<meta property="og:type" content="article">
<meta property="og:title" content="DreamFusion">
<meta property="og:url" content="https://iks-ran.github.io/2023/08/14/dreamfusion/index.html">
<meta property="og:site_name" content="iks-ran">
<meta property="og:description" content="DreamFusionMotivation Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data. 3D assetsare currently designed by hand in model">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://iks-ran.github.io/img/blogs/230814-DreamFusion/1.png">
<meta property="article:published_time" content="2023-08-14T13:50:45.311Z">
<meta property="article:modified_time" content="2023-09-19T05:19:55.485Z">
<meta property="article:author" content="iks-ran">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="AIGC">
<meta property="article:tag" content="Computer Vision">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://iks-ran.github.io/img/blogs/230814-DreamFusion/1.png">
  
  
  
  <title>DreamFusion - iks-ran</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"iks-ran.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>iks-ran</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/blogs/230814-DreamFusion/1.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="DreamFusion"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-08-14 21:50" pubdate>
          August 14, 2023 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.6k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          64 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">DreamFusion</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="DreamFusion"><a href="#DreamFusion" class="headerlink" title="DreamFusion"></a>DreamFusion</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul>
<li><p>Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data. 3D assets<br>are currently designed by hand in modeling software like Blender and Maya3D, a process requiring a great deal of time and expertise. Text-to-3D generative models could lower the barrier to entry for novices and improve the workflow of experienced artists.</p>
</li>
<li><p>GANs can learn controllable 3D generators from photographs of a single object category, by placing an adversarial loss on 2D image renderings of the output 3D object or scene . Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text.</p>
</li>
<li><p>Dream Fields , which uses frozen image-text<br>joint embedding models from CLIP and an optimization-based approach to train NeRFs, showed that pretrained 2D image-text models may be used for 3D synthesis, though 3D objects produced by this approach tend to lack realism and accuracy.</p>
</li>
</ul>
<h3 id="Differential-Image-Parameterization"><a href="#Differential-Image-Parameterization" class="headerlink" title="Differential Image Parameterization"></a>Differential Image Parameterization</h3><p>Diffusion models trained on pixels have traditionally been used to sample only pixels. Dreamfusion wants to create 3D models that look like good images when rendered from random anglesare instead of being interested in sampling pixels.</p>
<p>Such models can be specified as a differentiable image parameterization (DIP), where a differentiable generator $g$ transforms parameters $\theta$ to create an image $\bf{x} &#x3D; g(\theta)$. DIPs allow us to express constraints, optimize in more compact spaces (e.g. arbitrary resolution coordinate-based MLPs), or leverage more powerful optimization algorithms for traversing pixel space. For 3D, $\theta$ can be parameters of a 3D volume and $g$ can be a volumetric renderer.To learn these parameters, a loss function that can be applied to diffusion models is used.</p>
<h3 id="Score-Distillation-Sampling"><a href="#Score-Distillation-Sampling" class="headerlink" title="Score Distillation Sampling"></a>Score Distillation Sampling</h3><figure>
    <img src="/img/blogs/230814-DreamFusion/2.png" srcset="/img/loading.gif" lazyload>
    <figcaption>Fig.1 DDPM and Score Distillation Sampling</figcaption>
</figure>

<p>DreamFusion leverages the structure of diffusion models to enable tractable sampling via optimization —— a loss function that, when minimized, yields a sample. DreamFusion optimize over parameters $\theta$ such that $\bf{x} &#x3D; g(\theta)$ looks like a sample from the frozen diffusion model. To perform this optimization, a differentiable loss function where plausible images have low loss, and implausible images have high loss is required.</p>
<p>While reusing the diffusion training loss to find modes of the learned conditional density $p(\bf{x}|y)$ in high dimensions are often far from typical samples, the multiscale nature of diffusion model training may help to avoid these pathologies.</p>
<p>Consider the training process of a ddpm which predicts the noise added at a certain timestep. For a pretrained ddpm with true noisy images as input, the predicted noise should be very close to real noise. Inversely, if those noisy images do not look like good images, there will be great difference between predicted noise and real noise.</p>
<p>Minimizing the diffusion training loss with respect to a generated datapoint $\bf{x} &#x3D; g(\theta)$ gives $ \theta^{*} &#x3D; \text{arg min} _{\theta}\mathcal{L}_{\text{Diff}}(\phi, \bf{x} &#x3D; g(\theta))$ . In practice, this loss function did not produce realistic samples even when using an identity DIP where $\bf{x} &#x3D; \theta$.</p>
<p>To understand the difficulties of this approach, consider the gradient of $\mathcal{L}_{\text{Diff}}$:</p>
<div style="text-align: center;">
    <span style="display: inline-block;">
        \begin{equation}
        \displaystyle\nabla_{\theta}\mathcal{L}_{\text{Diff}}(\phi,\mathbf{x}=g(\theta))=\mathbb{E}_{t,\epsilon}\Bigg{[}w(t)\underbrace{\left(\hat{\epsilon}_{\phi}({\mathbf{z}}_{t};y,t)-\epsilon\right)}_{\text{Noise Residual}}\underbrace{\frac{\partial\hat{\epsilon}_{\phi}({\mathbf{z}}_{t};y,t)}{\mathbf{z}_t}}_{\text{U-Net Jacobian}}\quad \underbrace{\frac{\partial\mathbf{x}}{\partial\theta}}_{\text{Generator Jacobian}}\Bigg{]}
        \end{equation}
    </span>
</div>

<p>Where the constant $\alpha _t\bf{I} &#x3D; \partial \bf{z}_t&#x2F;\partial \bf{x}$ is absorbed into $w(t)$. In practice, the U-Net Jacobian term is<br>expensive to compute (requires backpropagating through the diffusion model U-Net), and poorly conditioned for small noise levels as it is trained to approximate the scaled Hessian of the marginal density.Omitting the U-Net Jacobian term leads to an effective gradient for optimizing DIPs with diffusion models:</p>
<div style="text-align: center;">
    <span style="display: inline-block;">
        \begin{equation}
        \displaystyle\nabla_{\theta}\mathcal{L}_{\text{SDS}}(\phi,\mathbf{x}=g(\theta))\triangleq\mathbb{E}_{t,\epsilon}\left[w(t)\left(\hat{\epsilon}_{\phi}({\mathbf{z}}_{t};y,t)-\epsilon\right){\partial\mathbf{x}\over\partial\theta}\right]
        \end{equation}
    </span>
</div>

<p>Since the diffusion model directly predicts the update direction,there’s no need to backpropagate through the diffusion model; the model simply acts like an efficient, frozen critic that predicts image-space edits.</p>
<p>Empirically, the guidance weight $w$ is set to a large value for classifier-free guidance to improves quality.</p>
<h3 id="The-Dreamfusion-Algorithm"><a href="#The-Dreamfusion-Algorithm" class="headerlink" title="The Dreamfusion Algorithm"></a>The Dreamfusion Algorithm</h3><figure>
    <img src="/img/blogs/230814-DreamFusion/1.png" srcset="/img/loading.gif" lazyload>
    <figcaption>Fig.2 The Dreamfusion Algorithm</figcaption>
</figure>

<p>In DreamFusion, the Imagen model is chosen as the diffusion model.Only the pretrained $64\times 64$ base model (not the super-resolution cascade for generating higher-resolution images) is used with no modifications. To synthesize a scene from text, a model built on mip-NeRF 360 with random weights is initialized, then repeatedly render views of that NeRF from random camera positions and angles, using these renderings as the input to the score distillation loss function that wraps around Imagen.</p>
<p>Here is the pseudocode of the DreamFusion</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-built_in">params</span> = generator.init()<br>opt_state = optimizer.init(<span class="hljs-built_in">params</span>)<br>diffusion_model = diffusion.load_model()<br><span class="hljs-keyword">for</span> nstep <span class="hljs-keyword">in</span> iterations:<br>    t = <span class="hljs-built_in">random</span>.uniform(<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>)<br>    alpha_t, sigma_t = diffusion_model.get_coeffs(t)<br>    eps = <span class="hljs-built_in">random</span>.<span class="hljs-keyword">normal</span>(img_shape)<br>    x = generator(<span class="hljs-built_in">params</span>, &lt;other arguments&gt;...) <span class="hljs-comment"># Get an image observation.</span><br>    z_t = alpha_t * x + sigma_t * eps <span class="hljs-comment"># Diffuse observation.</span><br>    epshat_t = diffusion_model.epshat(z_t, y, t) <span class="hljs-comment"># Score function evaluation.</span><br>    g = grad(weight(t) * dot(stopgradient[epshat_t - eps], x), <span class="hljs-built_in">params</span>)<br>    <span class="hljs-built_in">params</span>, opt_state = optimizer.update(g, opt_state) <span class="hljs-comment"># Update params with optimizer.</span><br><span class="hljs-literal">return</span> <span class="hljs-built_in">params</span><br></code></pre></td></tr></table></figure>

<h3 id="Neural-Rendering-of-a-3D-Model"><a href="#Neural-Rendering-of-a-3D-Model" class="headerlink" title="Neural Rendering of a 3D Model"></a>Neural Rendering of a 3D Model</h3><p><strong>Shading.</strong> Traditional NeRF models emit radiance, which is RGB color conditioned on the ray direction from which the 3D point is being observed. In contrast, NeRF MLP in DreamFusion parameterizes the color of the surface itself, which is then lit by an illumination that can be controlled (a process commonly referred to as “shading”). An RGB albedo (the color of the material) is used for each point:</p>
<p>$$<br>(\tau, \mathbf{\rho}) &#x3D; \text{MLP}(\mathbf{\mu};\theta)<br>$$</p>
<p>where $\tau$ is volumetric density. Calculating the final shaded output color for the 3D point requires a normal vector indicating the local orientation of the object’s geometry. This surface normal vector can be computed by normalizing the negative gradient of density $\tau$ with respect to the 3D coordinate :$ {n}&#x3D;-\nabla_\tau &#x2F; \left \lVert \nabla_\tau\right\rVert\ $. With each normal n and material albedo $\rho$ , assuming some point light source with 3D coordinate $l$ and color $\ell_{\rho}$ , and an ambient light color $\ell_a$, we can render each point along the ray using diffuse reflectance  to produce a color $\bf{c}$ for each point:</p>
<p>$$<br>        \mathbf{c}&#x3D;{\rho}\circ\left({\ell}_{\rho}\circ\operatorname{max}\left(0,{n}\cdot({\ell}-{\mu})&#x2F;\left\lVert{\ell}-{\mu}\right\rVert\right)+{\ell}_{a}\right)<br>$$</p>
<p>With these colors and previously-generated densities, we approximate the volume rendering integral with the same rendering weights $w_i$ used in standard NeRF. It is beneficial to randomly replace the albedo color $\bf{c}$ with white $(1; 1; 1)$ to produce a “textureless” shaded output. This prevents the model from producing a degenerate solution in which scene content is drawn onto flat geometry to satisfy the text conditioning.</p>
<p><strong>Scene Structure.</strong> Scene is represented within a fixed bounding sphere, and an environment map generated from a second MLP that takes positionally-encoded ray direction as input is used to compute a background color.</p>
<p><strong>Geometry regularizers.</strong> A regularization penalty $\mathcal{L}_{orient} &#x3D; \sum_{i}stop\_grad(w_i) \max(0;n_i\cdot v)^2$ on the opacity along each ray to prevent unneccesarily filling in of empty space .A modified version of the orientation loss $\mathcal{L}_{opacity} &#x3D;\sqrt{\sum_i (w_i)^2 + 0.01}$ attempts to orient normals away from the camera so that the shading becomes darker</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.14988">Poole, B., Jain, A., Barron, J.T., &amp; Mildenhall, B. (2022). DreamFusion: Text-to-3D using 2D Diffusion. ArXiv, abs&#x2F;2209.14988.</a>
<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1CT411q7Pm/?spm_id_from=333.337.search-card.all.click&vd_source=e3817da2068f0aeb6cd83ad63f8c667d">DETAILS about DreamFusion: Text-to-3D using 2D Diffusion</a>
<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Papers/" class="category-chain-item">Papers</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Deep-Learning/">#Deep Learning</a>
      
        <a href="/tags/AIGC/">#AIGC</a>
      
        <a href="/tags/Computer-Vision/">#Computer Vision</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>DreamFusion</div>
      <div>https://iks-ran.github.io/2023/08/14/dreamfusion/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>iks-ran</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>August 14, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/08/15/SDFusion/" title="SDFusion">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">SDFusion</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/08/11/NeRF/" title="Neural Radiance Fields">
                        <span class="hidden-mobile">Neural Radiance Fields</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script type="text/javascript">
        var disqus_config = function() {
          this.page.url = 'https://iks-ran.github.io/2023/08/14/dreamfusion/';
          this.page.identifier = '/2023/08/14/dreamfusion/';
        };
        Fluid.utils.loadComments('#disqus_thread', function() {
          var d = document, s = d.createElement('script');
          s.src = '//' + 'fluid' + '.disqus.com/embed.js';
          s.setAttribute('data-timestamp', new Date());
          (d.head || d.body).appendChild(s);
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
