<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>利用 frp 进行内网穿透</title>
    <link href="/2024/01/29/reverse_proxy_with_frp/"/>
    <url>/2024/01/29/reverse_proxy_with_frp/</url>
    
    <content type="html"><![CDATA[<h2 id="利用-frp-进行内网穿透"><a href="#利用-frp-进行内网穿透" class="headerlink" title="利用 frp 进行内网穿透"></a>利用 frp 进行内网穿透</h2><h3 id="内网穿透"><a href="#内网穿透" class="headerlink" title="内网穿透"></a>内网穿透</h3><p>内网穿透是一种通过互联网将局域网内的服务暴露给外部网络的技术。在一些情况下，由于网络环境或安全限制，局域网内的设备无法直接通过公网访问。内网穿透技术通过一些手段，使外部网络能够访问局域网内的服务，就像这些服务直接部署在公网上一样。</p><h3 id="frp"><a href="#frp" class="headerlink" title="frp"></a>frp</h3><p>frp（Fast Reverse Proxy）是一款开源免费的内网穿透的工具，它使用了反向代理的方式来实现内网穿透。frp由客户端和服务器端组成，客户端运行在局域网内，服务器端运行在公网上。frp通过在服务器上创建一个反向代理端口，将外部请求转发到客户端上，从而实现对局域网内服务的访问。</p><h3 id="下载和安装："><a href="#下载和安装：" class="headerlink" title="下载和安装："></a>下载和安装：</h3><p>在 frp 的 <a href="https://github.com/fatedier/frp/releases/tag/v0.53.2">GitHub Releases</a> 下找到相应版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget https://github.com/fatedier/frp/releases/download/v0.53.2/frp_0.53.2_linux_amd64.tar.gz<br>tar -xf frp_0.53.2_linux_amd64.tar.gz<br></code></pre></td></tr></table></figure><p>之后进入目录，分别在局域网客户端 (Client) 和公网服务端 (Server) 进行配置</p><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs toml"><span class="hljs-comment"># vim frpc.toml</span><br><span class="hljs-comment"># Client</span><br><span class="hljs-attr">serverAddr</span> = <span class="hljs-string">&quot;YOUR_SERVER_IP&quot;</span><br><span class="hljs-attr">serverPort</span> = <span class="hljs-number">7000</span><br><br><span class="hljs-section">[[proxies]]</span><br><span class="hljs-attr">name</span> = <span class="hljs-string">&quot;web&quot;</span><br><span class="hljs-attr">type</span> = <span class="hljs-string">&quot;http&quot;</span><br><span class="hljs-attr">localPort</span> = <span class="hljs-number">5000</span> <span class="hljs-comment"># 局域网内 Client 的服务端口</span><br><span class="hljs-attr">customDomains</span> = [<span class="hljs-string">&quot;YOUR_CUSTOM_DOMAIN&quot;</span>, <span class="hljs-string">&quot;YOUR_SERVER_IP&quot;</span>, <span class="hljs-string">&quot;0.0.0.0&quot;</span>, <span class="hljs-string">&quot;localhost&quot;</span>, <span class="hljs-string">&quot;127.0.0.1&quot;</span>] <span class="hljs-comment"># 只能通过相应host访问，例如如果没有 &quot;localhost&quot; 一项，则就算在 frp Server的服务器上也无法通过 http://localhost 访问服务</span><br></code></pre></td></tr></table></figure><figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs toml"><span class="hljs-comment"># vim frps.toml</span><br><span class="hljs-comment"># Server</span><br><span class="hljs-attr">bindPort</span> = <span class="hljs-number">7000</span> <span class="hljs-comment"># 上面的 serverPort</span><br><span class="hljs-attr">vhostHTTPPort</span> = YOUR_SERVICE_PORT <span class="hljs-comment"># 代理服务的访问端口</span><br></code></pre></td></tr></table></figure><p>保存后分别在服务端和客户端启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Server</span><br>./frps -c frps.toml<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Client</span><br>./frpc -c frpc.toml<br></code></pre></td></tr></table></figure><p>其他配置可以参考 frp 的<a href="https://gofrp.org/">官方网站</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://gofrp.org/zh-cn/docs/">https://gofrp.org/zh-cn/docs/</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Web</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Flask + Gunicorn + Nginx 搭建 Web 服务</title>
    <link href="/2024/01/15/build_web_service/"/>
    <url>/2024/01/15/build_web_service/</url>
    
    <content type="html"><![CDATA[<h2 id="Flask-Gunicorn-Nginx-搭建-Web-服务"><a href="#Flask-Gunicorn-Nginx-搭建-Web-服务" class="headerlink" title="Flask + Gunicorn + Nginx 搭建 Web 服务"></a>Flask + Gunicorn + Nginx 搭建 Web 服务</h2><h3 id="Flask"><a href="#Flask" class="headerlink" title="Flask"></a>Flask</h3><p>Flask 是一个使用 Python 编写的轻量级 Web 框架，被广泛用于构建 Web 应用程序和 API, 开发者可以通过 Flask 在搭建本地 Web 服务。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, request<br>app = Flask(__name__)<br><br><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">&#x27;/&#x27;</span></span>)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">index</span>()<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello World!&quot;</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    app.run(port=<span class="hljs-number">5000</span>)<br></code></pre></td></tr></table></figure><p>这样，我们就创建了一个名为 app 的 Flask 应用程序，并定义了一个路由 &#x2F;，当访问 <code>127.0.0.1:5000</code> 时 返回 “Hello World!”，</p><h3 id="Gunicorn"><a href="#Gunicorn" class="headerlink" title="Gunicorn"></a>Gunicorn</h3><p>Gunicorn 是一个 Python WSGI HTTP 服务器，它可以帮助开发者在生产环境中运行 Flask 应用程序。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs text">➜  ~ gunicorn -h<br>usage: gunicorn [OPTIONS] [APP_MODULE]<br><br>options:<br>  ...<br>  -b ADDRESS, --bind ADDRESS<br>                        The socket to bind. [[&#x27;127.0.0.1:8000&#x27;]]<br>  ...<br>  -w INT, --workers INT<br>                        The number of worker processes for handling requests. [1]<br></code></pre></td></tr></table></figure><p>将上述 Flask 程序保存为 example.py, 则</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">gunicorn -w 4 -b 0.0.0.0:5555 example:app<br></code></pre></td></tr></table></figure><p>其中 example:app 代表了 example.py 中的 app 对象，启动后就可以通过 公网IP:端口访问。</p><h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h3><p>在 Nginx 的配置文件中添加一个新的服务器块，用于反向代理到 Gunicorn 服务器。进入Nginx 的默认配置文件目录 <code>/etc/nginx/conf.d</code>。新建 <code>$&#123;YOUR_SERVICE_NAME&#125;.conf</code>文件，内容可以参考</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-section">server</span> &#123;<br>    <span class="hljs-attribute">listen</span> <span class="hljs-number">80</span>;<br>    <span class="hljs-attribute">server_name</span> YourDomain.com;<br><br>    <span class="hljs-section">location</span> / &#123;<br>        <span class="hljs-attribute">proxy_pass</span> http://127.0.0.1:<span class="hljs-variable">$&#123;GUNICORN_SERVICE_PORT&#125;</span>;<br>        <span class="hljs-attribute">proxy_set_header</span> Host <span class="hljs-variable">$host</span>;<br>        <span class="hljs-attribute">proxy_set_header</span> X-Real-IP <span class="hljs-variable">$remote_addr</span>;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>如果需要安装配置SSL证书，可以参考 <a href="https://help.aliyun.com/zh/ssl-certificate/user-guide/overview-of-free-certificates-overview-of-free-certificates">免费SSL证书概述</a>, 相应的配置文件可以参考</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-section">server</span> &#123;<br>        <span class="hljs-attribute">listen</span> <span class="hljs-number">80</span>;<br>        <span class="hljs-attribute">server_name</span> YourDomain.com;<br><br>        <span class="hljs-comment"># 添加这一段用于重定向HTTP到HTTPS</span><br>        <span class="hljs-section">location</span> / &#123;<br>                <span class="hljs-attribute">return</span> <span class="hljs-number">301</span> https://<span class="hljs-variable">$host</span><span class="hljs-variable">$request_uri</span>;<br>        &#125;<br>&#125;<br><br><span class="hljs-section">server</span> &#123;<br>        <span class="hljs-attribute">listen</span> <span class="hljs-number">443</span> ssl;<br>        <span class="hljs-attribute">server_name</span> YourDomain.com;<br><br>        <span class="hljs-attribute">ssl_certificate</span> &lt;cert-file-name&gt;.pem;<br>        <span class="hljs-attribute">ssl_certificate_key</span> &lt;cert-file-name&gt;.key;<br><br>        <span class="hljs-comment"># 其余的SSL配置，如ssl_protocols、ssl_ciphers等</span><br>        <span class="hljs-attribute">ssl_session_timeout</span> <span class="hljs-number">5m</span>;<br>        <span class="hljs-attribute">ssl_protocols</span> TLSv1.<span class="hljs-number">1</span> TLSv1.<span class="hljs-number">2</span> TLSv1.<span class="hljs-number">3</span>; <span class="hljs-comment">#表示使用的TLS协议的类型。</span><br>        <span class="hljs-attribute">ssl_prefer_server_ciphers</span> <span class="hljs-literal">on</span>;<br>        <span class="hljs-section">location</span> / &#123;<br>                <span class="hljs-attribute">proxy_pass</span> http://127.0.0.1:<span class="hljs-variable">$&#123;GUNICORN_SERVICE_PORT&#125;</span>;<br>                <span class="hljs-attribute">proxy_set_header</span> X-Real-IP <span class="hljs-variable">$remote_addr</span>;<br>                <span class="hljs-attribute">proxy_set_header</span> X-Forwarded-For <span class="hljs-variable">$proxy_add_x_forwarded_for</span>;<br>                <span class="hljs-attribute">proxy_connect_timeout</span> <span class="hljs-number">500s</span>;<br>                <span class="hljs-attribute">proxy_read_timeout</span> <span class="hljs-number">500s</span>;<br>                <span class="hljs-attribute">proxy_send_timeout</span> <span class="hljs-number">500s</span>;<br>        &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>保存后检查是否存在语法错误并重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">nginx -s reload<br>nginx -t<br>sudo service nginx restart<br></code></pre></td></tr></table></figure><p>之后再域名记录中添加一条 <code>A Record</code>，Value 值 为服务器地址，过后就可以通过域名访问服务器上的 Web 服务了。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://help.aliyun.com/zh/ssl-certificate/user-guide/overview-of-free-certificates-overview-of-free-certificates">免费SSL证书概述</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Web</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Text to Image 使用记录</title>
    <link href="/2024/01/07/t2i_tips/"/>
    <url>/2024/01/07/t2i_tips/</url>
    
    <content type="html"><![CDATA[<h2 id="Text-to-Image-使用记录"><a href="#Text-to-Image-使用记录" class="headerlink" title="Text to Image 使用记录"></a>Text to Image 使用记录</h2><h3 id="常用模型"><a href="#常用模型" class="headerlink" title="常用模型"></a>常用模型</h3><ul><li>animagine-xl-3.0</li><li>anything-v5</li><li>OrangeMixs</li><li>…</li></ul><h3 id="常用-prompt"><a href="#常用-prompt" class="headerlink" title="常用 prompt"></a>常用 prompt</h3><h4 id="提升成图质量"><a href="#提升成图质量" class="headerlink" title="提升成图质量"></a>提升成图质量</h4><ul><li>highres</li><li>best quality</li><li>masterpiece</li><li>…</li></ul><h4 id="添加后期效果"><a href="#添加后期效果" class="headerlink" title="添加后期效果"></a>添加后期效果</h4><ul><li>chromatic cberration - 色差畸变</li><li>lens flare - 镜头光晕</li><li>light rays - 光线</li><li>Bbacklighting - 背光</li><li>diffraction spikes - 衍射尖峰</li><li>overexposure - 过曝</li><li>streaked hair - 条纹效应（通常指头发上的条纹）</li><li>shadow - 阴影</li><li>film grain - 胶片颗粒</li><li>blurry background - 模糊背景</li><li>bokeh - 散景</li><li>depth of field - 景深</li><li>cold&#x2F;warm tones - 冷&#x2F;暖色调</li><li>…</li></ul><h4 id="prompt-举例"><a href="#prompt-举例" class="headerlink" title="prompt 举例"></a>prompt 举例</h4><p>一个典型的prompt可以参考</p><blockquote><p>1girl, sea,sunflower field, island, school uniform, serafuku, summer, sunshine, morning, original, highres, chromatic aberration, lens flare, two-tone eyes, best quality, masterpiece, overexposure, waist-length black twintails, streaked hair, warm tones, blurred background, landscape, diffusion, bokeh effect, depth of field</p></blockquote><figure>    <img src="/img/blogs/240107-t2i_tips/1.png">    <figcaption>图.1 成图</figcaption></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://huggingface.co/Linaqruf/animagine-xl-2.0">https://huggingface.co/Linaqruf/animagine-xl-2.0</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://huggingface.co/stablediffusionapi/anything-v5">https://huggingface.co/stablediffusionapi/anything-v5</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://huggingface.co/WarriorMama777/OrangeMixs">https://huggingface.co/WarriorMama777/OrangeMixs</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AIGC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3D Gaussian Splatting</title>
    <link href="/2023/10/17/3d_gaussian_splatting/"/>
    <url>/2023/10/17/3d_gaussian_splatting/</url>
    
    <content type="html"><![CDATA[<h2 id="3D-Gaussian-Splatting"><a href="#3D-Gaussian-Splatting" class="headerlink" title="3D Gaussian Splatting"></a>3D Gaussian Splatting</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ( ≥ 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization&#x2F;density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows real-time rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/231017-3D_Gaussian_Splatting/1.png">    <figcaption>Fig.1 Overview</figcaption></figure><blockquote><p>Optimization starts with the sparse SfM point cloud and creates a set of 3D Gaussians. We then optimize and adaptively control the density of this set of Gaussians. During optimization we use our fast tile-based renderer, allowing competitive training times compared to SOTA fast radiance field methods. Once trained, our renderer allows real-time navigation for a wide variety of scenes.</p></blockquote><p>Starting from a set of images of a static scene together with the corresponding cameras calibrated by SfM which produces a sparse point cloud as a sideeffect, a set of 3D Gaussians that allows a very flexible optimization regime is defined by a position (mean), covariance matrix and opacity $\alpha$. This results in a reasonably compact representation of the 3D scene, in part because highly anisotropic volumetric splats can be used to represent fine structures compactly. The directional appearance component (color) of the radiance field is represented via spherical harmonics (SH), following standard practice. The algorithm proceeds to create the radiance field representation via a sequence of optimization steps of 3D Gaussian parameters, i.e., position, covariance, $\alpha$ and SH coefficients interleaved with operations for adaptive control of the Gaussian density. The key to the efficiency is the tile-based rasterizer that allows $\alpha$-blending of anisotropic splats, respecting visibility order thanks to fast sortingand a fast backward pass by tracking accumulated $\alpha$ values, without a limit on the number of Gaussians that can receive gradients.</p><h3 id="Point-Based-Rendering-and-Radiance-Fields"><a href="#Point-Based-Rendering-and-Radiance-Fields" class="headerlink" title="Point-Based Rendering and Radiance Fields"></a>Point-Based Rendering and Radiance Fields</h3><p>Point-based $\alpha$-blending and NeRF-style volumetric rendering share essentially the same image formation model. Specifically, the color $C$ is given by volumetric rendering along a ray:</p><p>$$<br>C&#x3D;\sum_{i&#x3D;1}^{N}T_{i}(1-\exp(-\sigma_{i}\delta_{i}))\mathbf{c}_{i}\hskip 5.0pt\text{ with }\hskip 5.0ptT_{i}&#x3D;\exp\left(-\sum_{j&#x3D;1}^{i-1}\sigma_{j}\delta_{j}\right)<br>$$</p><p>where samples of density $\sigma$, transmittance $T$ , and color $\mathbf{c}$ are taken along the ray with intervals $\delta_i$ . This can be re-written as</p><p>$$<br>C&#x3D;\sum_{i&#x3D;1}^{N}T_{i}\alpha_{i}\mathbf{c}_{i}<br>$$</p><p>with</p><p>$$<br>\alpha_{i}&#x3D;(1-\exp(-\sigma_{i}\delta_{i}))\hskip 5.0pt\text{and}\hskip 5.0ptT_{i}&#x3D;\prod_{j&#x3D;1}^{i-1}(1-\alpha_{i})<br>$$</p><p>A typical neural point-based approach computes the color $C$ of a pixel by blending $\mathcal{N}$ ordered points overlapping the pixel</p><p>$$<br>C&#x3D;\sum_{i\in\mathcal{N}}\mathbf{c}_{i}\alpha_{i}\prod_{j&#x3D;1}^{i-1}(1-\alpha_{j})<br>$$</p><p>where $\mathbf{c}_i$ is the color of each point and $\alpha_i$ is given by evaluating a 2D Gaussian with covariance $\Sigma$ multiplied with a learned per-point opacity.</p><p>Although the image formation model is the same, the rendering algorithm is very different. NeRFs are a continuous representation implicitly representing empty&#x2F;occupied space; expensive random sampling is required to find the samples with consequent noise and computational expense. In contrast, points are an unstructured, discrete representation that is flexible enough to allow creation, destruction, and displacement of geometry similar to NeRF. This is achieved by optimizing opacity and positions, while avoiding the shortcomings of a full volumetric representation.</p><h3 id="Differentiable-3D-Gaussian-Splatting"><a href="#Differentiable-3D-Gaussian-Splatting" class="headerlink" title="Differentiable 3D Gaussian Splatting"></a>Differentiable 3D Gaussian Splatting</h3><p>The 3D Gaussians are defined by a full 3D covariance matrix $\Sigma$ defined in world space centered at point (mean) $\mu$ and then multiplied by $\alpha$ in blending process.</p><p>$$<br>G(x) &#x3D; e^{-\frac{1}{2}(x)^T\Sigma ^{-1}(x)}<br>$$</p><p>To project the 3D Gaussians to 2D for rendering, given a viewing transformation $W$, the covariance matrix $\Sigma^{\prime}$ in camera coordinates is given as follows</p><p>$$<br>\Sigma^{\prime} &#x3D; JW\Sigma W^TJ^T<br>$$</p><p>where $J$ is the Jacobian of the affine approximation of the projective transformation. As Gradient descent cannot be easily constrained to produce valid covariance matrices which have physical meaning only when they are positive semi-definite, given a scaling matrix $S$ and rotation matrix $R$, the corresponding $\Sigma$ is defined as</p><p>$$<br>\Sigma &#x3D; RSS^TR^T<br>$$</p><p>To allow independent optimization of both factors, $S$ and $R$ are stored separately: a 3D vector $s$ for scaling and a quaternion $q$ to represent rotation. These can be trivially converted to their respective matrices and combined, making sure to normalize $q$ to obtain a valid unit quaternion.</p><h3 id="Fast-Differentiable-Rasterizer-for-Gaussians"><a href="#Fast-Differentiable-Rasterizer-for-Gaussians" class="headerlink" title="Fast Differentiable Rasterizer for Gaussians"></a>Fast Differentiable Rasterizer for Gaussians</h3><figure>    <img src="/img/blogs/231017-3D_Gaussian_Splatting/2.png" width=400>    <figcaption>Fig.2 GPU software rasterization of 3D Gaussians</figcaption></figure><ol><li>Divide the entire image into $16\times 16$ tiles, and select 3D Gaussians that are visible and have a confidence over 0.99 in each tile’s view frustum.</li><li>Assign a key for each splats instance with up to 64 bits where the lower 32 bits encode its projected depth and the higher bits encode the index of the overlapped tile.</li><li>Sort Gaussian splats by key to get the Gaussian list of each tile.</li><li>Start a thread block for each tile. Each thread block first cooperatively loads the of Gaussian splats into shared memory, and then for a given pixel, accumulates the color and value by traversing the list from front to back. </li><li>When the preset saturation is reached in a pixel (i.e. equal to 1), the corresponding thread stops running.</li><li>At regular intervals, threads in the tile are queried, and processing of the entire tile is terminated when all pixels are saturated (i.e. become 1).</li></ol><h3 id="Optimization-with-Adaptive-Density-Control-of-3D-Gaussians"><a href="#Optimization-with-Adaptive-Density-Control-of-3D-Gaussians" class="headerlink" title="Optimization with Adaptive Density Control of 3D Gaussians"></a>Optimization with Adaptive Density Control of 3D Gaussians</h3><h4 id="Adaptive-Control-of-Gaussians"><a href="#Adaptive-Control-of-Gaussians" class="headerlink" title="Adaptive Control of Gaussians"></a>Adaptive Control of Gaussians</h4><p>Due to the ambiguity of 3D to 2D projection, geometry may be placed incorrectly. Therefore, optimization requires the ability to create geometry and destroy or move it if it is not in the correct position. The quality of the covariance parameters of a 3D Gaussian distribution is critical to the compactness of the representation, since large uniform regions can be captured with a small number of large anisotropic Gaussian distributions.</p><p>To be able to transition from an initially sparse collection of 3D Gaussians to a denser collection that better represents the scene and has the correct parameters, the number of 3D Gaussians and their density per unit volume are controlled adaptively. The control algorithm densifies every 100 iterations and remove any Gaussians that are essentially transparent.</p><figure>    <img src="/img/blogs/231017-3D_Gaussian_Splatting/3.png">    <figcaption>Fig.3 Adaptive Control of Gaussians</figcaption></figure><p>See Fig.3, for a small Gaussian in the unreconstructed region, the new geometry is created by cloning the Gaussian and moving it along the position gradient direction. Large Gaussians in regions with high variance need to be split into smaller Gaussians. Two new Gaussian distributions replace the original large Gaussian distribution, and the scaling factor is determined experimentally to divide their scales, using the original Gaussian as the sampling PDF to initialize their positions.</p><p>Floaters near the camera tend to appear during optimization, so the way to control the growth of Gaussians is to set $\alpha$ close to 0 every 3000 iterations</p><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p>A Sigmoid activation function is used to constrain $\alpha$ in the range of $[0 - 1)$ , and an exponential activation function is used for the covariance.The initial covariance matrix is estimated as an isotropic Gaussian matrix with axes equal to the mean distance to the nearest three points. The loss function is a combination of L1 and D-SSIM terms </p><p>$$<br>\mathcal{L} &#x3D; (1 - \lambda)\mathcal{L}_1 + \lambda\mathcal{L}_{\text{D-SSIM}}<br>$$</p><figure>    <img src="/img/blogs/231017-3D_Gaussian_Splatting/4.png" width=400>    <figcaption>Fig.4 Optimization</figcaption></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2308.04079">Kerbl, B., Kopanas, G., Leimkuehler, T., &amp; Drettakis, G. (2023). 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics (TOG), 42, 1 - 14.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/654627437">3D Gaussian Splatting for Real-Time Radiance Field Rendering 笔记</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/661363274">论文简析：3D Gaussian Splatting !!!</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/655325345">3D Gaussian Splatting笔记</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://www.bilibili.com/video/BV1uV4y1Y7cA/?share_source=copy_web&vd_source=bd133237b66721b62ed05d453aa32bac">【论文讲解】用点云结合3D高斯构建辐射场，成为快速训练、实时渲染的新SOTA！</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HiFA</title>
    <link href="/2023/10/16/HiFA/"/>
    <url>/2023/10/16/HiFA/</url>
    
    <content type="html"><![CDATA[<h2 id="SparseNeus"><a href="#SparseNeus" class="headerlink" title="SparseNeus"></a>SparseNeus</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>The advancements in automatic text-to-3D generation have been remarkable. Most existing methods use pre-trained text-to-image diffusion models to optimize 3D representations like Neural Radiance Fields (NeRFs) via latent-space denoising score matching. Yet, these methods often result in artifacts and inconsistencies across different views due to their suboptimal optimization approaches and limited understanding of 3D geometry. Moreover, the inherent constraints of NeRFs in rendering crisp geometry and stable textures usually lead to a two-stage optimization to attain high-resolution details. This work proposes holistic sampling and smoothing approaches to achieve high-quality text-to-3D generation, all in a single-stage optimization. We compute denoising scores in the text-to-image diffusion model’s latent and image spaces. Instead of randomly sampling timesteps (also referred to as noise levels in denoising score matching), we introduce a novel timestep annealing approach that progressively reduces the sampled timestep throughout optimization. To generate high-quality renderings in a single-stage optimization, we propose regularization for the variance of z-coordinates along NeRF rays. To address texture flickering issues in NeRFs, we introduce a kernel smoothing technique that refines importance sampling weights coarse-to-fine, ensuring accurate and thorough sampling in high-density regions. Extensive experiments demonstrate the superiority of our method over previous approaches, enabling the generation of highly detailed and view-consistent 3D assets through a single-stage training process.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/231016-HiFA/1.png">    <figcaption>Fig.1 HiFA</figcaption></figure><blockquote><p>Overview of our proposed method for text-to-3D synthesis. We aim to optimize a 3D model $g(θ)$ using a pre-trained 2D latent diffusion prior. To achieve this, we employ a latent diffusion model to provide gradients. Specifically, the diffusion model takes a rendered image $x$ as input and provides the estimate of the input rendered image, denoted as $\hat{x}$. Unlike existing works that solely focus on computing noise residuals in the low-resolution latent space of the diffusion model, we propose a novel loss $\mathcal{L}_{\text{BGT+}}$ that computes a higherresolution image residual.</p></blockquote><h3 id="Advancing-Training-Process-with-Reformulated-SDS"><a href="#Advancing-Training-Process-with-Reformulated-SDS" class="headerlink" title="Advancing Training Process with Reformulated SDS"></a>Advancing Training Process with Reformulated SDS</h3><p>The SDS gradient in the latent space of SD is written as</p><p>$$<br>\nabla _{\theta}\mathcal{L}_{\text{SDS}}(\phi, {\bf z}) &#x3D; \mathbb{E}_{t, {\epsilon}}[\omega(t)(\hat{\epsilon} - \epsilon) \frac{\partial {\bf z}}{\partial \theta} ]<br>$$</p><p>which performs the diffusion process in the latent space, resulting in changes to the SDS gradients in this scenario. </p><p>A specific formulation for SDS can be formulated as </p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}     \displaystyle\mathcal{L}_{\text{SDS}}(\phi,{\bf z})& =\mathbb{E}_{t,{\bf {\epsilon}}}[\omega(t)\frac{\sqrt{\bar{\alpha}_{t}}}{2\sqrt{1-\bar{\alpha}_{t}}}\|{\bf z}-\hat{\bf z}_{\text{1step}}\|^{2}],\text{where} \\    & \displaystyle\hat{\bf z}_{\text{1step}}=\frac{1}{\sqrt{\bar{\alpha}_{t}}}({\bf z}_{t}-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\phi}({\bf z}_{t};{\bf y},t))    \end{aligned}    </span></div><p>with the proof</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}    \displaystyle\nabla_{\theta}\mathcal{L}_{\text{SDS}}(\phi,{\bf z}) = & \displaystyle\quad\omega(t)\frac{\sqrt{\bar{\alpha}_{t}}}{\sqrt{1-\bar{\alpha}_{t}}}({\bf z}-\hat{\bf z}_{\text{1step}})\frac{\partial{\bf z}}{\partial\theta} \\    = & \displaystyle\quad\omega(t)\frac{\sqrt{\bar{\alpha}_{t}}}{\sqrt{1-\bar{\alpha}_{t}}}({\bf z}-\frac{1}{\sqrt{\bar{\alpha}_{t}}}({\bf z}_{t}-\sqrt{1-\bar{\alpha}_{t}}\epsilon_{\phi}({\bf z}_{t};{\bf y},t)))\frac{\partial{\bf z}}{\partial\theta} \\    = & \displaystyle\quad\omega(t)\frac{\sqrt{\bar{\alpha}_{t}}}{\sqrt{1-\bar{\alpha}_{t}}}(\frac{\sqrt{1-\bar{\alpha}_{t}}}{\sqrt{\bar{\alpha}_{t}}}(\hat{\bf {\epsilon}}-{\bf \epsilon}))\frac{\partial z}{\partial\theta} \\    = & \displaystyle\quad\omega(t)(\hat{\bf {\epsilon}}-{\bf \epsilon})\frac{\partial{\bf z}}{\partial\theta}    \end{aligned}    </span></div><p>where $\hat{\bf z}_{\text{1step}}$ is the denoised $\hat{\bf z}$ estimate. To address OOD issue of the diffusion input and divergence issue of the diffusion output, HiFA employs a more accurate estimation for the latent vector ${\bf z}$, which is denoted as $\hat{\bf z}$, obtained through iterative denoisinginstead of using $\hat{\bf z}_{\text{1step}}$ for SDS, and gradually decreases the intensity of the added noise during training by annealing the step $t$. Thus, this adapted loss is<br>denoted as $\mathcal{L}_{\text{BGT}}$, formally written as</p><p>$$<br>\mathcal{L}_{\text{BGT}}(\phi,\mathbf{z},\mathbf{\hat{z}})&#x3D;\mathbb{E}_{t,\mathbf{\epsilon}}||\mathbf{\mathbf{z}-\hat{z}}||^{2}<br>$$</p><p>After obtaining the estimated latent vector $\hat{\bf z}$, a further adapted loss $\mathcal{L}_{\text{BGT+}}$ can be naturally obtained, by incorporating additional supervision for recovered images</p><p>$$<br>\mathcal{L}_{\text{BGT+}}(\phi,\mathbf{z},\mathbf{\hat{z}})&#x3D;\mathbb{E}_{t,\mathbf{\epsilon}}[||\mathbf{\mathbf{z}-\hat{z}}||^{2}+\lambda_{\text{rgb}}||\mathbf{\mathbf{x}-\hat{x}}||^{2}]<br>$$</p><p>$\lambda_{\text{rgb}}$ is a scaling parameter. The time step $t$ is scheduled by </p><p>$$<br>t &#x3D; t_{\max} - (t_{\max} - t_{\min})\sqrt{\frac{\text{iter}}{\text{total_iter}}}<br>$$</p><p>where $t$ decreases rapidly at the beginning of the training process and slows down toward the end. This scheduling strategy allocates more training steps to lower values of $t$, ensuring that fine-grained details can be adequately learned during the latter stages of training.</p><h3 id="Advanced-supervision-for-NeRFs"><a href="#Advanced-supervision-for-NeRFs" class="headerlink" title="Advanced supervision for NeRFs"></a>Advanced supervision for NeRFs</h3><figure>    <img src="/img/blogs/231016-HiFA/2.png" width=400>    <figcaption>Fig.2 Illustration of the depth loss $\mathcal{L}_d$</figcaption></figure><p>A regularization loss $\mathcal{L}_{d}$ on the disparity map ${\bf d}$ is leveraged to ensure multi-view consistency with a pre-trained depth predictor to estimate the pseudo ground truth of the disparity map ${\bf d}^*$</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}     \mathbf{d}^{\prime}=\frac{\langle\mathbf{d},\mathbf{d}^{*}\rangle\mathbf{d}}{\|\mathbf{d}\|^{2}}\text{, and }\mathcal{L}_{d}=\mathbb{E}_{\mathbf{d}}\|\mathbf{d}^{\prime}-\mathbf{d}^{*}\|^{2}    \end{aligned}    </span></div><p>To reduce the variance of the distribution of the sampled z-coordinates ${z}_i$ along the ray $r$, HiFA computes the z-variance along thecray $r$, denoted as $\sigma^{2}_{z_{r}}$</p><p>$$<br>\sigma^{2}_{z_{r}}&#x3D;\mathbb{E}_{z_{i}}[(z_{i}-\mu_{z_{r}})^{2}]&#x3D;\sum_{i}(z_{i}-\mu_{z_{r}})^{2}\frac{w_{i}}{\sum_{i}w_{i}}<br>$$</p><p>The regularization loss $\mathcal{L}_{\text{zvar}}$ for the variance $\sigma^{2}_{z_{r}}$ is defined as </p><p>$$<br>\mathcal{L}_{\text{zvar}}&#x3D;\mathbb{E}_{r}\delta_{r}\sigma^{2}_{z_{r}}\quad \delta_{r}&#x3D;1\text{ if }\sum_{i}w_{i}&gt;0.5,\text{ else }0<br>$$</p><p>and the total loss function is</p><p>$$<br>\mathcal{L}&#x3D;\quad\mathcal{L}_{\text{BGT+}}+\lambda_{d}\mathcal{L}_{d}+\lambda_{\text{zvar}}\mathcal{L}_{\text{zvar}}<br>$$</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2305.18766">Zhu, J., &amp; Zhuang, P. (2023). HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. ArXiv, abs&#x2F;2305.18766.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>One-2-3-45:One Image to 3D Mesh in 45s</title>
    <link href="/2023/09/30/One-2-3-45/"/>
    <url>/2023/09/30/One-2-3-45/</url>
    
    <content type="html"><![CDATA[<h2 id="One-2-3-45-One-Image-to-3D-Mesh-in-45s"><a href="#One-2-3-45-One-Image-to-3D-Mesh-in-45s" class="headerlink" title="One-2-3-45:One Image to 3D Mesh in 45s"></a>One-2-3-45:One Image to 3D Mesh in 45s</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/230930-One-2-3-45/1.png">    <figcaption>Fig.1 One-2-3-45</figcaption></figure><blockquote><p> Our method consists of three primary components: (a) <strong>Multi-view synthesis</strong>: we use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images in a two-stage manner. The input of Zero123 includes a single image and a relative camera transformation, which is parameterized by the relative spherical coordinates $(\Delta \theta, \Delta \phi, \Delta r)$. (b) <strong>Pose estimation</strong>: we estimate the elevation angle θ of the input image based on four nearby views generated by Zero123. We then obtain the poses of the multi-view images by combining the specified relative poses with the estimated pose of the input view. (c) <strong>3D reconstruction</strong>: We feed the multi-view posed images to an SDF-based generalizable neural surface reconstruction module for 360◦ mesh reconstruction.</p></blockquote><h3 id="Problems-of-Zero123-Based-Methods"><a href="#Problems-of-Zero123-Based-Methods" class="headerlink" title="Problems of Zero123 Based Methods"></a>Problems of Zero123 Based Methods</h3><p>Zero123 tends to generate predictions that are perceptually similar to the ground truth and have similar contours or boundaries, but the pixel-level appearance may not be exactly the same. Nevertheless, such inconsistencies between the source views are already fatal to traditional optimization-based methods. Thus, both traditional NeRF-based andor SDF-based methods failed to reconstruct high-quality 3D meshes from predictions of Zero123.</p><h3 id="2-Stage-Source-View-Selection-and-Groundtruth-Prediction-Mixed-Training"><a href="#2-Stage-Source-View-Selection-and-Groundtruth-Prediction-Mixed-Training" class="headerlink" title="2-Stage Source View Selection and Groundtruth-Prediction Mixed Training"></a>2-Stage Source View Selection and Groundtruth-Prediction Mixed Training</h3><p>To enable the module to learn to handle the inconsistent predictions from Zero123 and reconstruct a consistent 360◦ mesh, both the ground-truth RGB and depth values are supervised during training. </p><h3 id="Camera-Pose-Estimation"><a href="#Camera-Pose-Estimation" class="headerlink" title="Camera Pose Estimation"></a>Camera Pose Estimation</h3><p>Zero123 predicts four nearby views of the input image at first. Then for each elevation candidate angle,<br>compute the corresponding camera poses for the four images and calculate a reprojection error for this set of camera poses to measure the consistency between the images and the camera poses in a coarse-to-fine manner. The elevation angle with the smallest reprojection error is used to generate the camera poses for all 4 × n source views by combining the pose of the input view and the relative poses.</p><p><strong>Re-projection Error.</strong> </p><blockquote><p>For each triplet of images $(a, b, c)$ sharing a set of keypoints $P$, we consider each point $p \in P$. Utilizing images $a$ and $b$, we perform triangulation to determine the 3D location of $p$. We then project the 3D point onto the third image $c$ and calculate the reprojection error, which is defined as the $l1$ distance between the reprojected 2D pixel and the estimated keypoint in image c. By enumerating all image triplets and their corresponding shared keypoints, we obtain the mean projection error for each elevation angle candidate</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2306.16928">Liu, M., Xu, C., Jin, H., Chen, L., MukundVarma, T., Xu, Z., &amp; Su, H. (2023). One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization. ArXiv, abs&#x2F;2306.16928.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SparseNeuS</title>
    <link href="/2023/09/30/SparseNeus/"/>
    <url>/2023/09/30/SparseNeus/</url>
    
    <content type="html"><![CDATA[<h2 id="SparseNeus"><a href="#SparseNeus" class="headerlink" title="SparseNeus"></a>SparseNeus</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>We introduce SparseNeuS, a novel neural rendering based method for the task of surface reconstruction from multi-view images. This task becomes more difficult when only sparse images are provided as input, a scenario where existing neural reconstruction approaches usually produce incomplete or distorted results. Moreover, their inability of generalizing to unseen new scenes impedes their application in practice. Contrarily, SparseNeuS can generalize to new scenes and work well with sparse images (as few as 2 or 3). SparseNeuS adopts signed distance function (SDF) as the surface representation, and learns generalizable priors from image features by introducing geometry encoding volumes for generic surface prediction. Moreover, several strategies are introduced to effectively leverage sparse views for high-quality reconstruction, including 1) a multi-level geometry reasoning framework to recover the surfaces in a coarse-to-fine manner; 2) a multi-scale color blending scheme for more reliable color prediction; 3) a consistency-aware fine-tuning scheme to control the inconsistent regions caused by occlusion and noise. Extensive experiments demonstrate that our approach not only outperforms the state-of-the-art methods, but also exhibits good efficiency, generalizability, and flexibility.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/230930-SparseNeus/1.png">    <figcaption>Fig.1 SparseNeuS</figcaption></figure><p>Given a few views with known camera parameters, SparseNeuS first constructs cascaded geometry encoding volumes that encode local geometry surface information, and recover surfaces from the volumes in a coarse-to-fine manner.  SparseNeuS secondly leverages a multi-scale color blending module to predict colors by aggregating information from input images, and then combines the estimated geometry with predicted colors to render synthesized views using volume rendering. Finally, a consistencyaware fine-tuning scheme is proposed to further improve the obtained geometry with fine-grained details.</p><h3 id="Geometry-Reasoning"><a href="#Geometry-Reasoning" class="headerlink" title="Geometry Reasoning"></a>Geometry Reasoning</h3><p>SparseNeuS constructs cascaded geometry encoding volumes of two different resolutions for geometry reasoning, which aggregates image features to encode the information of local geometry. Specially, the coarse geometry is first extracted from a geometry encoding volume of low resolution, and then it is used to guide the geometry reasoning of the fine level.</p><h4 id="Geometry-Encoding-Volume"><a href="#Geometry-Encoding-Volume" class="headerlink" title="Geometry Encoding Volume"></a>Geometry Encoding Volume</h4><p>For the scene captured by $N$ input images $\{ I_{i} \}_{i&#x3D;0}^{N-1}$, a bounding box which can cover the region of interests is first estimated. The bounding box is defined in the camera coordinate system of the centered input image, and then grided into regular voxels. To construct a geometry encoding volume $M$, 2D feature maps $\{F_{i}\}_{i&#x3D;0}^{N-1}$ are extracted from the input images $\{I_{i}\}_{i&#x3D;0}^{N-1}$ by a 2D feature extraction network. Next, with the camera parameters of one image $I$, each vertex $v$ of the bounding box is projected to each feature map $F_{i}$ and gots its features $F_{i}(\pi _{i}(v))$ by interpolation, where $\pi_{i}(v)$ denotes the projected pixel location of $F_{i}$ For simplicity, $F_{i}(\pi_{i}(v))$ is abbreviated as $F_{i}(v)$. Initialized by a cost volume $B$ and aggregated by a sparse 3D CNN $\psi$, the geometry encoding volume $M$ is constructed using all the projected features $\{F_{i}\}_{i&#x3D;0}^{N-1}$ of each vertex: </p><p>$$<br>M &#x3D; \psi(B), \quad B&#x3D;\text{Var}(\{F_{i}\}_{i&#x3D;0}^{N-1})<br>$$</p><h4 id="Surface-Extraction"><a href="#Surface-Extraction" class="headerlink" title="Surface Extraction"></a>Surface Extraction</h4><p>Given an arbitrary 3D location $q$, an MLP network $f_{\theta}$ takes the combination of the 3D coordinate and its corresponding interpolated features of geometry encoding volume $M(q)$ as input, to predict the Signed Distance Function (SDF) $s(q)$ for surface representation. Specially, positional encoding PE is applied on its 3D coordinates, and the surface extraction operation is expressed as: $s(q) &#x3D; f_{\theta}(\text{PE}(q), M(q))$.</p><h4 id="Cascaded-Volumes-Scheme"><a href="#Cascaded-Volumes-Scheme" class="headerlink" title="Cascaded Volumes Scheme"></a>Cascaded Volumes Scheme</h4><p>For balancing the computational efficiency and reconstruction accuracy, SparseNeuS constructs cascaded geometry encoding volumes of two resolutions to perform geometry reasoning in a coarse-to-fine manner. A coarse geometry encoding volume is first constructed to infer the fundamental geometry, which presents the global structure of the scene but is relatively less accurate due to limited volume resolution. Guided by the obtained coarse geometry, a fine level geometry encoding volume is constructed to further refine the surface details. Numerous vertices far from the coarse surfaces can be discarded in the fine-level volume, which significantly reduces the computational memory burden and improves efficiency.</p><h3 id="Appearance-Prediction"><a href="#Appearance-Prediction" class="headerlink" title="Appearance Prediction"></a>Appearance Prediction</h3><p>Given an arbitrary 3D location $q$ on a ray with direction $d$, it is difficult for a network to directly regress color values for rendering novel views with those limited information. Thus, SparseNeuS predicts blending weights of the input images to generate new colors. A location $q$ is first projected to the input images to obtain the corresponding colors $\{I_i(q)\}^{N−1}_{i&#x3D;0}$. Then the colors from different views are blended together as the predicted color of $q$ using the estimated blending weights.</p><h4 id="Blending-Weights"><a href="#Blending-Weights" class="headerlink" title="Blending Weights"></a>Blending Weights</h4><p>SparseNeuS first projects $q$ onto the feature maps $\{F_{i}\}_{i&#x3D;0}^{N-1}$ to extract the corresponding features $\{F_{i}(q)\}_{i&#x3D;0}^{N-1}$ using bilinear interpolation. Then the mean and variance of the features $\{F_{i}(q)\}_{i&#x3D;0}^{N-1}$ from different views to capture the global photographic consistency information. Each feature $F_i(q)$ is concatenated with the mean and variance together, and then fed into a tiny MLP network to generate a new feature $F’(q)$. Next, the new feature $F’(q)$, the viewing direction of the query ray relative to the viewing direction of the $i$-th input image $\Delta d_i&#x3D;d−d_i$ , and the trilinearly interpolated volume encoding feature $M(q)$ into an MLP network $f_c$ to generate blending weight: $w^q_i &#x3D; f_c(F’_i(q), M(q), \Delta d_i)$. Finally, blending weights $\{w^q_i\}^{N−1}_{i&#x3D;0}$ are normalized using a Softmax operator.</p><h4 id="Color-Blending"><a href="#Color-Blending" class="headerlink" title="Color Blending"></a>Color Blending</h4><p><strong>Pixel-based.</strong> With the obtained blending weights, the color $c_q$ of a 3D location $q$ is predicted as the weighted sum of its projected colors $\{I_i(q)\}^{N−1}_{i&#x3D;0}$ on the input images. The color and SDF values of 3D points sampled on the ray are predicted to render the color of the query ray. The color and SDF values of the sampled points are aggregated to obtain the final colors of the ray using SDF based volume rendering. Although supervision on the colors rendered by pixel-based blending already induces effective geometry reasoning, the information of a pixel is local and lacks contextual information, thus usually leading to inconsistent surface patches when input is sparse.</p><p><strong>Patch-based.</strong> To enforce the synthesized colors and ground truth colors to be contextually consistent not only in pixel level but also in patch level and reduce the amount of computation, SparseNeuS leverages local surface plane assumption and homography transformation to achieve a more efficient implementation.</p><p>The key idea is to estimate a local plane of a sampled point to efficiently derive the local patch. Given a sampled point $q$, the normal direction $n_q$ can be estimated by compute the spatial gradient, i.e., $n_q &#x3D; \Delta s(q)$. Then, SparseNeuS samples a set of points on the local plane $(q, n_q)$, projects the sampled points to each view, and obtains the colors by interpolation on each input image. All the points on the local plane share the same blending weights with $q$, and thus only one query of the blending weights is needed. The local plane assumption,which considers the neighboring geometric information of a query 3D position, encodes contextual information of local patches and enforces better geometric consistency. By adopting patch-based volume rendering, synthesized regions contain more global information than single pixels, thus producing more informative and consistent shape context, especially in the regions with weak texture and changing intensity.</p><h4 id="Volume-Rendering"><a href="#Volume-Rendering" class="headerlink" title="Volume Rendering"></a>Volume Rendering</h4><p>Volume rendering is operated similarly to NeRF:</p><p>$$<br>U(r) &#x3D; \sum_{i&#x3D;1}^{M}T_{i}(1-\exp(\sigma_{i}))u_i, \quad \text{where}\quad T_i&#x3D;\exp(-\sum_{j&#x3D;1}^{i-1}\sigma_i)<br>$$</p><p>Where $r$ represents a certain ray, $U(r)$ denotes Pixel-based color $C(r)$ or Patch-based color $P(r)$, $u_i$ is similar to $U(r)$ except $i$ representing a certain point sampled on the ray and densities $\sigma_i$ is converted from sdf values $s_i$ by Neus.</p><h3 id="Per-scene-Fine-tuning"><a href="#Per-scene-Fine-tuning" class="headerlink" title="Per-scene Fine-tuning"></a>Per-scene Fine-tuning</h3><p>To avoid inaccurate outliers and lack of subtle details caused by the limited information in the sparse input views and the high diversity and complexity of different scenes, a novel fine-tuning scheme, which is conditioned on the inferred geometry, is used to reconstruct subtle details and generate finer-grained surfaces. </p><h4 id="Fine-tuning-Networks"><a href="#Fine-tuning-Networks" class="headerlink" title="Fine-tuning Networks"></a>Fine-tuning Networks</h4><p>In the fine-tuning, SparseNeuS directly optimizes the obtained fine-level geometry encoding volume and the signed distance function (SDF) network $f_{\theta}$, while the 2D feature extraction network and 3D sparse CNN networks are discarded. Moreover, the CNN based blending network used in the generic setting is replaced by a tiny MLP network. Although the CNN based network can be also used in per-scene fine-tuning,  a new tiny MLP can speed up the fine-tuning without loss of performance since the MLP is much smaller than the CNN based network. The MLP network still outputs blending weights $\{w^q_i\}^{N−1}_{i&#x3D;0}$ of a query 3D position $q$, but it takes the input as the combination of 3D coordinate $q$, the surface normal $n_q$, the ray direction $d$, the predicted SDF $s(q)$, and the interpolated feature of the geometry encoding volume $M(q)$. Specially, positional encoding PE is applied on the 3D position $q$ and the ray direction $d$. The MLP network $f’_c$ is defined as $\{w^q_i\}^{N−1}_{i&#x3D;0}&#x3D; f’_c(\text{PE}(q),\text{PE}(d), nq, s(q), M(q))$, where $\{w^q_i\}^{N−1}_{i&#x3D;0}$ are the predicted blending weights, and $N$ is the number of input images.</p><h4 id="Consistency-aware-Color-Loss"><a href="#Consistency-aware-Color-Loss" class="headerlink" title="Consistency-aware Color Loss"></a>Consistency-aware Color Loss</h4><p>in multi-view stereo, 3D surface points often do not have consistent projections across different views, since the projections may be occluded or contaminated by image noises. As a result, the errors of these regions suffer from sub-optima, and the predicted surfaces of the regions are always inaccurate and distorted. To tackle this problem, a consistency-aware color loss is used to automatically detect the regions lacking consistent projections and exclude these regions in the optimization:</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}     \mathcal{L}_{color} = & \sum_{r\in\mathbb{R}}O\left(r\right)\cdot\mathcal{D}_{pix}\left(C\left(r\right),\tilde{C}\left(r\right)\right)+\sum_{r\in\mathbb{R}}O\left(r\right)\cdot\mathcal{D}_{pat}\left(P\left(r\right),\tilde{P}\left(r\right)\right)\\    & +\lambda_{0}\sum_{r\in\mathbb{R}}log\left(O\left(r\right)\right)+\lambda_{1}\sum_{r\in\mathbb{R}}log\left(1-O\left(r\right)\right)    \end{aligned}    </span></div><p>Where $\mathbb{R}$ is the set of all query rays, $O (r)$ is the sum of accumulated weights along the ray $r$ obtained by volume rendering, $C (r)$ and $\hat{C} (r)$ are the rendered and ground truth pixel-based colors of the query ray respectively, $P (r)$ and $\hat{P} (r)$ are the rendered and ground truth patch-based colors of the query ray respectively, and $D_{\text{pix}}(L1 Loss)$ and $D_{\text{pat}}$(NCC Loss) are the loss metrics of the rendered pixel color and rendered patch colors respectively.</p><p>The rationale behind this formulation is, the points with inconsistent projections always have relatively large color errors that cannot be minimized in the optimization. Therefore, if the color errors are difficult to be minimized in optimization, the loss forces the sum of the accumulated weights $O (r)$ to be zero, such that the inconsistent regions will be excluded in the optimization. To control the level of consistency, SparseNeuS uses two logistic regularization terms: decreasing the ratio $\frac{\lambda_1}{\lambda_0}$ will lead to more regions being kept; otherwise, more regions are excluded and the surfaces are cleaner.</p><h3 id="Training-Loss"><a href="#Training-Loss" class="headerlink" title="Training Loss"></a>Training Loss</h3><p>By enforcing the consistency of the synthesized colors and ground truth colors, the training of SparseNeuS does not rely on 3D ground-truth shapes. The overall loss function is defined as a weighted sum of the three loss terms:</p><p>$$<br>\mathcal{L}&#x3D;\mathcal{L}_{\text{color }}+\alpha\mathcal{L}_{\text{eik}}+\beta\mathcal{L}_{\text{sparse}}<br>$$</p><p>Actually, in the early stage of generic training, the estimated geometry is relatively inaccurate, and 3D surface points may have large errors, where the errors do not provide clear clues on whether the regions are radiance consistent or not. Thus, only duiring the stage of generic training, an Eikonal term is applied on the sampled points to regularize the SDF values derived from the surface prediction network $f_{\theta}$ :</p><p>$$<br>\mathcal{L}_{eik}&#x3D;\frac{1}{\left|\mathbb{Q}\right|}\sum_{q\in\mathbb{Q}}\left({\left|\nabla f_{\theta}\left(q\right)\right|}_{2}-1\right)^{2}<br>$$</p><p>and a sparseness regularization term to penalize the uncontrollable free surfaces caused by lack of supervising the invisible query samples behind the visible surfaces to enable compact geometry surfaces:</p><p>$$<br>\mathcal{L}_{sparse}&#x3D;\frac{1}{\left|\mathbb{Q}\right|}\sum_{q\in\mathbb{Q}}\exp\left(-\tau\cdot\left|s(q)\right|\right)<br>$$</p><p>Where $\tau$ is a hyperparamter to rescale the SDF value.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2206.05737">Long, X., Lin, C., Wang, P., Komura, T., &amp; Wang, W. (2022, October). Sparseneus: Fast generalizable neural surface reconstruction from sparse views. In European Conference on Computer Vision (pp. 210-227). Cham: Springer Nature Switzerland.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VSCode上用debugpy搭配time调试DDP程序</title>
    <link href="/2023/09/30/vsc_ddpdebug/"/>
    <url>/2023/09/30/vsc_ddpdebug/</url>
    
    <content type="html"><![CDATA[<h2 id="VSCode上用debugpy搭配time调试DDP程序"><a href="#VSCode上用debugpy搭配time调试DDP程序" class="headerlink" title="VSCode上用debugpy搭配time调试DDP程序"></a>VSCode上用debugpy搭配time调试DDP程序</h2><p>由于不少任务并不是通过python直接启动的，尤其是使用slurm等方式调度的集群，由于机卡分离，难以直接从vscode启动调试；DDP由于其多进程的特性，调试起来也十分麻烦。debugpy与time搭配使用算是这种条件下一个不错的调试方案。</p><p>launch.json基本配置</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-comment">// 使用 IntelliSense 了解相关属性。 </span><br>    <span class="hljs-comment">// 悬停以查看现有属性的描述。</span><br>    <span class="hljs-comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span><br>    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;0.2.0&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;configurations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Python: 远程调试&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;python&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;request&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;attach&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;listen&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;host&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;0.0.0.0&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-attr">&quot;port&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">5678</span><br>            <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;pathMappings&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>                <span class="hljs-punctuation">&#123;</span><br>                    <span class="hljs-attr">&quot;localRoot&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="hljs-punctuation">,</span> <br>                    <span class="hljs-attr">&quot;remoteRoot&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;.&quot;</span><br>                <span class="hljs-punctuation">&#125;</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>在需要调试的程序内加入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> debugpy<br>...<br><span class="hljs-keyword">if</span> trainer.global_rank == <span class="hljs-number">0</span>:<br>    debugpy.connect([<span class="hljs-string">&#x27;$&#123;IP_ADDRESS&#125;&#x27;</span>, <span class="hljs-number">5678</span>]) <span class="hljs-comment"># ip地址为launch.json所在的机器地址</span><br><span class="hljs-keyword">else</span>:<br>    time.sleep(...)<br>...<br></code></pre></td></tr></table></figure><p>通过在适当位置加入time.sleep来防止其他主进程继续运行导致主进程断开与vscode的连接。另外也可以在所有进程共用的部分使用time.sleep，不过这样只能调试sleep之前的部分。</p><p>调试时先在vscode上左上角开启调试，之后终端运行程序等待连接即可。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/560405414">Debugpy——如何使用VSCode调试无法直接执行的Python程序</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VS Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VSCode 自定义背景</title>
    <link href="/2023/09/19/vsc_bg/"/>
    <url>/2023/09/19/vsc_bg/</url>
    
    <content type="html"><![CDATA[<h2 id="VSCode-自定义背景"><a href="#VSCode-自定义背景" class="headerlink" title="VSCode 自定义背景"></a>VSCode 自定义背景</h2><p>打开 <code>workbench.desktop.main.css</code>, 加入以下内容</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">body</span>&#123;<br>    <span class="hljs-attribute">background-image</span>: <span class="hljs-built_in">url</span>(<span class="hljs-string">$&#123;PATH_TO_YOUT_BACKGROUND&#125;</span>); <span class="hljs-comment">/* 背景图片路径 */</span><br>    <span class="hljs-attribute">background-repeat</span>: no-repeat;<br>    <span class="hljs-attribute">background-position</span>: center;<br>    <span class="hljs-attribute">background-size</span>: cover; <span class="hljs-comment">/* 可以换成100%等 */</span><br>    <span class="hljs-attribute">opacity</span>: <span class="hljs-number">0.80</span>; <span class="hljs-comment">/* 推荐 */</span><br>&#125;<br></code></pre></td></tr></table></figure><p>之后重启VSCode即可</p>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VS Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LoRA:Low-Rank Adaptation of LLMs</title>
    <link href="/2023/09/17/LoRA/"/>
    <url>/2023/09/17/LoRA/</url>
    
    <content type="html"><![CDATA[<h2 id="LoRA-Low-Rank-Adaptation-of-LLMs"><a href="#LoRA-Low-Rank-Adaptation-of-LLMs" class="headerlink" title="LoRA:Low-Rank Adaptation of LLMs"></a>LoRA:Low-Rank Adaptation of LLMs</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by a factor of 10,000 and the GPU memory requirement by a factor of 3. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at <a href="https://github.com/microsoft/LoRA">https://github.com/microsoft/LoRA</a>.</p></blockquote><h3 id="Low-Rank-Parametrize-Update-Matrices"><a href="#Low-Rank-Parametrize-Update-Matrices" class="headerlink" title="Low-Rank-Parametrize Update Matrices"></a>Low-Rank-Parametrize Update Matrices</h3><figure>    <img src="/img/blogs/230917-LoRA/1.png">    <figcaption>Fig.1 LoRA</figcaption></figure><p>The weight matrices of dense layers in a neural network typically have full-rank. According to previous research, when adapting to a specific task, the pre-trained language models have a low “instrisic dimension” and can still learn efficiently despite a random projection to a smaller subspace. Inspired by this, LoRA comes from the hypothesis that the updates to the weights also have a low “intrinsic rank” during adaptation. For a pre-trained weight matrix $W_0 \in \mathbb{R}^{d\times k}$, LoRA constrains its update by representing the latter with a low-rank decomposition $W_0 + \Delta W &#x3D; W_0 + BA$, where $B \in \mathbb{R}^{d\times r},A \in \mathbb{R}^{r\times k}$, and the rank $r \ll \min(d,k)$. During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ contain trainable parameters. Note both $W_0$ and $\Delta W &#x3D; BA$ are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For $h &#x3D; W_0x$, then :</p><p>$$<br>h&#x3D;W_0x+\Delta Wx&#x3D;W_0x+BAx<br>$$</p><p>See Fig.1, A is initialized by random Gaussian initialization and B is zero initialized, so $\Delta W &#x3D; BA$ is zero at the beginning of training. $\Delta Wx$ can be scaled by $\frac{\alpha}{r}$ , where $\alpha$ is a constant in $r$. This scaling helps to reduce the need to retune hyperparameters when $r$ varies.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2106.09685">Hu, J.E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. ArXiv, abs&#x2F;2106.09685.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用移动硬盘作为MacBook的系统盘</title>
    <link href="/2023/09/05/sys_on_pssd_for_macos/"/>
    <url>/2023/09/05/sys_on_pssd_for_macos/</url>
    
    <content type="html"><![CDATA[<h2 id="利用移动硬盘作为MacBook的系统盘"><a href="#利用移动硬盘作为MacBook的系统盘" class="headerlink" title="利用移动硬盘作为MacBook的系统盘"></a>利用移动硬盘作为MacBook的系统盘</h2><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>ROM是可以替换的，RAM是不可或缺的。</p><p>对macbook而言，在同样要多花不少钱的情况下，升级RAM显然比升级ROM性价比要高——毕竟ROM不足可以用移动硬盘解决，但移动硬盘也只能作为U盘一类的东西存贮文件，有些软件（比如AE）的缓存会占用系统不少空间，把这些文件夹放在移动硬盘内可能会导致软件找不到路径，在系统盘大小不够的情况下，将移动硬盘作为外置系统盘不失为一种解决办法，而且成本会低很多（比如到23年9月，macbook的ROM从256G到512G需要人民币1500，够买一个Thunderbolt3&#x2F;USB4硬盘盒和一张2T的三星980Pro移动硬盘了）。</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><blockquote><p>外置SSD最好不要是那种雷电和USB都支持的款式。虽然物理接口上是一样的，但两者的数据交换协议是不同的。我近10年的经验告诉我，苹果在开机的时候似乎系统在自动选择雷电还是USB的时候有些迷惑。如果你的外置同时支持两种协议，估计会导致混乱。</p></blockquote><blockquote><p>安装系统到外置的时候，我个人不建议直接在原来的MacOS上运行安装程序。即使你选择的是外接SSD，在系统里面安装系统，有机会新系统和原系统共用某些核心部分，导致新系统更新和升级有问题。最好的方法是制作USB开机盘，里面有安装程序和下载好的系统。制作USB盘非常容易，1个16GB以上的USB盘，然后直接用苹果自己的command在terminal里面弄就行了。</p></blockquote><blockquote><p>就是外置SSD真的是比内置在绝对性能上差，即使你用上高速NVME配合高速硬盘盒，中间除了SSD本身，还有协议计算带来的损失。不过，外置SSD跑系统并非完全不行，内存16GB以上的，我实际体验觉得没什么问题。因为卡这一点几乎都是来自于内存不够情况下的swap缓冲。如果你的Mac只有8GB，swap是巨大的，非常不推荐外置跑系统。如果16GB以上，可以试试，除了需要重度读写的软件以外，一般使用足够了，因为很少用到swap。</p></blockquote><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>首先制作USB盘，苹果官方提供了<a href="https://support.apple.com/zh-cn/HT201372">创建可引导的 macOS 安装器</a>的详细步骤。如果终端执行命令的过程中出现 <code>APFS disks may not be used as bootable install media.</code>，需要将磁盘格式化为 MacOS 拓展格式，可以在分区里找。</p><ol><li>确保电源连接，重启</li><li>重启时按住<code>option</code>，直到出现卷宗选择界面，选择刚才安装的卷宗，继续</li><li>确保做好时间机器备份后抹除需要安装的磁盘分区</li><li>返回安装界面，左上角实用工具-&gt;启动安全性实用工具，改为无安全性+允许外部介质或可移动介质启动</li><li>进行安装，选择要安装的磁盘，后通过迁移助理恢复数据</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://support.apple.com/zh-cn/HT201372">创建可引导的 macOS 安装器</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://www.bilibili.com/video/BV1NP411B7vt/?spm_id_from=333.880.my_history.page.click&vd_source=e3817da2068f0aeb6cd83ad63f8c667d">Mac系统安装到外置固态硬盘，丐版Mac瞬间扩容至2T</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://www.bilibili.com/video/BV1pu411h7Hr/?spm_id_from=333.337.search-card.all.click&vd_source=e3817da2068f0aeb6cd83ad63f8c667d">丐版 Mac Mini 扩容 2TB 教程，500块搞定固态+硬盘盒</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>macOS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zero-1-to-3:Zero-shot One Image to 3D Object</title>
    <link href="/2023/08/24/zero1to3/"/>
    <url>/2023/08/24/zero1to3/</url>
    
    <content type="html"><![CDATA[<h2 id="Zero-1-to-3"><a href="#Zero-1-to-3" class="headerlink" title="Zero-1-to-3"></a>Zero-1-to-3</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>We introduce Zero-1-to-3, a framework for changing the camera viewpoint of an object given just a single RGB image. To perform novel view synthesis in this under-constrained setting, we capitalize on the geometric priors that large-scale diffusion models learn about natural images. Our conditional diffusion model uses a synthetic dataset to learn controls of the relative camera viewpoint, which allow new images to be generated of the same object under a specified camera transformation. Even though it is trained on a synthetic dataset, our model retains a strong zero-shot generalization ability to out-of-distribution datasets as well as in-the-wild images, including impressionist paintings. Our viewpoint-conditioned diffusion approach can further be used for the task of 3D reconstruction from a single image. Qualitative and quantitative experiments show that our method significantly outperforms state-of-the-art single-view 3D reconstruction and novel view synthesis models by leveraging Internet-scale pre-training.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/230824-zero1to3/1.png">    <figcaption>Fig.1 Zero-1-to-3</figcaption></figure><p>Given a single RGB image $x \in \mathbb{R}^{H\times W\times 3}$ of an object, the model synthesizes an image of the object from a different camera viewpoint:<br>$$<br>\hat{x}_{R, T} &#x3D; f(x, R, T)<br>$$<br>where $\hat{x}_{R,T}$ is the synthesized image and ${R} \in \mathbb{R}^{3\times3}$ and ${T} \in \mathbb{R}^{3}$ are the relative camera rotation and translation of the desired viewpoint, respectively.</p><h3 id="Learning-to-Control-Camera-Viewpoint"><a href="#Learning-to-Control-Camera-Viewpoint" class="headerlink" title="Learning to Control Camera Viewpoint"></a>Learning to Control Camera Viewpoint</h3><p>Since diffusion models have been trained on internetscale data, their support of the natural image distribution likely covers most viewpoints for most objects, but these viewpoints cannot be controlled in the pre-trained models. Zero123 aims to o teach the model a mechanism to control the camera extrinsics with which a photo is captured so that unlock the ability to perform novel view synthesis. To this end, given a dataset of paired images and their relative camera extrinsics $\{x, x_{(R, T)}, R, T\}$, Zero123 is fine-tuned from a pre-trained diffusion model Stable Diffusion. This fine-tuning allows controls to be “bolted on” and the diffusion model can retain the ability to generate photorealistic images, except now with control of viewpoints. This compositionality establishes zero-shot capabilities in the model, where the final model can synthesize new views for object classes that lack 3D assets and never appear in the fine-tuning set.</p><h3 id="View-Conditioned-Diffusion"><a href="#View-Conditioned-Diffusion" class="headerlink" title="View-Conditioned Diffusion"></a>View-Conditioned Diffusion</h3><p>3D reconstruction from a single image requires both lowlevel perception (depth, shading, texture, etc.) and highlevel understanding (type, function, structure, etc.). Therefore, On one hand, a CLIP embedding of the input image is concatenated with $(R, T)$ to form a “posed CLIP” embedding $c(x, R, T)$ as the condition of diffusion U-Net, which provides high-level semantic information of the input image. On the other hand, the input image is channel-concatenated with the image being denoised, assisting the model in keeping the identity and<br>details of the object being synthesized. To be able to apply classifier-free guidance the input image and the posed CLIP embedding are setting a null vector randomly, and the conditional information is scaled during inference.</p><figure>    <img src="/img/blogs/230824-zero1to3/2.png" width=400>    <figcaption>Fig.2 Spherical Coordinate System</figcaption></figure><p>In fact,  a spherical coordinate system is used to represent camera locations and their relative transformations. Thus, During training, when two images from different viewpoints are sampled, let their camera locations be $(\theta_1, \phi_1, r_21)$ and $(\theta_2, \phi_2, r_2)$. Their relative camera transformation is $(\theta_1-\theta_2, \phi_1-\phi_2, r_1-r_2)$. What’s more, the azimuth angle is encoded with $\phi \rightarrow [\sin (\phi), \cos (\phi)]$ due its incontinuity. So actually, the image CLIP embedding (dimension 768) and the pose vector $[\Delta \theta, \sin (\Delta \phi), \cos (\Delta \phi), \Delta r]$ are concatenated and and fed into another fully-connected layer $(772 \rightarrow 768)$ to ensure compatibility with the diffusion model architecture. The learning rate of this layer is scaled up to be $10\times$ larger than the other layers.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2303.11328">Liu, R., Wu, R., Hoorick, B.V., Tokmakov, P., Zakharov, S., &amp; Vondrick, C. (2023). Zero-1-to-3: Zero-shot One Image to 3D Object. ArXiv, abs&#x2F;2303.11328.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NeuralField-LDM</title>
    <link href="/2023/08/19/NFLDM/"/>
    <url>/2023/08/19/NFLDM/</url>
    
    <content type="html"><![CDATA[<h2 id="NeuralField-LDM"><a href="#NeuralField-LDM" class="headerlink" title="NeuralField-LDM"></a>NeuralField-LDM</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Automatically generating high-quality real world 3D scenes is of enormous interest for applications such as virtual reality and robotics simulation. Towards this goal, we introduce NeuralField-LDM, a generative model capable of synthesizing complex 3D environments. We leverage Latent Diffusion Models that have been successfully utilized for efficient high-quality 2D content creation. We first train a scene auto-encoder to express a set of image and pose pairs as a neural field, represented as density and feature voxel grids that can be projected to produce novel views of the scene. To further compress this representation, we train a latent-autoencoder that maps the voxel grids to a set of latent representations. A hierarchical diffusion model is then fit to the latents to complete the scene generation pipeline. We achieve a substantial improvement over existing state-of-the-art scene generation models. Additionally, we show how NeuralField-LDM can be used for a variety of 3D content creation applications, including conditional scene generation, scene inpainting and scene style manipulation.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/230819-NFLDM/1.png">    <figcaption>Fig.1 NeuralField-LDM</figcaption></figure><blockquote><p>Overview of NeuralField-LDM. We first encode RGB images with camera poses into a neural field represented by density and feature voxel grids. We compress the neural field into smaller latent spaces and fit a hierarchical latent diffusion model on the latent space. Sampled latents can then be decoded into a neural field that can be rendered into a given viewpoint.</p></blockquote><h3 id="Scene-Auto-Encoder"><a href="#Scene-Auto-Encoder" class="headerlink" title="Scene Auto-Encoder"></a>Scene Auto-Encoder</h3><figure>    <img src="/img/blogs/230819-NFLDM/2.png">    <figcaption>Fig.2 Scene Auto-Encoder</figcaption></figure><p>The goal of the scene auto-encoder is to obtain a 3D representation of the scene from input images by learning to reconstruct them. Each input image is processed with a 2D CNN then lifted up to 3D and merged into the shared voxel grids.</p><p>The scene encoder is a 2D CNN and processes each RGB image $i_{1..N}$ separately, producing a $R^{H\times W\times (D+C)}$ dimensional 2D tensor for each image, where $H$ and $W$ are smaller than $i$’s size. Then a discrete frustum of size $H \times  W \times  D$ with the camera poses $\mathcal{k}$ for each image is defined. Take the $d’$th channel of the CNN’s output at pixel $(h, w)$ as the density value of the frustum entry at $(h, w, d)$. The occupancy weight $O$ of each element $(h, w, d)$ in the frustum using the density values $\sigma \geq 0$ is :</p><p>$$<br>O(h, w, d) &#x3D; \exp (-\sum_{j&#x3D;0}^{d-1}\sigma_{(h, w, j)}\delta_{j})(1-\exp (-\sigma_{d}\delta_{d}))<br>$$</p><p>where $\delta_j$ is the distance between each depth in the frustum. Using the occupancy weights, put the last $C$ channels of the CNN’s output into the frustum $F$ to get:</p><p>$$<br>F(h, w, d) &#x3D; [ O(h, w, d)\phi(h, w), \sigma(h, w, d) ]<br>$$</p><p>where $\phi(h, w)$ denotes the $C$-channeled feature vector at pixel $(h, w)$ which is scaled by $O(h, w, d)$ for $F$ at depth $d$. After the frustum of each view being constructed, they are transformed into world coordinates and fused into a shared 3D neural field $V$ which contains both $V_{\text{feat}}$ and $V_{\text{density}}$. For each voxel indexed by $(x, y, z)$, the density and feature are defined as mean of the corresponding frustum entries. 2D features are extracted by using the camera poses $\mathcal{k}$ to project on $V$ and then fed into a CNN decoder that produces the output image.</p><p>The scane auto-encoding pipeline is trained with an image reconstruction loss and a depth supervision loss.</p><h3 id="Latent-Voxel-Auto-Encoder"><a href="#Latent-Voxel-Auto-Encoder" class="headerlink" title="Latent Voxel Auto-Encoder"></a>Latent Voxel Auto-Encoder</h3><p>$V_{\text{feat}}$ and $V_{\text{density}}$ are concatenated along the channel dimension and thein encoded into a hierarchy of three latents: 1D global latent $g$, 3D coarse latent $c$, and 2D fine latent $f$. The intuition for this design is that $g$ is responsible for representing the global properties of the scene, such as the time of the day, $c$ represents coarse 3D scene structure, and $f$ is a 2D tensor with the same horizontal size $X × Y$ as $V$ , which gives further details for each location $(x, y)$ in BEV perspective. The camera trajectory information are concatenated to $g$ and also being learned to sample.</p><p>The encoder is a 2D CNN taking $V$ as input, the vertical axis of which is concatenated along the channel dimension. The decoder uses conditional group normalization layers with $g$ as the conditioning variable. LAE is trained with the voxel reconstruction loss along with the image reconstruction loss with a fixed scene auto-encoder.</p><h3 id="Hierarchical-Latent-Diffusion-Models"><a href="#Hierarchical-Latent-Diffusion-Models" class="headerlink" title="Hierarchical Latent Diffusion Models"></a>Hierarchical Latent Diffusion Models</h3><figure>    <img src="/img/blogs/230819-NFLDM/3.png" width=400>    <figcaption>Fig.3 Latent Diffusion Models</figcaption></figure><p>Given the latent variables $g$, $c$, $f$ that represent a voxel-based scene representation $V$ ,the generative model based on DDPM is defined as $p(V, g, c, f)&#x3D;p(V|g, c, f)p(f|g, c)p(c|g)p(g)$. $g$ is conditioned by conditional group normalization layers and $c$ is conditioned by being interpolated and concatenated to the input of $p(f|g, c)$. </p><h3 id="Post-Optimizing-Generated-Neural-Fields"><a href="#Post-Optimizing-Generated-Neural-Fields" class="headerlink" title="Post-Optimizing Generated Neural Fields"></a>Post-Optimizing Generated Neural Fields</h3><p>The initially generated voxels, $V$, canbe further optimized by rendering viewpoints from the scene and applying SDS loss on each image independently.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.html">Kim, S., Brown, B., Yin, K., Kreis, K., Schwarz, K., Li, D., Rombach, R., Torralba, A., &amp; Fidler, S. (2023). NeuralField-LDM: Scene Generation With Hierarchical Latent Diffusion Models. In Proceedings of the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR) (pp. 8496-8506).</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ATISS:Autoregressive Transformers for Indoor Scene Synthesis</title>
    <link href="/2023/08/19/ATISS/"/>
    <url>/2023/08/19/ATISS/</url>
    
    <content type="html"><![CDATA[<h2 id="ATISS"><a href="#ATISS" class="headerlink" title="ATISS"></a>ATISS</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation. In this paper, we present ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and  its floor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room rearrangement with any objects specified by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8x faster than existing methods.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/230819-ATISS/1.png">    <figcaption>Fig.1 ATISS</figcaption></figure><p>Suppose a scene comprises an unordered set of $M$ objects $\mathcal{O} &#x3D; \{o_j\}^M_{j&#x3D;1}$ and its floor shape ${\bf F}$. Objects in a scene are represented as labeled 3D bounding boxes and we model them with four random variables that describe their category, size, orientation and location, $o_j &#x3D; \{ {\bf c}_j, {\bf s}_j, {\bf t}_j, {\bf r}_j\}$. The category ${\bf c}_j$ is modeled using a categorical variable over the total number of object categories in the dataset and the size ${\bf s}_j$, location ${\bf t}_j$ and orientation ${\bf r}_j$ are modelled with mixture of logistics distributions.:</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        {\bf \alpha}_j\in \{ {\bf c}_j, {\bf s}_j, {\bf t}_j \} \sim \sum_{k=1}^{K}\pi_{k}^{\alpha}\text{logistic}(\mu_k^{\alpha}, \sigma_{k}^{\alpha})        \end{equation}    </span></div><p>where $\pi_k^{\alpha}$, $\mu_k^{\alpha}$ and $\sigma_k^{\alpha}$ denote the weight, mean and variance of the $k$-th ($K$ is a hyper parameter) logistic distribution used for modeling a certain attribute $\alpha$. </p><p>ATISS consists of four main components: (i) the <strong>layout encoder</strong> that maps the room shape to a feature representation ${\bf F}$, (ii) the <strong>structure encoder</strong> that maps the objects in the scene into per-object context embeddings ${\bf C} &#x3D; \{C_j\}^M_{j&#x3D;1}$ , (iii) the <strong>transformer encoder</strong> that takes ${\bf F}$, ${\bf C}$ and a query embedding ${\bf q}$ and predicts the features ${\bf \hat{q}}$ for the next object to be generated and (iv) the <strong>attribute extractor</strong> that predicts the attributes of the next object to be added in the scene. </p><h3 id="Layout-Encoder"><a href="#Layout-Encoder" class="headerlink" title="Layout Encoder"></a>Layout Encoder</h3><p>The layout encoder is simply a ResNet-18 that extracts a feature representation ${\bf F}\in \mathbb{R}^{64}$ for the top-down orthographic projection of the floor.</p><h3 id="Structure-Encoder"><a href="#Structure-Encoder" class="headerlink" title="Structure Encoder"></a>Structure Encoder</h3><figure>    <img src="/img/blogs/230819-ATISS/3.png" width=400>    <figcaption>Fig.3 Structure Encoder</figcaption></figure><p>The structure encoder predicts the per-object context embeddings ${\bf C}_j$ conditioned on the object attributes. For the object category ${\bf c}_j$ , a learnable embedding  $\lambda(\cdot)$ is used, whereas for the location ${\bf t}_j$ , the size ${\bf s}_j$ and orientation ${\bf r}_j$ , ATISS employs the cosine positional encoding $\gamma (\cdot)$ which is applied separately in each dimension of ${\bf t}_j$ and ${\bf s}_j$ .The output of each embedding layer are concatenated into an 512-dimensional feature vector, which is then mapped to the 64-dimensional per-object context embedding.</p><h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><p>The transformer consists of 4 layers with 8 attention heads. The queries, keys and values have 64 dimensions and the intermediate representations for the MLPs have 1024 dimensions. The input set of the transformer is ${\bf I &#x3D; F} \cup \{C_j\}^M_{j&#x3D;1} $, where $M$ denotes the number of objects in the scene and ${\bf q} \in \mathbb{R}^{64}$ is a learnable object query vector that allows the transformer to predict output features ${\bf \hat{q}} \in \mathbb{R}^{64}$ used for generating the next object to be added in the scene.</p><h3 id="Attribute-Extractor"><a href="#Attribute-Extractor" class="headerlink" title="Attribute Extractor"></a>Attribute Extractor</h3><figure>    <img src="/img/blogs/230819-ATISS/4.png">    <figcaption>Fig.3 Attribute Extractor for Category and Size</figcaption></figure><p>The attribute extractor consists of four MLPs that autoregressively predict the object attributes: object category first, followed by position, orientation and size. The MLP for the object category is a linear layer with 64 hidden dimensions that predicts $C$ class probabilities per object. The MLPs for the location, orientation and size predict the mean, variance and mixing coefficient for the $K$ logistic distributions for each attribute:</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}    c_{\theta}: \mathbb{R}^{64} & \rightarrow \mathbb{R}^{C} & \hat{\bf{q}} \mapsto \hat{\bf{c}} \\    t_{\theta}: \mathbb{R}^{64}\times\mathbb{R}^{L_c} & \rightarrow \mathbb{R}^{3\times 3\times K} & (\hat{\bf{q}}, \lambda(\bf{c})) \mapsto \hat{\bf{t}} \\    r_{\theta}: \mathbb{R}^{64}\times\mathbb{R}^{L_c}\times\mathbb{R}^{L_t} & \rightarrow \mathbb{R}^{1\times 3\times K} & (\hat{\bf{q}}, \lambda(\bf{c}), \gamma(\bf{t})) \mapsto \hat{\bf{r}} \\    s_{\theta}: \mathbb{R}^{64}\times\mathbb{R}^{L_c}\times\mathbb{R}^{L_t}\times\mathbb{R}^{L_r} & \rightarrow \mathbb{R}^{3\times 3\times K} & (\hat{\bf{q}}, \lambda(\bf{c}), \gamma(\bf{t}), \gamma(\bf{r})) \mapsto \hat{\bf{s}} \\    \end{aligned}    </span></div><p>See Fig.3, ${\bf c}$ represents concatenation. Note that the attribute extractor for size in Fig.3 omits rotation ${\bf r}_j$.</p><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><figure>    <img src="/img/blogs/230819-ATISS/2.png">    <figcaption>Fig.3 Training Overview</figcaption></figure><p>While training, given a scene with $M$ objects (coloured squares), we first randomly permute them and then keep the first $T$ objects (here $T &#x3D; 3$). Conditioned on ${\bf C}$ and ${\bf F}$, the network predicts the attribute distributions of the next object to be added in the scene and is trained to maximize the log-likelihood of the $T +1$ object from the permuted scene.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2110.03675">Paschalidou, D., Kar, A., Shugrina, M., Kreis, K., Geiger, A., &amp; Fidler, S. (2021). ATISS: Autoregressive Transformers for Indoor Scene Synthesis. Neural Information Processing Systems.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SDFusion</title>
    <link href="/2023/08/15/SDFusion/"/>
    <url>/2023/08/15/SDFusion/</url>
    
    <content type="html"><![CDATA[<h2 id="SDFusion"><a href="#SDFusion" class="headerlink" title="SDFusion"></a>SDFusion</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>In this work, we present a novel framework built to simplify 3D asset generation for amateur users. To enable interactive generation, our method supports a variety of input modalities that can be easily provided by a human, including images, text, partially observed shapes and combinations of these, further allowing to adjust the strength of each input. At the core of our approach is an encoderdecoder, compressing 3D shapes into a compact latent representation, upon which a diffusion model is learned. To enable a variety of multi-modal inputs, we employ taskspecific encoders with dropout followed by a cross-attention mechanism. Due to its flexibility, our model naturally supports a variety of tasks, outperforming prior works on shape completion, image-based 3D reconstruction, and text-to-3D. Most interestingly, our model can combine all these tasks into one swiss-army-knife tool, enabling the user to perform shape generation using incomplete shapes, images, and textual descriptions at the same time, providing the relative weights for each input and facilitating interactivity. Despite our approach being shape-only, we further show an efficient method to texture the generated shape using largescale text-to-image models.</p></blockquote><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><figure>    <img src="/img/blogs/230815-SDFusion/1.png">    <figcaption>Fig.1 SDFusion</figcaption></figure><p>See Fig.1 . SDFusion takes signed distance function (SDFs) as input,  which are known to represent well highresolution shapes with arbitrary topology. To side-step the issue that 3D representations are infamous for demanding high computational resources, an auto-encoder is utilized to compress 3D shapes into a more compact low-dimensional representation. Diffusion models are used to learn the probability distribution over the introduced latent space. Furthermore, SDFusion adopt taskspecific encoders and a cross-attention mechanism to support multiple conditioning inputs, and apply classifierfree guidance to enable flexible conditioning usage.</p><h3 id="3D-Shape-Compression-of-SDF"><a href="#3D-Shape-Compression-of-SDF" class="headerlink" title="3D Shape Compression of SDF"></a>3D Shape Compression of SDF</h3><p>To compress the 3D shape $\bf{X}$ into a lower-dimensional yet compact latent space, SDFusion leverages a 3D-variant of the VQ-VAE Specifically, the employed 3D VQ-VAE contains an encoder $E_{\varphi}$ to encode the 3D shape into the latent space, and a decoder $D_{\tau}$ to decode the latent vectors back to 3D space. Given an input shape represented via the T-SDF $\bf{X}\in \mathbb{R}^{D\times D\times D}$, then<br>$$<br>\displaystyle{\bf z}&#x3D;E_{\varphi}({\bf X}),\quad\text{and}\quad{\bf X}^{\prime}&#x3D;D_{\tau}(\text{VQ}({\bf z}))<br>$$</p><p>where $\bf{z} \in \mathbb{R}^{d\times d\times d}$ is the latent vector, latent dimension $d$ is smaller than 3D shape dimension $D$, and $\text{VQ}$ is the quantization step which maps the latent variable $\bf{z}$ to the nearest element in the codebook $\mathcal{Z}$. The encoder $E_{\varphi}$, decoder $D_{\tau}$ , and codebook $\mathcal{Z}$ are jointly optimized. The VQ-VAE is pre-trained with reconstruction loss, commitment loss, and VQ objective.</p><h3 id="Latent-Diffusion-Model-for-SDF"><a href="#Latent-Diffusion-Model-for-SDF" class="headerlink" title="Latent Diffusion Model for SDF"></a>Latent Diffusion Model for SDF</h3><p>Using the trained encoder Eφ, SDFs are encoded into a compact and low-dimensional latent variable ${\bf z}&#x3D;E_{\varphi}({\bf X})$. Then a timeconditional 3D UNet $\epsilon_\theta$ is chosed as denoising model. </p><h3 id="Learning-the-Conditional-Distribution"><a href="#Learning-the-Conditional-Distribution" class="headerlink" title="Learning the Conditional Distribution"></a>Learning the Conditional Distribution</h3><p>SDFusion incorporats multiple conditional input modalities at once with task-specific encoders $E_{\varphi}$ and a cross-attention module. To further allow for more flexibility in controlling the distribution, classifier-free guidance is adopted to for conditional generation. The objective function reads as follows:</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}    L(\theta , \{\phi_i\}) := \mathop {\mathbb{ E }} \limits_{\bf{z} , \bf{c} , \epsilon , t} \left [ ||{\epsilon - \epsilon _\theta ({\bf{z}}_t, t, F\{D\circ E{\phi_i}({\bf{c}}_i)\}) || }^2 \right ]    \end{aligned}    </span></div><p>where $E{\phi_i}(\bf{c}_i)$ is the task-specific encoder for the $i$th modality, $D$ is a dropout operation enabling classifier-free guidance, and $F$ is a feature aggregation function. In this work, $F$ refers to a simple concatenation.</p><p>At inference time, given conditions from multiple modalities, then</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}     &\epsilon_\theta ({\bf{z}}_t, t, F\{E{\phi_i}~\forall i\}) = \epsilon_\theta ({\bf{z}}_t, t, \boldsymbol {\emptyset })+\\ & \sum_i s_i\left (\epsilon_\theta ({\bf{z}}_t, t, F\{E{\phi_i}({\bf{c}}_i), E{\phi _j}({\bf{c}}_j) : {\bf{c}}_j= \boldsymbol {\emptyset } ~\forall j\neq i\}) \right .\\ &\left .- \epsilon \theta ({\bf{z}}_t, t, \boldsymbol {\emptyset })\right )    \end{aligned}    </span></div><p>where $s_i$ denotes the weight of conditions from the $i$th modality and $\phi$ denotes a condition filled with zeros. For shape completion, given a partial observation of a shape, SDFusion performs blended diffusion. For single-view 3D reconstruction, SDFusion adopts CLIP as the image encoder. For text-guided 3D generation, SDFusion adopts BERT as the text encoder.</p><h3 id="3D-Shape-Texturing-with-a-2D-Model"><a href="#3D-Shape-Texturing-with-a-2D-Model" class="headerlink" title="3D Shape Texturing with a 2D Model"></a>3D Shape Texturing with a 2D Model</h3><figure>    <img src="/img/blogs/230815-SDFusion/2.png">    <figcaption>Fig.2 3D Shape Texturing</figcaption></figure><p>Starting from a generated T-SDF $\bf{X}$, then it is converted to a density tensor $\sigma$ by using VolSDF. After that, a 5D radiance field $F_\theta$ learns to obtain color $\bf{c}$ through scored distillation sampling (SDS) similar to DreamFusion.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2212.04493">Cheng, Y., Lee, H., Tulyakov, S., Schwing, A.G., &amp; Gui, L. (2022). SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation. ArXiv, abs&#x2F;2212.04493.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DreamFusion</title>
    <link href="/2023/08/14/dreamfusion/"/>
    <url>/2023/08/14/dreamfusion/</url>
    
    <content type="html"><![CDATA[<h2 id="DreamFusion"><a href="#DreamFusion" class="headerlink" title="DreamFusion"></a>DreamFusion</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>Applying diffusion models to other modalities has been successful, but requires large amounts of modality-specific training data. 3D assets<br>are currently designed by hand in modeling software like Blender and Maya3D, a process requiring a great deal of time and expertise. Text-to-3D generative models could lower the barrier to entry for novices and improve the workflow of experienced artists.</p></li><li><p>GANs can learn controllable 3D generators from photographs of a single object category, by placing an adversarial loss on 2D image renderings of the output 3D object or scene . Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text.</p></li><li><p>Dream Fields , which uses frozen image-text<br>joint embedding models from CLIP and an optimization-based approach to train NeRFs, showed that pretrained 2D image-text models may be used for 3D synthesis, though 3D objects produced by this approach tend to lack realism and accuracy.</p></li></ul><h3 id="Differential-Image-Parameterization"><a href="#Differential-Image-Parameterization" class="headerlink" title="Differential Image Parameterization"></a>Differential Image Parameterization</h3><p>Diffusion models trained on pixels have traditionally been used to sample only pixels. Dreamfusion wants to create 3D models that look like good images when rendered from random anglesare instead of being interested in sampling pixels.</p><p>Such models can be specified as a differentiable image parameterization (DIP), where a differentiable generator $g$ transforms parameters $\theta$ to create an image $\bf{x} &#x3D; g(\theta)$. DIPs allow us to express constraints, optimize in more compact spaces (e.g. arbitrary resolution coordinate-based MLPs), or leverage more powerful optimization algorithms for traversing pixel space. For 3D, $\theta$ can be parameters of a 3D volume and $g$ can be a volumetric renderer.To learn these parameters, a loss function that can be applied to diffusion models is used.</p><h3 id="Score-Distillation-Sampling"><a href="#Score-Distillation-Sampling" class="headerlink" title="Score Distillation Sampling"></a>Score Distillation Sampling</h3><figure>    <img src="/img/blogs/230814-DreamFusion/2.png">    <figcaption>Fig.1 DDPM and Score Distillation Sampling</figcaption></figure><p>DreamFusion leverages the structure of diffusion models to enable tractable sampling via optimization —— a loss function that, when minimized, yields a sample. DreamFusion optimize over parameters $\theta$ such that $\bf{x} &#x3D; g(\theta)$ looks like a sample from the frozen diffusion model. To perform this optimization, a differentiable loss function where plausible images have low loss, and implausible images have high loss is required.</p><p>While reusing the diffusion training loss to find modes of the learned conditional density $p(\bf{x}|y)$ in high dimensions are often far from typical samples, the multiscale nature of diffusion model training may help to avoid these pathologies.</p><p>Consider the training process of a ddpm which predicts the noise added at a certain timestep. For a pretrained ddpm with true noisy images as input, the predicted noise should be very close to real noise. Inversely, if those noisy images do not look like good images, there will be great difference between predicted noise and real noise.</p><p>Minimizing the diffusion training loss with respect to a generated datapoint $\bf{x} &#x3D; g(\theta)$ gives $ \theta^{*} &#x3D; \text{arg min} _{\theta}\mathcal{L}_{\text{Diff}}(\phi, \bf{x} &#x3D; g(\theta))$ . In practice, this loss function did not produce realistic samples even when using an identity DIP where $\bf{x} &#x3D; \theta$.</p><p>To understand the difficulties of this approach, consider the gradient of $\mathcal{L}_{\text{Diff}}$:</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        \displaystyle\nabla_{\theta}\mathcal{L}_{\text{Diff}}(\phi,\mathbf{x}=g(\theta))=\mathbb{E}_{t,\epsilon}\Bigg{[}w(t)\underbrace{\left(\hat{\epsilon}_{\phi}({\mathbf{z}}_{t};y,t)-\epsilon\right)}_{\text{Noise Residual}}\underbrace{\frac{\partial\hat{\epsilon}_{\phi}({\mathbf{z}}_{t};y,t)}{\mathbf{z}_t}}_{\text{U-Net Jacobian}}\quad \underbrace{\frac{\partial\mathbf{x}}{\partial\theta}}_{\text{Generator Jacobian}}\Bigg{]}        \end{equation}    </span></div><p>Where the constant $\alpha _t\bf{I} &#x3D; \partial \bf{z}_t&#x2F;\partial \bf{x}$ is absorbed into $w(t)$. In practice, the U-Net Jacobian term is<br>expensive to compute (requires backpropagating through the diffusion model U-Net), and poorly conditioned for small noise levels as it is trained to approximate the scaled Hessian of the marginal density.Omitting the U-Net Jacobian term leads to an effective gradient for optimizing DIPs with diffusion models:</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        \displaystyle\nabla_{\theta}\mathcal{L}_{\text{SDS}}(\phi,\mathbf{x}=g(\theta))\triangleq\mathbb{E}_{t,\epsilon}\left[w(t)\left(\hat{\epsilon}_{\phi}({\mathbf{z}}_{t};y,t)-\epsilon\right){\partial\mathbf{x}\over\partial\theta}\right]        \end{equation}    </span></div><p>Since the diffusion model directly predicts the update direction,there’s no need to backpropagate through the diffusion model; the model simply acts like an efficient, frozen critic that predicts image-space edits.</p><p>Empirically, the guidance weight $w$ is set to a large value for classifier-free guidance to improves quality.</p><h3 id="The-Dreamfusion-Algorithm"><a href="#The-Dreamfusion-Algorithm" class="headerlink" title="The Dreamfusion Algorithm"></a>The Dreamfusion Algorithm</h3><figure>    <img src="/img/blogs/230814-DreamFusion/1.png">    <figcaption>Fig.2 The Dreamfusion Algorithm</figcaption></figure><p>In DreamFusion, the Imagen model is chosen as the diffusion model.Only the pretrained $64\times 64$ base model (not the super-resolution cascade for generating higher-resolution images) is used with no modifications. To synthesize a scene from text, a model built on mip-NeRF 360 with random weights is initialized, then repeatedly render views of that NeRF from random camera positions and angles, using these renderings as the input to the score distillation loss function that wraps around Imagen.</p><p>Here is the pseudocode of the DreamFusion</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-built_in">params</span> = generator.init()<br>opt_state = optimizer.init(<span class="hljs-built_in">params</span>)<br>diffusion_model = diffusion.load_model()<br><span class="hljs-keyword">for</span> nstep <span class="hljs-keyword">in</span> iterations:<br>    t = <span class="hljs-built_in">random</span>.uniform(<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>)<br>    alpha_t, sigma_t = diffusion_model.get_coeffs(t)<br>    eps = <span class="hljs-built_in">random</span>.<span class="hljs-keyword">normal</span>(img_shape)<br>    x = generator(<span class="hljs-built_in">params</span>, &lt;other arguments&gt;...) <span class="hljs-comment"># Get an image observation.</span><br>    z_t = alpha_t * x + sigma_t * eps <span class="hljs-comment"># Diffuse observation.</span><br>    epshat_t = diffusion_model.epshat(z_t, y, t) <span class="hljs-comment"># Score function evaluation.</span><br>    g = grad(weight(t) * dot(stopgradient[epshat_t - eps], x), <span class="hljs-built_in">params</span>)<br>    <span class="hljs-built_in">params</span>, opt_state = optimizer.update(g, opt_state) <span class="hljs-comment"># Update params with optimizer.</span><br><span class="hljs-literal">return</span> <span class="hljs-built_in">params</span><br></code></pre></td></tr></table></figure><h3 id="Neural-Rendering-of-a-3D-Model"><a href="#Neural-Rendering-of-a-3D-Model" class="headerlink" title="Neural Rendering of a 3D Model"></a>Neural Rendering of a 3D Model</h3><p><strong>Shading.</strong> Traditional NeRF models emit radiance, which is RGB color conditioned on the ray direction from which the 3D point is being observed. In contrast, NeRF MLP in DreamFusion parameterizes the color of the surface itself, which is then lit by an illumination that can be controlled (a process commonly referred to as “shading”). An RGB albedo (the color of the material) is used for each point:</p><p>$$<br>(\tau, \mathbf{\rho}) &#x3D; \text{MLP}(\mathbf{\mu};\theta)<br>$$</p><p>where $\tau$ is volumetric density. Calculating the final shaded output color for the 3D point requires a normal vector indicating the local orientation of the object’s geometry. This surface normal vector can be computed by normalizing the negative gradient of density $\tau$ with respect to the 3D coordinate :$ {n}&#x3D;-\nabla_\tau &#x2F; \left \lVert \nabla_\tau\right\rVert\ $. With each normal n and material albedo $\rho$ , assuming some point light source with 3D coordinate $l$ and color $\ell_{\rho}$ , and an ambient light color $\ell_a$, we can render each point along the ray using diffuse reflectance  to produce a color $\bf{c}$ for each point:</p><p>$$<br>        \mathbf{c}&#x3D;{\rho}\circ\left({\ell}_{\rho}\circ\operatorname{max}\left(0,{n}\cdot({\ell}-{\mu})&#x2F;\left\lVert{\ell}-{\mu}\right\rVert\right)+{\ell}_{a}\right)<br>$$</p><p>With these colors and previously-generated densities, we approximate the volume rendering integral with the same rendering weights $w_i$ used in standard NeRF. It is beneficial to randomly replace the albedo color $\bf{c}$ with white $(1; 1; 1)$ to produce a “textureless” shaded output. This prevents the model from producing a degenerate solution in which scene content is drawn onto flat geometry to satisfy the text conditioning.</p><p><strong>Scene Structure.</strong> Scene is represented within a fixed bounding sphere, and an environment map generated from a second MLP that takes positionally-encoded ray direction as input is used to compute a background color.</p><p><strong>Geometry regularizers.</strong> A regularization penalty $\mathcal{L}_{orient} &#x3D; \sum_{i}stop\_grad(w_i) \max(0;n_i\cdot v)^2$ on the opacity along each ray to prevent unneccesarily filling in of empty space .A modified version of the orientation loss $\mathcal{L}_{opacity} &#x3D;\sqrt{\sum_i (w_i)^2 + 0.01}$ attempts to orient normals away from the camera so that the shading becomes darker</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2209.14988">Poole, B., Jain, A., Barron, J.T., &amp; Mildenhall, B. (2022). DreamFusion: Text-to-3D using 2D Diffusion. ArXiv, abs&#x2F;2209.14988.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://www.bilibili.com/video/BV1CT411q7Pm/?spm_id_from=333.337.search-card.all.click&vd_source=e3817da2068f0aeb6cd83ad63f8c667d">DETAILS about DreamFusion: Text-to-3D using 2D Diffusion</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>AIGC</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Neural Radiance Fields</title>
    <link href="/2023/08/11/NeRF/"/>
    <url>/2023/08/11/NeRF/</url>
    
    <content type="html"><![CDATA[<h2 id="3D-Shape-Representations"><a href="#3D-Shape-Representations" class="headerlink" title="3D Shape Representations"></a>3D Shape Representations</h2><h3 id="Explicit-Representation"><a href="#Explicit-Representation" class="headerlink" title="Explicit Representation"></a>Explicit Representation</h3><p>The description of a scene is explicit, and the 3D representation of the scene can be seen directly, such as mesh, point cloud, voxel and volume which can directly visualize the corresponding scene.</p><p>Explicit representation will cause artifacts such as overlap because as it is usually discrete.</p><h3 id="Implicit-Representation"><a href="#Implicit-Representation" class="headerlink" title="Implicit Representation"></a>Implicit Representation</h3><p>Usually a function is used to describe the scene geometry. Implicit means to use an MLP to simulate the function, such as inputting 3D space coordinates and outputting corresponding geometric information.</p><p>It is a continuous representation that is suitable for large resolution scenes and usually does not require 3D signals for supervision</p><h2 id="Novel-View-Synthesis-NVS"><a href="#Novel-View-Synthesis-NVS" class="headerlink" title="Novel View Synthesis (NVS)"></a>Novel View Synthesis (NVS)</h2><p>Novel View Synthesis (NVS) refers to the problem of capturing a scene from a novel angle Given a dense sampling of views.</p><h2 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h2><figure>    <img src="/img/blogs/230811-nerf/1.png">    <figcaption>Fig.1 NeRF</figcaption></figure><p>NeRF proposed a 5D neural radiance field (MLP) realize the implicit representation of complex scenes. The neural radiance field encodes the entire 3D scene into the parameters of the neural network. </p><p>To be able to render a scene from any new viewpoint, a neural network needs to learn at least the RGB color $(r, g, b)$ and volume density $\sigma$ of each point in space.</p><p>$$<br>(r, g, b, \sigma) &#x3D; \text{MLP}(x, y, z, \theta, \phi)<br>$$</p><p>To optimize the MLP, based on the classic Volume rendering, a differentiable rendering process is proposed (using layered sampling to improve rendering efficiency). A positional encoding is proposed to map 5D coordinates to a high-dimensional space, which is convenient for MLP learning.</p><h3 id="Volume-Rendering"><a href="#Volume-Rendering" class="headerlink" title="Volume Rendering"></a>Volume Rendering</h3><p>Suppose the camera is located at $\mathbf{O}$, and the light direction is $\mathbf{d}$, then the equation of the light is $\mathbf{r}(t)&#x3D;\mathbf{O}+t\mathbf{d}$, where t represents the distance between $\mathbf{O}$ and a certain point on the ray,  its predicted pixel color $C(\mathbf{r})$ is</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        C(\mathbf{r}) = \int_{t_n}^{t_f}T(t)\sigma(\mathbf{r}(t))\mathbf{c}(\mathbf{r}(t),\mathbf{d})dt,\textrm{ where }T(t)=\exp{\left(-\int_{t_n}^{t}\sigma(\mathbf{r}(s))ds\right)}        \end{equation}    </span></div><p>$T(t)$ represents the proportion of light transmitted to the $t$ point, $\sigma(t)\mathrm{d}t$ Indicates the proportion of the light that will be blocked in a small neighborhood near the $t$ point. The multiplication of the two is the proportion of light that reaches $t$ and is blocked at point $t$, and then multiplied by the color $\textbf{c}(\textbf{r}(t), \textbf{d})$ is the contribution of this point to the final color of the light. The integration interval $[t_n, t_f]$ represents the closest intersection point $t_{near}$ and the farthest intersection point $t_{far}$ of the ray with the medium.</p><p>In actual calculation, we need to use discrete sums to approximate the integral value. That is, some discrete points are collected on the light, and their colors are weighted and summed.A stratified sampling approach is used where we partition $[t_n, t_f]$ into $N$ evenly-spaced bins and then draw one sample uniformly at random from within each bin:</p><p>$$<br>t_{i}\sim\mathcal{U}\left[ t_{n}+\frac{i-1}{N}(t_{f}-t_{n}), t_{n}+\frac{i}{N}(t_{f}-t_{n}) \right]<br>$$</p><p>Then we can use these samples to estimate $C(\mathbf{r})$ with the quadrature rule discussed in the volume rendering:</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        \hat{C}(\mathbf{r}) = \sum _{i=1}^{N}T_{i}(1-\exp{(-\sigma _{i}\delta _{i}}))\mathbf{c}_{i}\, \textrm{ where }T_{i}=\exp{\left(-\sum _{j=1}^{i-1}\sigma _{j}\delta _{j}\right)}        \end{equation}    </span></div><p>where $\delta_i &#x3D; t_{i+1} − t_i$ is the distance between adjacent samples. This function for calculating $\hat{C}(\mathbf{r})$ from the set of $(c_i , \sigma_i)$ values is trivially differentiable and reduces to traditional alpha compositing with alpha values $\alpha_i &#x3D;(1-\exp{(-\sigma _{i}\delta _{i}}))$.</p><h3 id="Optimizing-a-Neural-Radiance-Field"><a href="#Optimizing-a-Neural-Radiance-Field" class="headerlink" title="Optimizing a Neural Radiance Field"></a>Optimizing a Neural Radiance Field</h3><h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p>As deep networks are biased towards learning lower frequency functions and mapping the inputs to a higher dimensional space using high frequency functions before passing them to the network enables better fitting of data that contains high frequency variation, the network $F_{\Theta}$ is a composition of two functions ${F_{\mathrm{\Theta}}&#x3D;F^{\prime}_{\mathrm{\Theta}}\circ\gamma}$, Here $\gamma$ is a mapping from $\mathbb{R}$ into a higher dimensional space $\mathbb{R}^{2L}$ the same as used in <a href="https://iks-ran.github.io/2023/07/19/transformer/">transformer</a>, and $F^{\prime}_{\mathrm{\Theta}}$ is still simply a regular MLP.</p><p>This function is applied separately to each of the three coordinate values in $\mathbf{x}$ (which are normalized to lie in $[−1, 1]$) and to the three components of the Cartesian viewing direction unit vector $\mathbf{d}$ (which by construction lie in $[−1, 1]$). In NeRF, $L &#x3D; 10$ for $\mathbf{x}$ and $L &#x3D; 4$ for $\mathbf{d}$.</p><h4 id="Hierarchical-Volume-Sampling"><a href="#Hierarchical-Volume-Sampling" class="headerlink" title="Hierarchical Volume Sampling"></a>Hierarchical Volume Sampling</h4><p>The rendering strategy of densely evaluating the neural radiance field network at $N$ query points along each camera ray is inefficient as free space and occluded regions that do not contribute to the rendered image are still sampled repeatedly. Thus, a hierarchical representation that increases rendering efficiency by allocating samples proportionally to their expected effect on the final rendering is used. </p><p>Instead of just using a single network to represent the scene, NeRF simultaneously optimize two networks: one “coarse” and one “fine”. First sample a set of $N_c$ locations using stratified sampling, and evaluate the “coarse” network at these locations as described before. Given the output of this “coarse” network, a more informed sampling of points are produced along each ray where samples are biased towards the relevant parts of the volume. To do this, the alpha composited color from the coarse network $\hat{C}(\mathbf{r})$ is rewrited as a weighted sum of all sampled colors $c_i$ along the ray:</p><p>$$<br>\hat{C}_{c}(\mathbf{r})&#x3D;\sum_{i&#x3D;1}^{N_{c}}w_{i}c_{i},\quad w_{i}&#x3D;T_{i}(1-\exp{(-\sigma_{i}\delta_{i}))}<br>$$</p><p>Normalizing these weights as $\hat{w}_{i}&#x3D;\frac{w_{i}}{\sum_{j&#x3D;1}^{N_c}w_{j}}$ produces a piecewise-constant PDF along the ray and then sample a second set of $N_f$locations from this distribution using inverse transform sampling, evaluate the “fine” network using all $N_c+N_f$ samples. This procedure allocates more samples to regions expected to contain visible content, addressing a similar goal as importance sampling. In NeRF, $N_c &#x3D; 64$ and $N_f &#x3D; 128$.</p><h4 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h4><figure>    <img src="/img/blogs/230811-nerf/2.png">    <figcaption>Fig.2 MLP in NeRF</figcaption></figure><p>See Fig.2 . Input vectors are shown in green, intermediate hidden layers are shown in blue, output vectors are shown in red, and the number inside each block signifies the vector’s dimension. All layers are standard fully-connected layers, black arrows indicate layers with ReLU activations, orange arrows indicate layers with no activation, dashed black arrows indicate layers with sigmoid activation, and “+” denotes vector concatenation. The positional encoding of the input location $(\gamma(\mathbf{x}))$ is passed through 8 fully-connected ReLU layers, each with 256 channels. An additional layer outputs the volume density $\sigma$ (which is rectified using a ReLU to ensure that the output volume density is nonnegative) and a 256-dimensional feature vector. This feature vector is concatenated with the positional encoding of the input viewing direction $(\gamma(\mathbf{d}))$, and is processed by an additional fully-connected ReLU layer with 128 channels. A final layer (with a sigmoid activation) outputs the emitted RGB radiance at position $\mathbf{x}$, as viewed by a ray with direction $\mathbf{d}$.</p><p>The loss is simply the total squared error between the rendered and true pixel colors for both the coarse and fine renderings:</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        \mathcal{L} = \sum_{\mathbf{r}\in \mathcal{R}}\left[ \left \lVert \hat{C}_{c}(\mathbf{r})-C(\mathbf{r})\right \rVert_{2}^{2}+\left \lVert \hat{C}_{f}(\mathbf{r})-C(\mathbf{r})\right \rVert_{2}^{2}\right]        \end{equation}    </span></div><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2003.08934">Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., &amp; Ng, R. (2020). NeRF. Communications of the ACM, 65, 99 - 106.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://yconquesty.github.io/blog/ml/nerf/">A Surge in NeRF</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/524523357">神经辐射场(NeRF)-代码无痛解读</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://www.zhihu.com/question/526879513">NeRF（神经辐射场）有相关的物理（光学）原理支撑吗？</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Computer Vision</tag>
      
      <tag>Neural Radiance Fields</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>networksetup:macOS上的终端网络配置管理工具</title>
    <link href="/2023/08/10/networksetup/"/>
    <url>/2023/08/10/networksetup/</url>
    
    <content type="html"><![CDATA[<h2 id="networksetup"><a href="#networksetup" class="headerlink" title="networksetup"></a>networksetup</h2><p>networksetup 是macOS自带的，作为 macOS 操作系统的一部分提供给用户的网络配置管理工具, 用户能在终端通过 networksetup 。管理网络设置和配置。 它提供了用于配置网络首选项、接口、代理、DNS 设置等的各种功能。 </p><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">networksetup -<span class="hljs-built_in">help</span><br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 显示服务列表 星号 (*) 表示网络服务已禁用</span><br>networksetup -listallnetworkservices<br><span class="hljs-comment"># 例如</span><br>An asterisk (*) denotes that a network service is disabled.<br>USB 10/100/1000 LAN<br>Wi-Fi<br>Thunderbolt Bridge<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 显示具有相应设备名称和以太网地址的硬件端口列表</span><br>networksetup -listallhardwareports<br><span class="hljs-comment"># 例如</span><br>Hardware Port: Thunderbolt Bridge<br>Device: bridge0<br>Ethernet Address: ********<br><br>Hardware Port: Wi-Fi<br>Device: en0<br>Ethernet Address: ********<br><br>Hardware Port: Thunderbolt 1<br>Device: en1<br>Ethernet Address: ********<br><br>Hardware Port: Thunderbolt 2<br>Device: en2<br>Ethernet Address: ********<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 开启或关闭 &lt;device name&gt; 的Wi-Fi</span><br>networksetup -setairportpower &lt;device name&gt; &lt;on off&gt;<br><span class="hljs-comment"># 查询 &lt;device name&gt; Wi-Fi的开启状态</span><br>networksetup -getairportpower &lt;device name&gt;<br><br><span class="hljs-comment"># 将 &lt;device name&gt; 的 Wi-Fi 网络设置为 &lt;nwtwork&gt;. </span><br><span class="hljs-comment"># 是否包含密码可选，如果包含，将存储在钥匙串中</span><br>networksetup -setairportnetwork &lt;device name&gt; &lt;network&gt; [password]<br><span class="hljs-comment"># 例如 Wi-Fi的设备为en0, 网络名称为ZJUWLAN-Secure, 密码为000000</span><br><span class="hljs-comment"># networksetup -setairportnetwork en0 ZJUWLAN-Secure 000000</span><br><span class="hljs-comment"># 查询 &lt;device name&gt; Wi-Fi连接</span><br>networksetup -getairportnetwork &lt;device name&gt;<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 使用 &lt;domain&gt;(即address) 和 &lt;port number&gt; 设置 &lt;networkservice&gt; 的 http 代理。</span><br><span class="hljs-comment">#（可选）为 &lt;authenticated&gt; 指定 &lt;on&gt; 或 &lt;off&gt; 以启用和禁用经过身份验证的代理支持。 </span><br><span class="hljs-comment"># 如果打开经过身份验证的代理支持，请指定 &lt;用户名&gt; 和 &lt;密码&gt;。</span><br>networksetup -setwebproxy &lt;networkservice&gt; &lt;domain&gt; &lt;port number&gt; &lt;authenticated&gt; &lt;username&gt; &lt;password&gt;<br><span class="hljs-comment"># 例如 networksetup -setwebproxy Wi-Fi 127.0.0.1 20172</span><br><span class="hljs-comment"># 设置 https 代理，可以用 </span><br>networksetup -setsecurewebproxy &lt;networkservice&gt; &lt;domain&gt; &lt;port number&gt; &lt;authenticated&gt; &lt;username&gt; &lt;password&gt;<br><br><span class="hljs-comment"># 将 &lt;networkservice&gt; 的绕过域名服务器设置为 &lt;domain1&gt; [domain2] [...]</span><br><span class="hljs-comment"># 可以指定任意数量的域名服务器 为 &lt;domain1&gt; 指定“Empty”以清除所有域名条目。</span><br>networksetup -setproxybypassdomains &lt;networkservice&gt; &lt;domain1&gt; [domain2] [...]<br><span class="hljs-comment"># 例如 networksetup -setproxybypassdomains Wi-Fi &quot;*.local&quot; &quot;*.cn&quot; 169.254/16</span><br><br><span class="hljs-comment"># 开启或关闭代理</span><br>networksetup -setwebproxystate &lt;networkservice&gt; &lt;on off&gt;<br>networksetup -setsecurewebproxystate &lt;networkservice&gt; &lt;on off&gt;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
    <tags>
      
      <tag>macOS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>StyleGAN:A Style-Based Generator Architecture</title>
    <link href="/2023/08/10/stylegan/"/>
    <url>/2023/08/10/stylegan/</url>
    
    <content type="html"><![CDATA[<h2 id="StyleGAN"><a href="#StyleGAN" class="headerlink" title="StyleGAN"></a>StyleGAN</h2><figure>    <img src="/img/blogs/230810-stylegan/1.png">    <figcaption>Fig.1 StyleGAN Architecture</figcaption></figure><p>Traditionally, a generator feeds the latent code $\mathbf{z}$ in the latent space $\mathcal{Z}$ though the input layer only, but in StyleGAN the input is mapped to an intermediate latent space $\mathcal{W}$ through a non-linear mapping network $\mathcal{f}: \mathcal{Z}\rightarrow \mathcal{W}$ which first produces $\mathbf{w}\in \mathcal{W}$, to control the generator $\mathcal{g}$ through adaptive instance normalization (AdaIN) at each convolution layer, see Fig.1 . Gaussian noise is added after each convolution, before evaluating the nonlinearity. “A” is a learned affine transform, and “B” applies learned per-channel scaling factors to the noise input. The mapping network $\mathcal{f}$ consists of 8 layers and the synthesis network $\mathcal{g}$ consists of 18 layers— two for each resolution $(4^2 − 1024^2)$. The output of the last layer is converted to RGB using a separate $1 \times 1$ convolution.</p><h3 id="Mapping-Network"><a href="#Mapping-Network" class="headerlink" title="Mapping Network"></a>Mapping Network</h3><p>…</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2006.11239">Karras, T., Laine, S., &amp; Aila, T. (2018). A Style-Based Generator Architecture for Generative Adversarial Networks. 2019 IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR), 4396-4405.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Generative Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Denoising Diffusion Probabilistic Models</title>
    <link href="/2023/08/07/ddpm/"/>
    <url>/2023/08/07/ddpm/</url>
    
    <content type="html"><![CDATA[<h2 id="Denoising-Diffusion-Probabilistic-Models"><a href="#Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="Denoising Diffusion Probabilistic Models"></a>Denoising Diffusion Probabilistic Models</h2><h3 id="Basics-of-Probability"><a href="#Basics-of-Probability" class="headerlink" title="Basics of Probability"></a>Basics of Probability</h3><h4 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h4><div style="text-align: center;">    <span style="display: inline-block;">        \begin{aligned}        P(A, B, C)  & = P(C|A, B)P(A, B) = P(C|A, B)P(B|A)P(A) \\        P(B, C|A) & = \frac{P(A, B, C)}{P(A)} = P(C|A, B)P(B|A) \\        \end{aligned}    </span></div><h4 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h4><p>The probability of the current state is only related to the previous moment, for example, considering Markov relationship​ $A \rightarrow B \rightarrow C$, then </p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{aligned}        P(A, B, C)  & = P(C|B)P(B|A)P(A) \\        P(B, C|A) & = P(C|B)P(B|A) \\        \end{aligned}    </span></div><h4 id="Reparameterization"><a href="#Reparameterization" class="headerlink" title="Reparameterization"></a>Reparameterization</h4><p>For a neural network, If we want to sample from the Gaussian distribution $\mathcal{N}(\mu, \sigma^2)$, the result obtained is not differentiable. Thus, we can first sample $\epsilon$ from the standard distribution, and then get $\sigma * \epsilon + \mu$​. Then the randomness is transferred to $\mu$ and $\sigma$, and ​and​ are used as part of the affine transformation network.</p><h3 id="Forward-Diffusion-Process"><a href="#Forward-Diffusion-Process" class="headerlink" title="Forward Diffusion Process"></a>Forward Diffusion Process</h3><figure>    <img src="/img/blogs/230807-ddpm/6.png">    <figcaption>Fig.1 Diffusion Probabilistic Model</figcaption></figure><p>Given a data point sampled from a real data distribution $\mathbf{x}_0 \sim q(\mathbf{x})$, let us define a <em>forward diffusion process</em> in which we add small amount of Gaussian noise to the sample in $T$ steps, producing a sequence of noisy samples $\mathbf{x}_1, \dots, \mathbf{x}_T$. The step sizes are controlled by a variance schedule ${\beta_t \in (0, 1)}_{t&#x3D;1}^T$</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        q(\textbf{x}_t \vert \textbf{x}_{t-1}) := \mathcal{N}(\textbf{x}_t; \sqrt{1 - \beta _t} \textbf{x}_{t-1}, \beta _t\textbf{I}) \quad        q(\textbf{x}_{1:T} \vert \textbf{x}_0) := \prod^T_{t=1} q(\textbf{x}_t \vert \textbf{x}_{t-1})        \end{equation}    </span></div><p>The data sample $\textbf{x}_0$ gradually loses its distinguishable features as the step $t$ becomes larger. Eventually when $T \to \infty$, $\textbf{x}_T$ is equivalent to an isotropic Gaussian distribution.</p><p>Let $\alpha_t &#x3D; 1 - \beta _t$ and $\bar{\alpha}_t &#x3D; \prod _{i&#x3D;1}^t \alpha_i$, then</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{aligned}        \mathbf{x}_t &= \sqrt{\alpha_t}\mathbf{x}_{t-1} + \sqrt{1 - \alpha_t}\boldsymbol{\epsilon}_{t-1} & \text{ ;where } \boldsymbol{\epsilon}_{t-1}, \boldsymbol{\epsilon}_{t-2}, \dots \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \\        &= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \bar{\boldsymbol{\epsilon}}_{t-2} & \text{ ;where } \bar{\boldsymbol{\epsilon}}_{t-2} \text{ merges two Gaussians (*).} \\        &= \dots \\        &= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon} \\        q(\mathbf{x}_t \vert \mathbf{x}_0) &= \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})\end{aligned}    </span></div><p>(*) Recall that when we merge two Gaussians  with different variance, $\mathcal{N}(\mathbf{0}, \sigma _1^2\mathbf{I})$ and $\mathcal{N}(\mathbf{0}, \sigma _2^2\mathbf{I})$, the new distribution is $\mathcal{N}(\mathbf{0}, (\sigma _1^2 + \sigma _2^2)\mathbf{I})$. Here the merged standard deviation is $\sqrt{(1 - \alpha _t) + \alpha _t (1-\alpha _{t-1})} &#x3D; \sqrt{1 - \alpha _t\alpha _{t-1}}$.</p><p>Usually, we can afford a larger update step when the sample gets noisier, so $\beta _1 &lt; \beta _2 &lt; \dots &lt; \beta _T$ and therefore $\bar{\alpha}_1 &gt; \dots &gt; \bar{\alpha}_T$.</p><h3 id="Reverse-Diffusion-Process"><a href="#Reverse-Diffusion-Process" class="headerlink" title="Reverse Diffusion Process"></a>Reverse Diffusion Process</h3><figure>    <img src="/img/blogs/230807-ddpm/4.png">    <figcaption>Fig.2 Diffusion Process</figcaption></figure><p>If we can reverse the above process and sample from $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ , we will be able to recreate the true sample from a Gaussian noise input, $\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ . </p><p>Note that if $\beta _t$ is small enough, $q(\mathbf{x}_t | \mathbf{x}_{t-1})$ will also be Gaussian. </p><p>Unfortunately, we cannot easily estimate $q(\mathbf{x}_t | \mathbf{x}_{t-1})$ because it needs to use the entire dataset and therefore we need to learn a model $p_{\theta}$ to approximate these conditional probabilities in order to run the <em>reverse diffusion process</em>.</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        p_\theta(\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod^T_{t=1} p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) \quad        p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t) := \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))        \end{equation}    </span></div><p>It is noteworthy that the reverse conditional probability is tractable when conditioned on $\mathbf{x}_0$</p><div style="text-align: center;">    <span style="display: inline-block;">        \begin{equation}        q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; {\tilde{\boldsymbol{\mu}}}(\mathbf{x}_t, \mathbf{x}_0), {\tilde{\beta}_t} \mathbf{I})        \end{equation}    </span></div><p>As we know, $\mathcal{N}(\mu, \sigma^2)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$. Using Bayes’ rule, we have</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}        q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)         &= \frac{ q(\mathbf{x}_t, \mathbf{x}_{t-1}, \mathbf{x}_0) }{ q(\mathbf{x}_{t}, \mathbf{x}_0) } =  q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{q(\mathbf{x}_{t-1}, \mathbf{x}_0) }{ q(\mathbf{x}_{t}, \mathbf{x}_0) } \\        &= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } = q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\        &\propto \exp \Big(-\frac{1}{2} \big(\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\        &= \exp \Big(-\frac{1}{2} \big(\frac{\mathbf{x}_t^2 - 2\sqrt{\alpha_t} \mathbf{x}_t {\mathbf{x}_{t-1}} \color{black}{+ \alpha_t} {\mathbf{x}_{t-1}^2} }{\beta_t} + \frac{ {\mathbf{x}_{t-1}^2} \color{black}{- 2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0} {\mathbf{x}_{t-1}} \color{black}{+ \bar{\alpha}_{t-1} \mathbf{x}_0^2}  }{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\        &= \exp\Big( -\frac{1}{2} \big( {(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})} \mathbf{x}_{t-1}^2 - {(\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)} \mathbf{x}_{t-1} \color{black}{ + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}        \end{aligned}    </span></div><p>where $C(\mathbf{x}_t, \mathbf{x}_0)$ is some function not involving $\mathbf{x}_{t-1}$ and details are omitted. Thus, following the standard Gaussian density function, the mean and variance can be parameterized as follows </p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}    \tilde{\beta}_t     &= 1/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})     = 1/(\frac{\alpha_t - \bar{\alpha}_t + \beta_t}{\beta_t(1 - \bar{\alpha}_{t-1})})    = {\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\    \tilde{\boldsymbol{\mu}}_t (\mathbf{x}_t, \mathbf{x}_0)    &= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0)/(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}) \\    &= (\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1} }}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0) {\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \\    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0\\    \end{aligned}    </span></div><p>As discussed above, we can represent $\mathbf{x}_0 &#x3D; \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t)$ and plug it into the above equation and obtain</p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}    \tilde{\boldsymbol{\mu}}_t    &= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\    &= {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \Big)}    \end{aligned}    </span></div><p>where $\epsilon_t$ is sampled from distribution $p_{\theta}$. Thus, </p><div style="text-align: center;">    <span style="display: inline-block;">    \begin{aligned}    \boldsymbol{\mu}_\theta(\mathbf{x}_t, t) &= {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big)} \\    \mathbf{x}_{t-1} &= \mathcal{N}(\mathbf{x}_{t-1}; \frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \Big), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t))    \end{aligned}    </span></div><h3 id="Pytorch-Implementation"><a href="#Pytorch-Implementation" class="headerlink" title="Pytorch Implementation"></a>Pytorch Implementation</h3><figure>    <img src="/img/blogs/230807-ddpm/5.png">    <figcaption>Fig.3 The training and sampling algorithms in DDPM</figcaption></figure><p>Notice</p><ul><li><code>nan</code> and <code>inf</code> may appear if the precision is not enough</li><li>$t$ participates in training through embedding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DDPM</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, T, beta_1, beta_T</span>):<br>        <span class="hljs-built_in">super</span>(DDPM, self).__init__()<br>        self.model = model<br>        self.T = T<br>        self.beta_1 = beta_1<br>        self.beta_T = beta_T<br>        self._init_constant()<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_constant</span>(<span class="hljs-params">self</span>):<br><br>        betas = torch.linspace(self.beta_1, self.beta_T, self.T, dtype=torch.float64)<br>        alphas = <span class="hljs-number">1</span> - betas<br>        cumprod_alphas = torch.cumprod(alphas, dim=<span class="hljs-number">0</span>)<br>        prev_cumprod_alphas = F.pad(cumprod_alphas[:-<span class="hljs-number">1</span>], (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), value = <span class="hljs-number">1.</span>)<br><br>        self.forward_coef1 = torch.sqrt(cumprod_alphas)<br>        self.forward_coef2 = torch.sqrt(<span class="hljs-number">1</span> - cumprod_alphas)<br>        self.reverse_std = torch.sqrt(betas * (<span class="hljs-number">1.</span> - prev_cumprod_alphas) / (<span class="hljs-number">1.</span> - cumprod_alphas))<br>        self.reverse_mean_coef1 = <span class="hljs-number">1</span> / self.forward_coef1<br>        self.reverse_mean_coef2 = self.reverse_mean_coef1 * betas / self.forward_coef2<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x0</span>):<br><br>        b = x0.shape[<span class="hljs-number">0</span>]<br>        t = torch.randint(<span class="hljs-number">0</span>, self.T, size=(b, ))<br>        noise = torch.rand_like(x0)<br>        coef1 = F.embedding(t, self.forward_coef1.unsqueeze(-<span class="hljs-number">1</span>)).unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>        coef2 = F.embedding(t, self.forward_coef2.unsqueeze(-<span class="hljs-number">1</span>)).unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br><br>        xt = coef1 * x0 + coef2 * noise<br>        denoise = self.model(xt, t)<br><br>        <span class="hljs-keyword">return</span> noise, denoise<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, xT</span>):<br><br>        xt = xT<br>        b = xT.shape[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(self.T)):<br>            t = torch.ones(size=(b, ), dtype=torch.long) * step<br>            mean_coef1 = F.embedding(t, self.reverse_mean_coef1.unsqueeze(-<span class="hljs-number">1</span>)).unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>            mean_coef2 = F.embedding(t, self.reverse_mean_coef2.unsqueeze(-<span class="hljs-number">1</span>)).unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>            std = F.embedding(t, self.reverse_std.unsqueeze(-<span class="hljs-number">1</span>)).unsqueeze(-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>            denoise = self.model(xt, t)<br>            mean = mean_coef1 * xt - mean_coef2 * denoise<br>            <span class="hljs-keyword">if</span> step &gt; <span class="hljs-number">0</span>:<br>                noise = torch.randn_like(xt)<br>            <span class="hljs-keyword">else</span>:<br>                noise = <span class="hljs-number">0</span><br>            xt = mean + std * noise<br>        x0 = torch.clip(xt, <span class="hljs-built_in">min</span>=-<span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>=<span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-keyword">return</span> x0<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2006.11239">Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. ArXiv, abs&#x2F;2006.11239.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Weng, Lilian. (Jul 2021). What are diffusion models? Lil’Log.</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/530602852">DDPM解读（一）| 数学基础，扩散与逆扩散过程和训练推理方法</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://github.com/zoubohao/DenoisingDiffusionProbabilityModel-ddpm-">https://github.com/zoubohao/DenoisingDiffusionProbabilityModel-ddpm-</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">https://github.com/lucidrains/denoising-diffusion-pytorch/tree/main</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://space.bilibili.com/373596439/video">54、Probabilistic Diffusion Model概率扩散模型理论与完整PyTorch代码详细解读</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Generative Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Generative Adverserial Networks</title>
    <link href="/2023/07/28/gan/"/>
    <url>/2023/07/28/gan/</url>
    
    <content type="html"><![CDATA[<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><figure>    <img src="/img/blogs/230728-gan/2.png" alt="Fig.1 GAN Architecture" width=200>    <figcaption>Fig.1 GAN Architecture</figcaption></figure><p>The GAN architecture is illustrated in Fig.1 . There are two pieces in GAN architecture —— a generator and a discriminator. The generator is used to generate objects that need to be generated from the latent space The discriminator attempts to distinguish fake and real data from each other. Both networks are in competition with each other. The generator network attempts to fool the discriminator network. At that point, the discriminator network adapts to the new fake data. This information, in turn is used to improve the generator network, and so on.</p><h2 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h2><p>The discriminator is a binary classifier to distinguish if the input is real (from real data) or fake (from the generator). Typically, the discriminator outputs a scalar prediction $o\in \mathbb{R}$ for input $\textbf{x}$, such as using a fully connected layer with hidden size 1, and then applies sigmoid function to obtain the predicted probability $D(\textbf{x}) &#x3D; \frac{1}{1+e^{-o}}$ . Assume the label $y$ for the true data is 1 and 0 for the fake data. Train the discriminator to minimize the cross-entropy loss.</p><p>$$<br>\min\limits_{D}{-y\log{D(\textbf{x})} - (1-y)\log{(1-D(\textbf{x}))} }<br>$$</p><p>For the generator, it first draws some parameter $\textbf{z}\in \mathbb{R}^d$ from a source of randomness, for example, a normal distribution. It then applies a function to generate $x’&#x3D;G(\textbf{z})$. The goal of the generator is to fool the discriminator to classify $x’&#x3D;G(\textbf{z})$ as true data $D(G(\textbf{z}))\approx 1$. In other words, for a given discriminator $D$, update the parameters of the generator $G$ to maximize the cross-entropy loss when $y&#x3D;0$.</p><p>$$<br>\max\limits_{G}{-(1-y)\log{(1-D(G(\textbf{z})))}}&#x3D;\max\limits_{G}{-\log{(1-D(G(\textbf{z})))}}<br>$$</p><p>If the generator does a perfect job, then $D(\textbf{x}’)\approx 1$ , so the above loss is near 0, which results in the gradients that are too small to make good progress for the discriminator. So commonly, minimize the following loss:</p><p>$$<br>\min\limits_{G}{-y\log{D(G(\textbf{z}))}}&#x3D;\min\limits_{G}{-\log{D(G(\textbf{z}))}}<br>$$</p><p>which is just feeding $x’&#x3D;G(\textbf{z})$ into the discriminator but giving label $y&#x3D;1$.</p><p>To sum up $G$, and $D$ are playing a “minimax” game with the comprehensive objective function:<br>$$<br>\min\limits_{D}\max\limits_{D}{ -E_{\text{x$\sim$Data}}\log{D(\textbf{x})}-E_{\text{z$\sim$Noise}}\log{(1-D(G(\textbf{z})))} }<br>$$</p><h2 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h2><figure>    <img src="/img/blogs/230728-gan/3.png">    <img src="/img/blogs/230728-gan/1.png">    <figcaption>Fig.2 Training Procedure</figcaption></figure><p>See Fig.2, GAN are trained by simultaneously updating the discriminative distribution ($D$, blue, dashed line) so that it discriminates between samples from the data generating distribution (black, dotted line) $p_{x}$ from those of the generative distribution $p_g (G)$ (green, solid line). The lower horizontal line is the domain from which $\textbf{z}$ is sampled, in this case uniformly. The horizontal line above is part of the domain of $\textbf{x}$. The upward arrows show how the mapping $\textbf{x} &#x3D; G(\textbf{z})$ imposes the non-uniform distribution $p_g$ on transformed samples. $G$ contracts in regions of high density and expands in regions of low density of $p_g$. (a) Consider an adversarial pair near convergence: $p_g$ is similar to $p_{data}$ and $D$ is a partially accurate classifier. (b) In the inner loop of the algorithm $D$ is trained to discriminate samples from data, converging to $D^{∗}(\textbf{x}) &#x3D;\frac{p_{data}(\textbf{x})}{p_{data}(\textbf{x})+p_g(\textbf{x})}$ . (c) After an update to $G$, gradient of $D$ has guided $G(\textbf{z})$ to flow to regions that are more likely to be classified as data. (d) After several steps of training, if $G$ and $D$ have enough capacity, they will reach a point at which both cannot improve because $p_g &#x3D; p_{data}$. The discriminator is unable to differentiate between the two distributions, i.e. $D(\textbf{x}) &#x3D; \frac{1}{2}$.</p><h3 id="Pytorch-Implementation"><a href="#Pytorch-Implementation" class="headerlink" title="Pytorch Implementation"></a>Pytorch Implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">G, D, dataloader, optimizer_G, optimizer_D, latent_dim, epochs</span>):<br>    loss = nn.BCELoss()<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-keyword">for</span> real, _ <span class="hljs-keyword">in</span> dataloader:<br>            batch_size = real.shape[<span class="hljs-number">0</span>]<br>            REAL = torch.ones(batch_size, <span class="hljs-number">1</span>))<br>            FAKE = torch.zeros(batch_size, <span class="hljs-number">1</span>))<br>            z = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, size=(batch_size, latent_dim))<br>            fake = G(z)<br><br>            loss_d = (loss(D(real), REAL) + loss(D(fake.detach()), FAKE) / <span class="hljs-number">2</span><br>            optimizer_D.zero_grad()<br>            loss_d.backward()<br>            optimizer_D.step()<br><br>            loss_g = loss(D(fake), REAL)<br>            optimizer_G.zero_grad()<br>            loss_g.backward()<br>            optimizer_G.step()<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/1810.04805">Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., &amp; Bengio, Y. (2014). Generative Adversarial Nets. NIPS.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://d2l.ai/index.html">Journal of Machine Learning Research 21 (23), 1-7, 2020. 183, 2020. Dive into deep learning. 2020. A Zhang, ZC Lipton, M Li, AJ Smola.</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Generative Model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vision Transformer</title>
    <link href="/2023/07/27/ViT/"/>
    <url>/2023/07/27/ViT/</url>
    
    <content type="html"><![CDATA[<h2 id="Vision-Transformer"><a href="#Vision-Transformer" class="headerlink" title="Vision Transformer"></a>Vision Transformer</h2><h3 id="Inductive-bias"><a href="#Inductive-bias" class="headerlink" title="Inductive bias"></a>Inductive bias</h3><p>Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution. Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><h4 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h4><p>As the standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, an image $x\in \mathbb{R}^{H\times W\times C}$ is reshaped into a sequence of flattened 2D patches $x_p\in \mathbb{R}^{N\times (P^2\cdot C)}$ , where $N&#x3D;HW&#x2F;P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. A trainable linear projection is added to project for projection into $D$ dimensions.</p><h4 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h4><p>Standard learnable 1D position embeddings are added to the patch embeddings to retain positional information. <em>Dropout is applied after every dense layer except for the the qkv-projections and directly after adding positional- to patch embeddings</em>.</p><h4 id="CLS-Token"><a href="#CLS-Token" class="headerlink" title="[CLS] Token"></a><code>[CLS]</code> Token</h4><p>Similar to BERT’s <code>[CLS]</code> token, a learnable embedding is prepended to the sequence of embedded patches ($z_0^0&#x3D;x_{cls}$), whose state at the output of the Transformer encoder ($z_{L}^0$) serves as the image representation $y$. Both during pre-training and fine-tuning, a classification head is attached to $z_L^0$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.</p><p>The Pytorch implementation of the whole embedding layer is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> einops<br><span class="hljs-keyword">from</span> einops.layers.torch <span class="hljs-keyword">import</span> Rearrange<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ViTEmbeddingLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_patches, patch_height, patch_width, patch_dim, d_model, dropout</span>):<br>        <span class="hljs-built_in">super</span>(ViTEmbeddingLayer, self).__init__()<br>        self.img_to_patch = Rearrange(<span class="hljs-string">&#x27;b c (n_h p_h) (n_w p_w) -&gt; b (n_h n_w) (c p_h p_w)&#x27;</span>, p_h=patch_height, p_w=patch_width)<br>        self.proj = nn.Sequential(nn.LayerNorm(patch_dim), <br>                                  nn.Linear(patch_dim, d_model), <br>                                  nn.LayerNorm(d_model))<br>        self.pos_ebd = nn.Parameters(torch.randn(<span class="hljs-number">1</span>, num_patches + <span class="hljs-number">1</span>, d_model))<br>        seld.cls_ebd = nn.Parameters(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, d_model))<br>        self.drop = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br><br>        patches = self.proj(self.img_to_patch(<span class="hljs-built_in">input</span>))<br><br>        b, n = patches.shape[:<span class="hljs-number">2</span>]<br>        cls = einops.repeat(self.cls_ebd, <span class="hljs-string">&#x27;1 1 d -&gt; b 1 d&#x27;</span>, b=b)<br>        output = torch.cat([cls_tokens, patches], dim=<span class="hljs-number">1</span>) + self.pos_ebd[:, :(n+<span class="hljs-number">1</span>)]<br><br>        <span class="hljs-keyword">return</span> self.drop(output)<br></code></pre></td></tr></table></figure><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>The Transformer encoder in ViT is a little different from the original Transformer. LayerNorm is peformed before each sub-layer and non-linearity in MLP is GELU.</p><div style="text-align: center;">    <span style="display: inline-block;">        \[        \begin{aligned}        z_0 & = [x_{cls};x_p^1E;x_p^2E;...;x_p^NE]+E_{pos}, E\in \mathbb{R}^{(P^2\cdot C)\times D}, E_{pos}\in \mathbb{R}^{(N+1)\times D}\\        z_l' & = \text{MSA}(\text{LN}(z_{l-1}))+z_{l-1}\qquad l=1...L\\        z_l & = \text{MLP}(\text{LN}(z_l'))+z_l'\qquad \qquad l=1...L\\        y & = \text{LN}(z_L^0)        \end{aligned}        \]    </span></div><h3 id="ViT-Pytorch-Implementation"><a href="#ViT-Pytorch-Implementation" class="headerlink" title="ViT Pytorch Implementation"></a>ViT Pytorch Implementation</h3><p>Code of other modules can be modified from <a href="https://iks-ran.github.io/2023/07/19/transformer/">this blog</a> .</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VisionTransformer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes, img_height, img_width, patch_height, patch_width, patch_dim, d_model, n_heads, d_ff, dropout, gap=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(VisionTransformer, self).__init__()<br>        num_patches = (img_height // patch_height) * (img_width // patch_width)<br>        self.ebd = ViTEmbeddingLayer(num_patches, patch_height, patch_width, patch_dim, d_model, dropout)<br>        self.blks = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff, dropout) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks)])<br>        self.gap = gap<br>        self.fc = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br><br>        output = self.ebd(<span class="hljs-built_in">input</span>, segment)<br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:<br>            output = blk(output, mask)<br><br>        <span class="hljs-keyword">if</span> self.gap:<br>            output = einops.reduce(output, <span class="hljs-string">&#x27;b n d -&gt; b d&#x27;</span>, <span class="hljs-string">&#x27;mean&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            output = output[:, <span class="hljs-number">0</span>]<br>        output = F.softmax(self.fc(output), dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/2010.11929">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp; Houlsby, N. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv, abs&#x2F;2010.11929.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Computer Vision</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BERT:Bidirectional Encoder Representations from Transformers</title>
    <link href="/2023/07/26/bert/"/>
    <url>/2023/07/26/bert/</url>
    
    <content type="html"><![CDATA[<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><h3 id="Input-x2F-Output-Representations"><a href="#Input-x2F-Output-Representations" class="headerlink" title="Input&#x2F;Output Representations"></a>Input&#x2F;Output Representations</h3><figure>    <img src="/img/blogs/230726-bert/2.png" alt="Fig.1 Input Represention">    <figcaption>Fig.1 Input Represention</figcaption></figure><p>For handling a variety of down-stream tasks, the input representation of BERT is able to unambiguously represent both a single sentence and a pair of sentences in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together.</p><p>The first token of every sequence is always a special classification token <code>[CLS]</code>. The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence and separated with a special token <code>[SEP]</code>. A learned embedding to every token is added to indicate whether it belongs to sentence A or sentence B. The input embeddings are the sum of the token embeddings, the segmentation embeddings and the position embeddings.. The model architecture of BERT is nearly the same as.</p><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> einops<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BERTEmbeddingLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, max_len, d_model</span>):<br>        <span class="hljs-built_in">super</span>(BERTEmbeddingLayer, self).__init__()<br>        self.t = nn.Embedding(vocab_size, d_model)<br>        self.p = nn.Embedding(max_len, d_model)<br>        <span class="hljs-comment"># A: 1, B: 2</span><br>        self.s = nn.Embedding(<span class="hljs-number">3</span>, d_model, padding_idx=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, segment</span>):<br><br>        device = <span class="hljs-built_in">input</span>.device<br>        b, seq_len = <span class="hljs-built_in">input</span>.shape[:<span class="hljs-number">2</span>]<br>        pos = einops.repeat(torch.arange(seq_len, dtype=torch.long, device=device), <span class="hljs-string">&#x27;s -&gt; b s&#x27;</span>, b=b)<br>        output = self.t(<span class="hljs-built_in">input</span>) + self.p(pos) + self.s(segment)<br><br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h3 id="Masked-Language-Model"><a href="#Masked-Language-Model" class="headerlink" title="Masked Language Model"></a>Masked Language Model</h3><p>In order to train a deep bidirectional representation, BERT takes randomly masked sequences as input (those masked tokens are replaced by <code>[MASK]</code>) and then predicts those masked tokens. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary. </p><p>Although this allows a bidirectional pre-trained model, a downside is that MLM creats a mismatch between pre-training and fine-tuning, since the <code>[MASK]</code> token does not appear during fine-tuning. To mitigate this, those “masked” words are not always replaced by the actual <code>[MASK]</code> token. The training data generator chooses 15% of the token positions at random for prediction.</p><p>Assuming the unlabeled sentence is <code>my dog is hairy</code> and during the random masking procedure <code>hairy</code> is choosen to be masked.</p><ul><li>80% of the time: Replace the word with the <code>[MASK]</code> token, e.g., <code>my dog is hairy</code> -&gt; <code>my dog is [MASK]</code></li><li>10% of the time: Replace the word with a random word, e.g., <code>my dog is hairy</code> -&gt; <code>my dog is apple</code></li><li>10% of the time: Keep the word unchanged, e.g., <code>my dog is hairy</code> -&gt; <code>my dog is hairy</code>. The purpose of this is to bias the representation towards the actual observed word.</li></ul><p>The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token. Additionally, because random replacement only occurs for 1.5% of all tokens, this does not seem to harm the model’s language understanding capability.</p><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MaskedLanguageModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, d_model</span>):<br>        <span class="hljs-built_in">super</span>(MaskedLanguageModel, self).__init__()<br>        self.fc = nn.Linear(d_model, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-keyword">return</span> F.softmax(self.fc(<span class="hljs-built_in">input</span>), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p>Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, a binarized next sentence prediction task that can be trivially generated from any monolingual corpus is added. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext)</p><p>The next sentence prediction task can be illustrated in the following examples.</p><pre><code class="hljs">Input   =   [CLS] the man went to [MASK] store [SEP]                  he bought a gallon [MASK] milk [SEP]Label   =   𝙸𝚜𝙽𝚎𝚡𝚝Input=   [CLS] the man [MASK] to the store [SEP]                  penguin [MASK] are flight ##less birds [SEP]Label=   𝙽𝚘𝚝𝙽𝚎𝚡𝚝</code></pre><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NextSentencePredict</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model</span>):<br>        <span class="hljs-built_in">super</span>(NextSentencePredict, self).__init__()<br>        self.fc = nn.Linear(d_model, <span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-keyword">return</span> F.softmax(self.fc(<span class="hljs-built_in">input</span>), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h3 id="BERT-Pytorch-Implementation"><a href="#BERT-Pytorch-Implementation" class="headerlink" title="BERT Pytorch Implementation"></a>BERT Pytorch Implementation</h3><p>Code of other modules are from <a href="https://iks-ran.github.io/2023/07/19/transformer/">this blog</a> <em>NOTE: The activation function of Point-wise Feed-Forward Networks in BERT is GELU instead of ReLU</em> .</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BERT</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_blocks, vocab_size, max_len, d_model, n_heads, d_ff, dropout</span>):<br>        <span class="hljs-built_in">super</span>(BERT, self).__init__()<br>        self.ebd = BERTEmbeddingLayer(vocab_size, max_len, d_model)<br>        self.blks = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff, dropout) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks)])<br>        self.mlm = MaskedLanguageModel(vocab_size, d_model)<br>        self.nsp = NextSentencePredict(d_model)<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, segment, mask=<span class="hljs-literal">None</span></span>):<br><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            mask = <span class="hljs-built_in">input</span> &gt; <span class="hljs-number">0</span><br><br>        output = self.ebd(<span class="hljs-built_in">input</span>, segment)<br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:<br>            output = blk(output, mask)<br><br>        mlm = self.mlm(output)<br>        nsp = self.nsp(output[:, <span class="hljs-number">0</span>])<br><br>        <span class="hljs-keyword">return</span> mlm, nsp<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/1810.04805">Devlin, J., Chang, M., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv, abs&#x2F;1810.04805.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Natural Language Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer</title>
    <link href="/2023/07/19/transformer/"/>
    <url>/2023/07/19/transformer/</url>
    
    <content type="html"><![CDATA[<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><figure>    <img src="/img/blogs/230719-transformer/1.png" alt="Fig.1 Transformer" width=400>    <figcaption>Fig.1 Transformer</figcaption></figure><p>Transformer was firsr used for machine translation with an an encoder-decoder structure, see Fig.1. The input (source) and output (target) sequence embeddings are added with positional encoding before being fed into the encoder and the decoder that stack modules based on self-attention.</p><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><figure>    <img src="/img/blogs/230719-transformer/2.png" alt="Fig.2 Scaled Dot-Product Attention" width=200>    <figcaption>Fig.2 Scaled Dot-Product Attention</figcaption></figure><p>Transformer uses “Scaled Dot-Product Attention”. The input consists of queries $Q$ and keys $Q$ of dimension $d_k$, and values  $V$ of dimension $d_v$. This attention computes the dot products of the query with all keys, divide each by $\sqrt{d_k}$ and apply a softmax function to obtain the weights on the values. The scaling factor $d_k$ is used to counteract the effect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.<br>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p><p>The Pytorch implementation is </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> einops<br><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ScaledDotProductAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_k</span>):<br>        <span class="hljs-built_in">super</span>(ScaledDotProductAttention, self).__init__()<br>        self.scaling_factor = math.sqrt(d_k)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, q, k, v, mask=<span class="hljs-literal">None</span></span>):<br><br>        qk = einops.einsum(q, k, <span class="hljs-string">&#x27;... q_s d_k, ... kv_s d_k -&gt; ... q_s kv_s&#x27;</span>)<br><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            qk = qk.masked_fill(mask == <span class="hljs-number">0</span>, -<span class="hljs-number">1e8</span>)<br><br>        score = F.softmax(qk / self.scaling_factor, dim=-<span class="hljs-number">1</span>)<br>        attn = einops.einsum(score, v, <span class="hljs-string">&#x27;... q_s kv_s, ... kv_s d_v -&gt; ... q_s d_v&#x27;</span>)<br>        <br>        <span class="hljs-keyword">return</span> attn<br></code></pre></td></tr></table></figure><h4 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h4><figure>    <img src="/img/blogs/230719-transformer/3.png" alt="Fig.3 Multi-head Attention" width=300>    <figcaption>Fig.3 Multi-head Attention</figcaption></figure><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries, it’s beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k, d_k$ and $d_v$ dimensions, respectively. On each of these projected versions of queries, keys and values, the attention function is performed in parallel, yielding $d_v$-dimensional output values. These are concatenated and once again projected, resulting in the final values. In this work, $d_k&#x3D;d_v&#x3D;d_{model} &#x2F; h$</p><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> einops.layers.torch <span class="hljs-keyword">import</span> Rearrange, Repeat<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, n_heads</span>):<br>        <span class="hljs-built_in">super</span>(MultiHeadAttention, self).__init__()<br>        self.d_model = d_model<br>        self.n_heads = n_heads<br>        self.w_q = nn.Linear(d_model, d_model)<br>        self.w_k = nn.Linear(d_model, d_model)<br>        self.w_v = nn.Linear(d_model, d_model)<br>        self.to_multi_head = Rearrange(<span class="hljs-string">&#x27;... s (h d_k) -&gt; ... h s d_k&#x27;</span>, h=n_heads)<br>        self.mask_repeat = Repeat(mask, <span class="hljs-string">&#x27;... s -&gt; ... h s d_k&#x27;</span>, h=n_heads, d_k=d_model // n_heads)<br>        self.attention = ScaledDotProductAttention(d_model // n_heads)<br>        self.concat = Rearrange(<span class="hljs-string">&#x27;... h s d_k -&gt; ... s (h d_k)&#x27;</span>)<br>        self.w_o = nn.Linear(d_model, d_model)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, q, k, v, mask=<span class="hljs-literal">None</span></span>):<br><br>        mh_q, mh_k, mh_v = self.to_multi_head([self.w_q(q), self.w_k(k), self.w_v(v)])<br><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            mask = self.mask_repeat(mask)<br>        mh_attn = self.attention(mh_q, mh_k, mh_v, mask=mask)<br>        attn = self.concat(mh_attn)<br><br>        o = self.w_o(attn)<br>        <br>        <span class="hljs-keyword">return</span> o<br></code></pre></td></tr></table></figure><h4 id="Point-wise-Feed-Forward-Networks"><a href="#Point-wise-Feed-Forward-Networks" class="headerlink" title="Point-wise Feed-Forward Networks"></a>Point-wise Feed-Forward Networks</h4><p>This consists of two linear transformations with a ReLU activation in between.</p><p>$$<br>\text{FFN}(x) &#x3D; \max(0, xW_1+b_1)W_2+b_2<br>$$</p><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PointWiseFFN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, d_ff</span>):<br>        <span class="hljs-built_in">super</span>(PointWiseFFN, self).__init__()<br>        self.linear_1 = nn.Linear(d_model, d_ff)<br>        self.act = nn.ReLU()<br>        self.linear_2 = nn.Linear(d_model, d_ff)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-keyword">return</span> self.linear_2(self.act(self.linear_1(<span class="hljs-built_in">input</span>)))<br></code></pre></td></tr></table></figure><h4 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h4><p>Since transformer contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, “positional encodings” as added to the input embeddings. For transformer, the authors use sine and cosine functions of different frequencies:</p><div style="text-align: center;">    <span style="display: inline-block;">        \[        \begin{aligned}        PE(pos, 2i) & = \sin (pos/10000^{2i/d_{model}}) \\        PE(pos, 2i+1) & = \cos (pos/10000^{2i/d_{model}}) \\        \end{aligned}        \]    </span></div><p>As the two embedding layers and the pre-softmax linear transformation share the same weight, the output of embedding layer should be multiplied by $d_{model}$ (because the pre-softmax linear layer usually inited by xaiver init)</p><p>$$<br>W ～ N(0, 1&#x2F;d_{model})<br>$$</p><p>The Pytorch implementation is </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionalEncoding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, max_len, d_model</span>):<br>        <span class="hljs-built_in">super</span>(PositionalEncoding, self).__init__()<br>        self.scaling_factor = math.sqrt(d_model)<br>        self.p = torch.zeros((<span class="hljs-number">1</span>, max_len, d_model))<br>        index = torch.arange(max_len, dtype=torch.float32).reshape(<br>            -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) / torch.<span class="hljs-built_in">pow</span>(<span class="hljs-number">10000</span>, torch.arange(<br>            <span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>, dtype=torch.float32) / d_model)<br>        self.p[:, :, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(index)<br>        self.p[:, :, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(index)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br><br>        device = <span class="hljs-built_in">input</span>.device<br>        seq_len = <span class="hljs-built_in">input</span>.shape[<span class="hljs-number">1</span>]<br>        output = <span class="hljs-built_in">input</span> * self.scaling_factor + self.p[:, :seq_len].to(device)<br><br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>The encoder is composed of a stack of identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. A residual connection is employed around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is $\text{LayerNorm}(x+\text{Sublayer}(x))$, where $\text{Sublayer}(x)$ is the function implemented by the sub-layer itself. </p><p>Dropout is applied to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition,  dropout is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.</p><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, n_heads, d_ff, dropout</span>):<br>        <span class="hljs-built_in">super</span>(EncoderBlock, self).__init__()<br>        self.sublayer_1 = MultiHeadAttention(d_model, n_heads)<br>        self.drop_1 = nn.Dropout(dropout)<br>        self.norm_1 = nn.LayerNorm(d_model)<br>        self.sublayer_2 = FFN(d_model, d_ff)<br>        self.drop_2 = nn.Dropout(dropout)<br>        self.norm_2 = nn.LayerNorm(d_model)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, src_mask=<span class="hljs-literal">None</span></span>):<br><br>        o1 = self.sublayer_1(src, src, src, src_mask)<br>        src = self.norm_1(src + self.drop_1(o1))<br><br>        o2 = self.sublayer_2(src)<br>        src = self.norm_2(src + self.drop_2(o2))<br><br>        <span class="hljs-keyword">return</span> src<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_blocks, vocab_size, max_len, d_model, n_heads, d_ff, dropout</span>):<br>        <span class="hljs-built_in">super</span>(Encoder, self).__init__()<br>        self.ebd = nn.Embedding(vocab_size, d_model, padding_idx=<span class="hljs-number">0</span>)<br>        self.pos_encode = PositionalEncoding(max_len, d_model)<br>        self.drop = nn.Dropout(dropout)<br>        self.blks = nn.ModuleList([EncoderBlock(d_model, n_heads, d_ff, dropout) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks)])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, src_mask=<span class="hljs-literal">None</span></span>):<br><br>        src = self.drop(self.pos_encode(self.ebd(src)))<br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:<br>            src = blk(output, src_mask)<br><br>        <span class="hljs-keyword">return</span> src<br></code></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>The decoder is also composed of a stack of identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. The self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.</p><p>The Pytorch implementation is</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_model, n_heads, d_ff, dropout</span>):<br>        <span class="hljs-built_in">super</span>(DecoderBlock, self).__init__()<br>        self.sublayer_1 = MultiHeadAttention(d_model, n_heads)<br>        self.drop_1 = nn.Dropout(dropout)<br>        self.norm_1 = nn.LayerNorm(d_model)<br>        self.sublayer_2 = MultiHeadAttention(d_model, n_heads)<br>        self.drop_2 = nn.Dropout(dropout)<br>        self.norm_2 = nn.LayerNorm(d_model)<br>        self.sublayer_3 = FFN(d_model, d_ff)<br>        self.drop_3 = nn.Dropout(dropout)<br>        self.norm_3 = nn.LayerNorm(d_model)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, tgt, src_mask=<span class="hljs-literal">None</span>, tgt_mask=<span class="hljs-literal">None</span></span>):<br><br>        o1 = self.sublayer_1(tgt, tgt, tgt, tgt_mask)<br>        tgt = self.norm_1(tgt + self.drop_1(o1))<br><br>        o2 = self.sublayer_2(src, src, tgt, src_mask)<br>        tgt = self.norm_2(tgt + self.drop_2(o2))<br><br>        o3 = self.sublayer_3(tgt)<br>        tgt = self.norm_3(tgt + self.drop_3(o3))<br><br>        <span class="hljs-keyword">return</span> tgt<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Dencoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_blocks, vocab_size, max_len, d_model, n_heads, d_ff, dropout</span>):<br>        <span class="hljs-built_in">super</span>(Dencoder, self).__init__()<br>        self.ebd = nn.Embedding(vocab_size, d_model)<br>        self.pos_encode = PositionalEncoding(max_len, d_model)<br>        self.drop = nn.Dropout(dropout)<br>        self.blks = nn.ModuleList([DecoderBlock(d_model, n_heads, d_ff, dropout) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks)])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, tgt, src_mask=<span class="hljs-literal">None</span>, tgt_mask=<span class="hljs-literal">None</span></span>):<br><br>        tgt = self.drop(self.pos_encode(self.ebd(tgt)))<br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:<br>            tgt = blk(src, tgt, src_mask, tgt_mask)<br><br>        <span class="hljs-keyword">return</span> tgt<br></code></pre></td></tr></table></figure><h3 id="Transformer-Pytorch-Implementation"><a href="#Transformer-Pytorch-Implementation" class="headerlink" title="Transformer Pytorch Implementation"></a>Transformer Pytorch Implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Transformer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_blocks, src_vocab_size, tar_vocab_size, src_max_len, tar_max_len, d_model, n_heads, d_ff, dropout</span>):<br>        <span class="hljs-built_in">super</span>(Transformer, self).__init__()<br>        self.encoder = Encoder(num_blocks, src_vocab_size, src_max_len, d_model, n_heads, d_ff, dropout)<br>        self.decoder = Decoder(num_blocks, tar_vocab_size, tar_max_len, d_model, n_heads, d_ff, dropout)<br>        self.fc = nn.Linear(d_model, tar_vocab_size)<br>        <span class="hljs-comment"># weight sharing</span><br>        self.fc.weight = self.decoder.ebd.weight<br>        <span class="hljs-keyword">if</span> src_vocab_size == tar_vocab_size: <br>            self.encoder.ebd.weight = self.decoder.ebd.weight<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, src, tgt, src_mask=<span class="hljs-literal">None</span>, tgt_mask=<span class="hljs-literal">None</span></span>):<br><br>        <span class="hljs-keyword">if</span> src_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            src_mask = src &gt; <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> tgt_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            tgt_mask = tgt &gt; <span class="hljs-number">0</span><br><br>        enc_output = self.encoder(src, src_mask)<br>        dec_output = self.decoder(enc_output, tgt, src_mask, tgt_mask)<br><br>        output = F.softmax(self.fc(dec_output[:, <span class="hljs-number">0</span>]), dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/1706.03762">Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://d2l.ai/index.html">Journal of Machine Learning Research 21 (23), 1-7, 2020. 183, 2020. Dive into deep learning. 2020. A Zhang, ZC Lipton, M Li, AJ Smola.</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Natural Language Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Tokenization And Embedding</title>
    <link href="/2023/07/16/tokenization-embedding/"/>
    <url>/2023/07/16/tokenization-embedding/</url>
    
    <content type="html"><![CDATA[<h2 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h2><figure>    <img src="/img/blogs/230716-tk-ebd/2.png" alt="Fig.1 Tokenization">    <figcaption>Fig.1 Tokenization</figcaption></figure>Encode input sequence (usually setence in NLP) with tokenizers to sequence of integers. For example, <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>inputs = <span class="hljs-string">&quot;Never gonna give you up.&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>tokenized = tokenize(inputs)<br><span class="hljs-meta">&gt;&gt;&gt; </span>tokenized<br>[<span class="hljs-number">12</span> <span class="hljs-number">234</span> <span class="hljs-number">56</span> <span class="hljs-number">4</span> <span class="hljs-number">69</span> <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><h3 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte-Pair Encoding (BPE)"></a>Byte-Pair Encoding (BPE)</h3><p>Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units. BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization. More advanced pre-tokenization include rule-based tokenization which uses Moses for most languages, or GPT which uses Spacy and ftfy, to count the frequency of each word in the training corpus.</p><p>After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Desired vocabulary size is a hyperparameter to define before training the tokenizer.</p><p>BPE is a word segmentation algorithm based on statistics, which is highly dependent on the corpus. <strong>If the corpus is small, the effect is generally not good</strong>.</p><h4 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h4><ol><li>Count all occurrences of words in the input and add a word terminator <code>&lt;/w&gt;</code> after each word</li><li>Split all words into individual characters</li><li>Combine the most frequently occurring words<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">&#123;<span class="hljs-string">&#x27;a&#x27;</span>:<span class="hljs-number">16</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">8</span>&#125; -&gt; &#123;<span class="hljs-string">&#x27;ab&#x27;</span>:<span class="hljs-number">12</span>, <span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;c&#x27;</span>: <span class="hljs-number">8</span>&#125;<br></code></pre></td></tr></table></figure></li><li>Iterate 3. repeatedly until the vocabulary size is equal to desired vocabulary size or the frequency of the next character pair is 1.</li></ol><h4 id="Encode"><a href="#Encode" class="headerlink" title="Encode"></a>Encode</h4><ol><li>Sort the words in the vocabulary according to their length, from long to short</li><li>For each word in the corpus, traverse the sorted vocabulary, determine whether the word&#x2F;subword (subword) in the vocabulary is a substring of the string, if it matches, output the current subword, and Continue to iterate over the remaining strings of words.</li><li>If after traversing the vocabulary, there is still a substring in the word that is not matched, then we replace it with a special subword, eg. <code>&lt;unk&gt;</code></li></ol><h4 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h4><p>Corpus decoding is to put all the output subwords together until the end is <code>&lt;\w&gt;</code>.</p><h3 id="Word-Piece"><a href="#Word-Piece" class="headerlink" title="Word-Piece"></a>Word-Piece</h3><p>Word-Piece is very similar to BPE. BPE uses the most frequently occurring combined constructor vocabulary, while Wordpiece uses the most frequently occurring $\frac{P(AB)}{P(A)P(B)}$ combined constructor vocabulary.</p><h3 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h3><p>Different from BPE or Word-Piece, Unigram’s algorithm idea is to start from a huge vocabulary, and then gradually delete the trim down vocabulary until the size meets the predefined size.</p><p>The initial vocabulary can use all the words separated by the pre-tokenizer, plus all high-frequency substrings.<br>The principle of removing words from the vocabulary each time is to minimize the predefined loss. During training, the formula for calculating loss is:<br>$$<br>Loss &#x3D; - \sum_{i&#x3D;1}^{N}\log \left(\sum_{x\in S(x_i)}p(x)\right)<br>$$</p><p>Assume that all the words in the training document are $x_1,x_2,…,x_N$, and the method of tokenizing each word is a set $S(x_i)$.</p><p>When a vocabulary is determined, the set $S(x_i)$ of tokenize methods for each word is determined, and each method corresponds to a probability $p(x)$.<br>If some words are deleted from the vocabulary, the set of tokenize types of some words will be reduced, and the summation items in log(*) will be reduced, thereby increasing the overall loss.</p><p>The Unigram algorithm will pick out 10%~20% of the words that make the loss increase the least from the vocabulary each time and delete them. Generally, the Unigram algorithm will be used in conjunction with the SentencePiece algorithm.</p><h3 id="Sentence-Piece"><a href="#Sentence-Piece" class="headerlink" title="Sentence-Piece"></a>Sentence-Piece</h3><p>Sentence-Piece is mainly used for multilingual models, and it does two important transformations:</p><ol><li>Encode characters in unicode mode, convert all input (English, Chinese and other languages) into unicode characters, and solve the problem of different multi-language encoding methods.</li><li>Encode spaces into <code>_</code>, such as <code>&#39;New York&#39;</code> will be converted into <code>[&#39;_&#39;, &#39;New&#39;, &#39;_York&#39;]</code>, this is also to be able to deal with multilingual problems.</li></ol><p>It treats a sentence as a whole and breaks it down into fragments without retaining the concept of natural words. Generally, it treats spaces as a special character, and then uses BPE or Unigram algorithm to construct a vocabulary.</p><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><figure>    <img src="/img/blogs/230716-tk-ebd/3.png" alt="Fig.1 Embedding">    <figcaption>Fig.1 Embedding</figcaption></figure>The essence of the Embedding matrix is a lookup table. Since the input sequence of integers, only rows in the embedding matrix is activated.<p>For example, suppose the Embedding matrix is </p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[<span class="hljs-comment">[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[0.2, 0.3, 0.4, 0.5, 0.6, 0.1]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[0.3, 0.4, 0.5, 0.6, 0.1, 0.2]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[0.4, 0.5, 0.6, 0.1, 0.2, 0.3]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[0.5, 0.6, 0.1, 0.2, 0.3, 0.4]</span>,</span><br><span class="hljs-comment"> <span class="hljs-comment">[0.6, 0.1, 0.2, 0.3, 0.4, 0.5]</span>]</span><br></code></pre></td></tr></table></figure><p>Then </p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs inform7"><span class="hljs-comment">[1,     <span class="hljs-comment">[<span class="hljs-comment">[0.2, 0.3, 0.4, 0.5, 0.6, 0.1]</span>, </span></span><br><span class="hljs-comment"><span class="hljs-comment"> 2,  -&gt;  <span class="hljs-comment">[0.3, 0.4, 0.5, 0.6, 0.1, 0.2]</span>, </span></span><br><span class="hljs-comment"><span class="hljs-comment"> 5]</span>      <span class="hljs-comment">[0.6, 0.1, 0.2, 0.3, 0.4, 0.5]</span>]</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
      <tag>Natural Language Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Data Parallel And Distributed Data Parallel</title>
    <link href="/2023/07/14/DP-DDP/"/>
    <url>/2023/07/14/DP-DDP/</url>
    
    <content type="html"><![CDATA[<h2 id="Data-Parallel"><a href="#Data-Parallel" class="headerlink" title="Data Parallel"></a>Data Parallel</h2><h3 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h3><figure>    <img src="/img/blogs/230714-dp-ddp/2.png" alt="Fig.1 Data Parallel" width=400>    <figcaption>Fig.1 Data Parallel</figcaption></figure><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs oxygene"><span class="hljs-keyword">While</span> iterating:<br>    <span class="hljs-number">1</span>. Scatter batch <span class="hljs-keyword">to</span> mini-batch <span class="hljs-keyword">and</span> distribute them across given GPUs<br>    <span class="hljs-number">2</span>. Replicate <span class="hljs-keyword">and</span> broadcast model <span class="hljs-keyword">and</span> paramaters <span class="hljs-keyword">from</span> master GPU <span class="hljs-keyword">to</span> other GPUs<br>    <span class="hljs-number">3</span>. <span class="hljs-keyword">Forward</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">each</span> GPU <span class="hljs-keyword">and</span> gather results <span class="hljs-keyword">on</span> master GPU<br>    <span class="hljs-number">4</span>. Compute loss <span class="hljs-keyword">and</span> distribute <span class="hljs-keyword">to</span> <span class="hljs-keyword">each</span> GPU<br>    <span class="hljs-number">5</span>. Complete backpropagation <span class="hljs-keyword">to</span> calculate the gradient<br>    <span class="hljs-number">6</span>. Gather gradient <span class="hljs-keyword">on</span> <span class="hljs-keyword">each</span> GPU <span class="hljs-keyword">and</span> update parameters <span class="hljs-keyword">with</span> average gradient<br>    ...<br></code></pre></td></tr></table></figure><h3 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h3><ul><li>Unbalanced load: more load on master GPU</li><li>Communication overhead: Assuming that there are $K$ GPUs, the total communication volume is $p$, and the communication bandwidth is $b$. Then using the Parameter Server algorithm, it takes a total of time $T&#x3D;2(K-1)\frac{p}{b}$. $T$ increases with the increase of $K$.</li><li>Single process: A Python process can only use one CPU kernel, that is, when a single-core multi-thread is concurrent, only one thread can be executed. Considering multi-core, multi-core and multi-threading may cause thread thrashing to cause waste of resources, so if Python wants to take advantage of multi-core, it is best to use multi-process.</li><li>Single Machine</li></ul><h3 id="Simple-Use"><a href="#Simple-Use" class="headerlink" title="Simple Use"></a>Simple Use</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">parallel_model = torch.nn.DataParallel(model)<br>...<br>predictions = parallel_model(inputs)<br>loss = loss_function(predictions, labels)<br>loss.mean().backward()<br>optimizer.step()<br>optimizer.zero_grad()<br></code></pre></td></tr></table></figure><h2 id="Distributed-Data-Parallel"><a href="#Distributed-Data-Parallel" class="headerlink" title="Distributed Data Parallel"></a>Distributed Data Parallel</h2><h3 id="Ring-Allreduce"><a href="#Ring-Allreduce" class="headerlink" title="Ring Allreduce"></a>Ring Allreduce</h3><figure>    <img src="/img/blogs/230714-dp-ddp/ring-gpus.png" alt="Fig.2 Scatter Alleduce" width=600>    <figcaption>Fig.2 Ring Allreduce</figcaption></figure>The GPUs in a ring allreduce are arranged in a logical ring. Each GPU should have a left neighbor and a right neighbor; it will only ever send data to its right neighbor, and receive data from its left neighbor.<p>The algorithm proceeds in two steps: first, a scatter-reduce, and then, an allgather. In the scatter-reduce step, the GPUs will exchange data such that every GPU ends up with a chunk of the final result. In the allgather step, the GPUs will exchange those chunks such that all GPUs end up with the complete final result.</p><h4 id="Scatter-Reduce"><a href="#Scatter-Reduce" class="headerlink" title="Scatter-Reduce"></a>Scatter-Reduce</h4><figure>    <img src="/img/blogs/230714-dp-ddp/scatter-reduce.gif" alt="Fig.3 Scatter-Reduce" width=600>    <figcaption>Fig.3 Scatter-Reduce</figcaption></figure><p>The parameters are divided into $K$ chunks, and adjacent GPUs pass different parameters. After passing $K-1$ times, the accumulation of a each parameter can be obtained on different GPUs.</p><h4 id="Allgather"><a href="#Allgather" class="headerlink" title="Allgather"></a>Allgather</h4><figure>    <img src="/img/blogs/230714-dp-ddp/all-gather.gif" alt="Fig.4 Allgather" width=600>    <figcaption>Fig.4 Allgather</figcaption></figure><p>After getting the accumulation of each parameter, do another transfer and synchronize to all GPUs.</p><h4 id="Communication-Overhead"><a href="#Communication-Overhead" class="headerlink" title="Communication Overhead"></a>Communication Overhead</h4><p>Assuming that there are $K$ GPUs, the total communication volume is $p$, and the communication bandwidth is $b$. Then using the Ring Allreduce, The amount of communication per GPU is $\frac{p}{K}$. It takes a total of time $T&#x3D;2(K-1)\frac{\frac{p}{K}}{b} &#x3D; (1 - \frac{1}{K})* 2\frac{p}{b} \approx 2\frac{p}{b}$ ,which is approximately independent of $K$.</p><h3 id="Procedure-1"><a href="#Procedure-1" class="headerlink" title="Procedure"></a>Procedure</h3><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs pf">Divide parameters into chunks and init <span class="hljs-keyword">state</span>. <br><span class="hljs-comment"># Each GPU receives a parameter chunk, and these parameters are replicated on that GPU. </span><br><span class="hljs-comment"># Each GPU has the full model structure on it, but only processes its own chunk of parameters.</span><br><span class="hljs-comment"># A process with a rank of 0 will broadcast the network initialization parameters to every other process to ensure that the models in each process have the same initialization value.</span><br>While iterating :<br>    <span class="hljs-number">1</span>. Get non-repeating batch input <span class="hljs-keyword">from</span> distributed sampler <span class="hljs-keyword">on</span> each GPU <br>    <span class="hljs-number">2</span>. Forward, compute loss and backpropogation <span class="hljs-keyword">to</span> compute gradient of each chunk<br>    <span class="hljs-number">3</span>. Ring Allreduce<br></code></pre></td></tr></table></figure><h3 id="Simple-Use-1"><a href="#Simple-Use-1" class="headerlink" title="Simple Use"></a>Simple Use</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data.distributed <span class="hljs-keyword">import</span> DistributedSampler<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br>...<br>torch.distributed.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>, ...)<br>...<br>local_rank = torch.distributed.get_rank()<br>device = torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>, local_rank)<br>model = model.to(device)<br>distrib_model = torch.nn.parallel.DistributedDataParallel(model,<br>                                                          device_ids=[local_rank],<br>                                                          output_device=local_rank, <br>                                                          ...)<br>sampler = DistributedSampler(dataset)<br>dataloader = DataLoader(dataset, sampler=sampler, ...)<br>...<br><span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> dataloader:<br>    predictions = distrib_model(inputs.to(device))         <br>    loss = loss_function(predictions, labels.to(device))   <br>    loss.backward()                                        <br>    optimizer.step()    <br>    optimizer.zero_grad()                                   <br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=... python -m torch.distributed.launch --nproc_per_node=<span class="hljs-variable">$&#123;NPROC_PER_NODE&#125;</span> <span class="hljs-variable">$&#123;TRAIN_FILE&#125;</span><br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://openreview.net/forum?id=oapKSVM2bcj">【分布式训练】单机多卡的正确打开方式（一）：理论基础</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/634846886">3.13 PyTorch:分布式训练（只简单介绍原理）</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://blog.csdn.net/Sihang_Xie/article/details/127328271">【PyTorch教程】PyTorch分布式并行模块DistributedDataParallel(DDP)详解</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">💥 Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/">Bringing HPC Techniques to Deep Learning</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/343951042">PyTorch 源码解读之 DP &amp; DDP：模型并行和分布式训练解析</a><a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:7" class="footnote-text"><span><a href="https://d2l.ai/index.html">Journal of Machine Learning Research 21 (23), 1-7, 2020. 183, 2020. Dive into deep learning. 2020. A Zhang, ZC Lipton, M Li, AJ Smola.</a><a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Einops:Clear and Reliable Tensor Manipulations with Einstein-like Notation</title>
    <link href="/2023/07/12/einops/"/>
    <url>/2023/07/12/einops/</url>
    
    <content type="html"><![CDATA[<h2 id="Fundamentals"><a href="#Fundamentals" class="headerlink" title="Fundamentals"></a>Fundamentals</h2><h3 id="Core-Operations"><a href="#Core-Operations" class="headerlink" title="Core Operations"></a>Core Operations</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> einops <span class="hljs-keyword">import</span> rearrange, reduce, repeat<br><span class="hljs-comment"># rearrange elements according to the pattern</span><br>output_tensor = rearrange(input_tensor, <span class="hljs-string">&#x27;t b c -&gt; b c t&#x27;</span>)<br><span class="hljs-comment"># combine rearrangement and reduction</span><br>output_tensor = reduce(input_tensor, <span class="hljs-string">&#x27;b c (h h2) (w w2) -&gt; b h w c&#x27;</span>, <span class="hljs-string">&#x27;mean&#x27;</span>, h2=<span class="hljs-number">2</span>, w2=<span class="hljs-number">2</span>)<br><span class="hljs-comment"># copy along a new axis</span><br>output_tensor = repeat(input_tensor, <span class="hljs-string">&#x27;h w -&gt; h w c&#x27;</span>, c=<span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h3 id="Pack-amp-Unpack"><a href="#Pack-amp-Unpack" class="headerlink" title="Pack &amp; Unpack"></a>Pack &amp; Unpack</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> einops <span class="hljs-keyword">import</span> pack, unpack<br><span class="hljs-comment"># pack and unpack allow reversibly &#x27;packing&#x27; multiple tensors into one.</span><br><span class="hljs-comment"># Packed tensors may be of different dimensionality:</span><br>packed,  ps = pack([class_token_bc, image_tokens_bhwc, text_tokens_btc], <span class="hljs-string">&#x27;b * c&#x27;</span>)<br>class_emb_bc, image_emb_bhwc, text_emb_btc = unpack(transformer(packed), ps, <span class="hljs-string">&#x27;b * c&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="Einsum"><a href="#Einsum" class="headerlink" title="Einsum"></a>Einsum</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> einops <span class="hljs-keyword">import</span> einsum, pack, unpack<br><span class="hljs-comment"># einsum is like ... einsum, generic and flexible dot-product </span><br><span class="hljs-comment"># but 1) axes can be multi-lettered  2) pattern goes last 3) works with multiple frameworks</span><br>C = einsum(A, B, <span class="hljs-string">&#x27;b t1 head c, b t2 head c -&gt; b head t1 t2&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Sequential, Conv2d, MaxPool2d, Linear, ReLU<br><span class="hljs-keyword">from</span> einops.layers.torch <span class="hljs-keyword">import</span> Rearrange<br><br>model = Sequential(<br>    ...,<br>    Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>),<br>    MaxPool2d(kernel_size=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># flattening without need to write forward</span><br>    Rearrange(<span class="hljs-string">&#x27;b c h w -&gt; b (c h w)&#x27;</span>),  <br>    Linear(<span class="hljs-number">16</span>*<span class="hljs-number">5</span>*<span class="hljs-number">5</span>, <span class="hljs-number">120</span>), <br>    ReLU(),<br>    Linear(<span class="hljs-number">120</span>, <span class="hljs-number">10</span>), <br>)<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://openreview.net/forum?id=oapKSVM2bcj">Rogozhnikov, A.: Einops: clear and reliable tensor manipulations with einstein-like notation. In: International Conference on Learning Representations (2022).</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Normalization——Batch Norm, Layer Norm, Instance Norm and Group Norm</title>
    <link href="/2023/07/11/Normalization/"/>
    <url>/2023/07/11/Normalization/</url>
    
    <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><p>For a tensor with the shape of <code>(N, C, D)</code>, where $N$ stands for batch-size, $C$ stands for features and D stands for feature map (for example, $D$ is <code>H, W</code> in computer vision), Z-Score Normalization of a certain dimension  $l$ is</p><p>$$<br>\hat{z}^{l} &#x3D; \frac{z^l - E(z^l)}{\sqrt{var(z^l)+\epsilon}}<br>$$</p><p>If some dimensions are ignored during normalization, the standardized data will retain the information between these dimensions; this may be a bit convoluted. Taking BN as an example, BN is standardized in each channel $C$, ignoring the pixel position, The sample number in the batch, so BN retains the association of data in these dimensions, but at the same time loses the data connection between channels.</p><p>Batch Normalization, Layer Normalization, Instance Normalization and Group Normalization are the most commonly used normalization methods based on Z-Score Normalization. They all consider and make a fuss about the input of the activation function, and normalize the input of the activation function in different ways.</p><figure>    <img src="/img/blogs/230711-normalizations/1.png" alt="Fig.1 Normalization methods">    <figcaption>Fig.1 Normalization methods</figcaption></figure><blockquote><p>Normalization methods. Each subplot shows a feature map tensor, with N as the batch axis, C as the channel axis, and (H, W) as the spatial axes. The pixels in blue are normalized by the same mean and variance, computed by aggregating the values of these pixels</p></blockquote><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><ul><li><p>In the process of deep neural network training, it is usually trained with each mini-batch input to the network, so that each batch has a different distribution, which makes the model training particularly difficult.</p></li><li><p>Internal Covariate Shift: In a deep neural network, the input of a middle layer is the output of the previous neural layer. Therefore, changes in the parameters of the neural layers before it lead to large differences in the distribution of its inputs. During the training process, the activation function will change the distribution of data in each layer. As the network deepens, this change (difference) will become larger and larger, making the model training particularly difficult, the convergence speed is very slow, and the gradient will disappear.</p></li></ul><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p>BN takes channel $C$ as $l$. The standard normalization of the net input $z^l$will make its values ​​concentrate around 0. If the sigmoid function is used, this value range is just close to the linear transformation range, which weakens the nonlinear nature of the neural network. Therefore, in order to normalize Without negatively affecting the representational power of the network, we can change the range of values through an additional scaling and translation transformation.</p><p>$$<br>\hat{z}^{l} &#x3D; \frac{z^l - E(z^l)}{\sqrt{var(z^l)+\epsilon}} \odot \gamma + \beta \Leftarrow BN_{\gamma,\beta}(z^l)\quad where\quad l\quad is\quad C<br>$$</p><p>Where $\gamma$ and $\beta$ are learnable variables. BN is usually used after the fully connected layer or convolutional layer, before the activation function.</p><p>During training, the BN layer will use the mean and standard deviation of the data in each batch to standardize the samples in the batch each time; at the same time, the global mean and variance on the training set will be continuously updated and saved by means of sliding average.</p><p>While testing, normalize each data point on the test set with the mean and variance of the saved training set.</p><h3 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h3><h4 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h4><ul><li>Allows large learning rate</li><li>Weaken the strong dependence on initialization and reduce the difficulty of weight initialization</li><li>Avoid overfitting as each of mini-batch contain information from others and each mini-batch is sampled randomly</li><li>Keep the mean and variance of the values in the hidden layer unchanged, control the distribution range of the data, and avoid gradient disappearance and gradient explosion</li><li>Have the same regularization effect as dropout. In terms of regularization, dropout is generally used for fully connected layers, and BN is used for convolutional layers</li><li>Alleviate internal covariate shift problem and increase training speed</li></ul><h4 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h4><ul><li>The mean and variance are calculated on a batch each time. If the batch-size is too small, the calculated mean and variance are not enough to represent the entire data distribution.</li><li>If the batch-size is too large, it will exceed the memory capacity; more epochs need to be run, resulting in a longer total training time; the direction of gradient descent will be fixed directly, making it difficult to update.</li></ul><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><h3 id="Motivation-1"><a href="#Motivation-1" class="headerlink" title="Motivation"></a>Motivation</h3><p>For neural networks whose input distribution changes dynamically in the neural network, such as RNN, the BN operation cannot be applied, even if filled with specific characters, it will cause the problem of uneven distribution of some channels.</p><h3 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h3><p>Compared with BN, LN takes batch $N$ instead of channel $C$ as $l$.Other parts in LN are the same with BN</p><p>$$<br>\hat{z}^{l} &#x3D; \frac{z^l - E(z^l)}{\sqrt{var(z^l)+\epsilon}} \odot \gamma + \beta \Leftarrow LN_{\gamma,\beta}(z^l)\quad where\quad l\quad is\quad N<br>$$</p><h3 id="Discussion-1"><a href="#Discussion-1" class="headerlink" title="Discussion"></a>Discussion</h3><p>LN does not require batch training, and can be normalized within a single piece of data. LN also does not depend on batch-size and the length of the input sequence, so it can be used in batch size 1 and RNN. The effect of LN on RNN is more obvious, but on CNN, the effect is not as good as BN.</p><h2 id="Instance-Normalization"><a href="#Instance-Normalization" class="headerlink" title="Instance Normalization"></a>Instance Normalization</h2><h3 id="Motivation-2"><a href="#Motivation-2" class="headerlink" title="Motivation"></a>Motivation</h3><p>In image stylization, the generated result mainly depends on an image instance, and the mean and variance of each channel of the feature map will affect the style of the final generated image. Therefore, normalizing the entire batch is not suitable for image stylization, so normalizing H and W can speed up model convergence and maintain the independence between each image instance.</p><h3 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h3><p>Compared with BN, IN takes each channel $C$ of each batch $N$ instead of channel $C$ as $l$.Other parts in IN are the same with BN</p><p>$$<br>\hat{z}^{l} &#x3D; \frac{z^l - E(z^l)}{\sqrt{var(z^l)+\epsilon}} \odot \gamma + \beta \Leftarrow LN_{\gamma,\beta}(z^l)\quad where\quad l\quad is\quad N * C<br>$$</p><h3 id="Discussion-2"><a href="#Discussion-2" class="headerlink" title="Discussion"></a>Discussion</h3><p>IN is a separate normalization operation for each channel in a sample, which is generally used for style migration, but if the feature map can use the correlation between channels, then IN is not suitable.</p><h2 id="Group-Normalization"><a href="#Group-Normalization" class="headerlink" title="Group Normalization"></a>Group Normalization</h2><h3 id="Motivation-3"><a href="#Motivation-3" class="headerlink" title="Motivation"></a>Motivation</h3><p>The proposed formula of GN is to solve the problem that BN has poor effect on small mini batch-size, and is suitable for tasks that occupy relatively large video memory, such as image segmentation. For this type of task, the batch size may only be a single digit, and no matter how large the video memory is, it will not be enough. When the batch size is a single digit, BN performs poorly, because there is no way to approximate the mean and standard deviation of the population through the amount of data of several samples. GN is also independent of batch, it is a compromise between LN and IN.</p><h3 id="Method-3"><a href="#Method-3" class="headerlink" title="Method"></a>Method</h3><p>GN divides channels of each sample into G groups. Compared with BN, GN takes each group $C &#x2F; G$ of each batch $N$ instead of channel $C$ as $l$.Other parts in GN are the same with BN</p><p>$$<br>\hat{z}^{l} &#x3D; \frac{z^l - E(z^l)}{\sqrt{var(z^l)+\epsilon}} \odot \gamma + \beta \Leftarrow LN_{\gamma,\beta}(z^l)\quad where\quad l\quad is\quad N * \frac{C}{G}<br>$$</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://zhuanlan.zhihu.com/p/524829507">深度学习基础 之 —- BN、LN、IN、GN、SN</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://arxiv.org/abs/1502.03167">Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span><a href="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450, 2016.</a><a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span><a href="https://arxiv.org/abs/1607.08022">Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv:1607.08022, 2016.</a><a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span><a href="https://arxiv.org/abs/1803.08494">Yuxin Wu and Kaiming He. Group normalization. arXiv:1803.08494, 2018.</a><a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Deep Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PointNet++:Aggregate Local Features with Sampling And Grouping</title>
    <link href="/2023/07/06/PointNet++/"/>
    <url>/2023/07/06/PointNet++/</url>
    
    <content type="html"><![CDATA[<h2 id="Review-of-PointNet"><a href="#Review-of-PointNet" class="headerlink" title="Review of PointNet"></a>Review of PointNet</h2><p>One of the most prominent shortcomings of PointNet is that PointNet does not capture local structures induced by the metric space points live in, because PointNet only extract features on single point, or use max pool to aggregate global points. This limits its ability to recognize fine-grained patterns and generalizability to complex scenes and has led to the poor performance of PointNet in segmentation, especially in partial segmentation scenarios.</p><h2 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet++"></a>PointNet++</h2><figure>    <img src="/img/blogs/230705-pointnet1+2/3.png" alt="Fig.1 Overrall of PointNet++">    <figcaption>Fig.1 Overrall of PointNet++</figcaption></figure><blockquote><p>Figure 2: Illustration of our hierarchical feature learning architecture and its application for set<br>segmentation and classification using points in 2D Euclidean space as an example. Single scale point<br>grouping is visualized here. For details on density adaptive grouping, see Fig. 3</p></blockquote><p>While PointNet uses a single max pooling operation to aggregate the whole point set, PointNet++ builds a hierarchical grouping of points and progressively abstract larger and larger local regions along the hierarchy based on CNNs. As show in Fig.1, the whole PointNeet++ can be divided into three parts, set abstraction, classification and segmentation. Here we mainly discuss set abstraction and segmentation (as classification is nearly the same as that in PointNet).</p><h3 id="Set-abstraction"><a href="#Set-abstraction" class="headerlink" title="Set abstraction"></a>Set abstraction</h3><p>The set abstraction level is made of three key layers: Sampling layer, Grouping layer and PointNet layer. The Sampling layer selects a set of points from input points use FPS, which defines the centroids of local regions. Grouping layer then constructs local region sets by finding “neighboring” points around the centroids. PointNet layer uses a mini-PointNet to encode local region patterns into feature vectors. A set abstraction level takes an $N\times(d + C)$ tensot as input that is from $N$ points with d-dim coordinates and C-dim point feature. It outputs an $N’\times (d + C’ )$ tensor of $N’$ subsampled points with d-dim coordinates and new $C’$-dim feature vectors summarizing local context.</p><p>Here’s the code of this layer:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PointNetSetAbstraction</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, npoint, radius, nsample, in_channel, mlp, group_all</span>):<br>        <span class="hljs-built_in">super</span>(PointNetSetAbstraction, self).__init__()<br>        self.npoint = npoint<br>        self.radius = radius<br>        self.nsample = nsample<br>        self.mlp_convs = nn.ModuleList()<br>        self.mlp_bns = nn.ModuleList()<br>        last_channel = in_channel<br>        <span class="hljs-keyword">for</span> out_channel <span class="hljs-keyword">in</span> mlp:<br>            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, <span class="hljs-number">1</span>))<br>            self.mlp_bns.append(nn.BatchNorm2d(out_channel))<br>            last_channel = out_channel<br>        self.group_all = group_all<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xyz, points</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Input:</span><br><span class="hljs-string">            xyz: input points position data, [B, C, N]</span><br><span class="hljs-string">            points: input points data, [B, D, N]</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            new_xyz: sampled points position data, [B, C, S]</span><br><span class="hljs-string">            new_points_concat: sample points feature data, [B, D&#x27;, S]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        xyz = xyz.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> points <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            points = points.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">if</span> self.group_all:<br>            new_xyz, new_points = sample_and_group_all(xyz, points)<br>        <span class="hljs-keyword">else</span>:<br>            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)<br>        <span class="hljs-comment"># new_xyz: sampled points position data, [B, npoint, C]</span><br>        <span class="hljs-comment"># new_points: sampled points data, [B, npoint, nsample, C+D]</span><br>        new_points = new_points.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [B, C+D, nsample,npoint]</span><br>        <span class="hljs-keyword">for</span> i, conv <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.mlp_convs):<br>            bn = self.mlp_bns[i]<br>            new_points =  F.relu(bn(conv(new_points)))<br><br>        new_points = torch.<span class="hljs-built_in">max</span>(new_points, <span class="hljs-number">2</span>)[<span class="hljs-number">0</span>]<br>        new_xyz = new_xyz.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> new_xyz, new_points<br></code></pre></td></tr></table></figure><h4 id="Grouping-layer"><a href="#Grouping-layer" class="headerlink" title="Grouping layer"></a>Grouping layer</h4><p>The input to this layer is a point set of size $N\times(d + C)$ and the coordinates of a set of centroids of size $N’\times d$. The output are groups of point sets of size $N’\times K \times (d + C)$, where each group corresponds to a local region and $K$ is the number of points sambled by ball query or kNN in the neighborhood of centroid points. Ball query finds all points that are within a radius to the query point (an upper limit of $K$ is set in implementation, if number of points is less than $K$, resample the centroid point). Compared with kNN, ball query’s local neighborhood guarantees a fixed region scale thus making local region feature more generalizable across space, which is preferred for tasks requiring local pattern recognition. $K$ varies across groups but the succeeding PointNet layer is able to convert flexible number of points into a fixed length local region feature vector.</p><p>Here’s the code of ball query:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">query_ball_point</span>(<span class="hljs-params">radius, nsample, xyz, new_xyz</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">        radius: local region radius</span><br><span class="hljs-string">        nsample: max sample number in local region</span><br><span class="hljs-string">        xyz: all points, [B, N, 3]</span><br><span class="hljs-string">        new_xyz: query points, [B, S, 3]</span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">        group_idx: grouped points index, [B, S, nsample]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    device = xyz.device<br>    B, N, C = xyz.shape<br>    _, S, _ = new_xyz.shape<br>    group_idx = torch.arange(N, dtype=torch.long).to(device).view(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, N).repeat([B, S, <span class="hljs-number">1</span>])<br>    sqrdists = square_distance(new_xyz, xyz)<br>    group_idx[sqrdists &gt; radius ** <span class="hljs-number">2</span>] = N<br>    group_idx = group_idx.sort(dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>][:, :, :nsample]<br>    group_first = group_idx[:, :, <span class="hljs-number">0</span>].view(B, S, <span class="hljs-number">1</span>).repeat([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, nsample])<br>    mask = group_idx == N<br>    group_idx[mask] = group_first[mask]<br>    <span class="hljs-keyword">return</span> group_idx<br></code></pre></td></tr></table></figure><h4 id="PointNet-layer"><a href="#PointNet-layer" class="headerlink" title="PointNet layer"></a>PointNet layer</h4><p>In this layer, the input are $N’$ local regions of points with data size $N’\times K \times (d + C)$. Each local region in the output is abstracted by its centroid and local feature that encodes the centroid’s neighborhood. Output data size is $N’\times K \times (d + C’)$. The coordinates of points in a local region are firstly translated into a local frame relative to the centroid point: $x^{(j)}_i &#x3D; x^{(j)}_i - \hat{x}^{(j)}$ for $i &#x3D; 1, 2, …, K$ and $j &#x3D; 1, 2, …, d$ where $\hat{x}$ is the coordinate of the centroid. By using relative coordinates together with point features we can capture point-to-point relations in the local region.</p><h4 id="MSG-and-MRG"><a href="#MSG-and-MRG" class="headerlink" title="MSG and MRG"></a>MSG and MRG</h4><figure>    <img src="/img/blogs/230705-pointnet1+2/4.png" alt="Fig.2 MSG and MRG" width=300>    <figcaption>Fig.2 MSG and MRG</figcaption></figure><blockquote><p>Figure 3: (a) Multi-scale cross-level adaptive scale selection grouping (MSG); (b) Multiresolution grouping (MRG).</p></blockquote><p>In the paper, the author conducted a comparative experiment to address the issue of PointNet’s poor performance on uneven point clouds. They used the original PointNet++ and found that its performance was not as good as PointNet in point clouds with uneven density. To improve the performance, PointNet++ introduces two solutions: Multi-Scale Grouping (MSG) and Multi-Resolution Grouping (MRG).</p><p>MSG uses multiple scales (radius) in each grouping layer to determine the range of the domain, and each range is extracted from the PointNet layer feature and then integrated to obtain a new multi-scale feature.But due to compute for serveral times, the MSG approach is computationally expensive.</p><p>Each feature of MRG consists of two parts: the features obtained by PointNet layer in the domain of this layer, and the features obtained by PointNet layer in the domain of the previous layer. When the point cloud density is uneven, different weights can be given to the left and right feature vectors by judging the point cloud density of the current patch. For example, when the density in the patch is too small, the points contained in the left eigenvector are more sparse, which is easily affected by undersampling, so the weight of the right eigenvector is increased.</p><figure>    <img src="/img/blogs/230705-pointnet1+2/5.png" alt="Fig.3 Exprements on MSG and MRG">    <figcaption>Fig.3 Exprements on MSG and MRG</figcaption></figure><blockquote><p>Figure 4: Left: Point cloud with random point dropout. Right: Curve showing advantage of our density adaptive strategy in dealing with non-uniform density. DP means random input dropout during training; otherwise training is on uniformly dense points. See Sec.3.3 for details.</p></blockquote><p>It can be seen that MSG and MRG have no improvement in classification accuracy compared with SSG (single-scale), but when the point cloud is very sparse, using MSG can maintain good robustness. Random input dropout (DP) also greatly improves the robustness.</p><h3 id="Segmentation"><a href="#Segmentation" class="headerlink" title="Segmentation"></a>Segmentation</h3><p>PointNet++ adopts a hierarchical propagation strategy with distance based interpolation and across level skip links. In a feature propagation level, PointNet++ propagates point features from $N_l \times (d + C)$ points to $N_{l-1}$ points where $N_{l-1}$ and $N_l$ (with $N_l \leq N_{l-1}$) are point set size of input and output of set abstraction level $l$. PointNet++ achieves feature propagation by interpolating feature values $f$ of $N_l$ points at coordinates of the $N_{l-1}$ points. Among the many choices for interpolation, PointNet++ uses inverse distance weighted average based on k nearest neighbors (as in following equation, in default $p &#x3D; 2, k &#x3D; 3$). The interpolated features on $N_{l-1}$ points are then concatenated with skip linked point features from the set abstraction level. Then the concatenated features are passed through a “unit pointnet”, which is similar to one-by-one convolution in CNNs. A few shared fully connected and ReLU layers are applied to update each point’s feature vector. The process is repeated until we have propagated features to the original set of points.<br>$$<br>f^{(j)}(x)&#x3D;\frac{\sum_{i&#x3D;1}^kw_i(x)f_i^{(i)}}{\sum_{i&#x3D;1}^kw_i(x)}\quad\text{where}\quad w_i(x)&#x3D;\frac{1}{d(x, x_i)^p}, j&#x3D;1,2,…,C<br>$$</p><p>Here’s the code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PointNetFeaturePropagation</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channel, mlp</span>):<br>        <span class="hljs-built_in">super</span>(PointNetFeaturePropagation, self).__init__()<br>        self.mlp_convs = nn.ModuleList()<br>        self.mlp_bns = nn.ModuleList()<br>        last_channel = in_channel<br>        <span class="hljs-keyword">for</span> out_channel <span class="hljs-keyword">in</span> mlp:<br>            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, <span class="hljs-number">1</span>))<br>            self.mlp_bns.append(nn.BatchNorm1d(out_channel))<br>            last_channel = out_channel<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, xyz1, xyz2, points1, points2</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Input:</span><br><span class="hljs-string">            xyz1: input points position data, [B, C, N]</span><br><span class="hljs-string">            xyz2: sampled input points position data, [B, C, S]</span><br><span class="hljs-string">            points1: input points data, [B, D, N]</span><br><span class="hljs-string">            points2: input points data, [B, D, S]</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            new_points: upsampled points data, [B, D&#x27;, N]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        xyz1 = xyz1.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        xyz2 = xyz2.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>        points2 = points2.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        B, N, C = xyz1.shape<br>        _, S, _ = xyz2.shape<br><br>        <span class="hljs-keyword">if</span> S == <span class="hljs-number">1</span>:<br>            interpolated_points = points2.repeat(<span class="hljs-number">1</span>, N, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            dists = square_distance(xyz1, xyz2)<br>            dists, idx = dists.sort(dim=-<span class="hljs-number">1</span>)<br>            dists, idx = dists[:, :, :<span class="hljs-number">3</span>], idx[:, :, :<span class="hljs-number">3</span>]  <span class="hljs-comment"># [B, N, 3]</span><br><br>            dist_recip = <span class="hljs-number">1.0</span> / (dists + <span class="hljs-number">1e-8</span>)<br>            norm = torch.<span class="hljs-built_in">sum</span>(dist_recip, dim=<span class="hljs-number">2</span>, keepdim=<span class="hljs-literal">True</span>)<br>            weight = dist_recip / norm<br>            interpolated_points = torch.<span class="hljs-built_in">sum</span>(index_points(points2, idx) * weight.view(B, N, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>), dim=<span class="hljs-number">2</span>)<br><br>        <span class="hljs-keyword">if</span> points1 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            points1 = points1.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>            new_points = torch.cat([points1, interpolated_points], dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            new_points = interpolated_points<br><br>        new_points = new_points.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">for</span> i, conv <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.mlp_convs):<br>            bn = self.mlp_bns[i]<br>            new_points = F.relu(bn(conv(new_points)))<br>        <span class="hljs-keyword">return</span> new_points<br></code></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/1706.02413">Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NIPS, 2017.</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch">Pytorch_Pointnet_Pointnet2</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Computer Vision</tag>
      
      <tag>Point Cloud</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PointNet:The Pioneer of Point Cloud Deep Learning</title>
    <link href="/2023/07/05/PointNet/"/>
    <url>/2023/07/05/PointNet/</url>
    
    <content type="html"><![CDATA[<h2 id="Point-Cloud-Data"><a href="#Point-Cloud-Data" class="headerlink" title="Point Cloud Data"></a>Point Cloud Data</h2><p>Point cloud data refers to a set of vectors in a 3D coordinate system. A normal point cloud object is usually with 2D shape <code>(n, 3+X)</code>, where n is the number of points, 3 stands for 3D coordinates and X contains other features like color and orientation. </p><figure>    <img src="/img/blogs/230705-pointnet1+2/2.png" alt="Fig.1 Point Cloud Data Visualization" width=300>    <figcaption>Fig.1 Point Cloud Data Visualization</figcaption></figure><p>Point cloud data has the following characteristics:</p><ul><li>Permutation Invariance: A point cloud object with $n$ points can be permuted in $n!$ different orders, but the point set always represents the same object no matter how they are permuted.</li><li>Transformation Invariance: Classification and segmentation outputs should be unchanged if the object undergoes rigid transformations.</li><li>Interaction among points: The points are from a space with a distance metric which means that points are not isolated.</li></ul><p>To use point cloud data, Volumetric CNNs transform point data into volumes, but is constrained by its resolution due to data sparsity and computation cost of 3D convolution; Multiview CNNs try to render 3D point cloud or shapes into 2D images and apply 2D conv nets to<br>classify them, which is nontrivial to extend them to scene understanding or other 3D tasks such as point classification and shape completion.</p><h2 id="PointNet"><a href="#PointNet" class="headerlink" title="PointNet"></a>PointNet</h2><p>PointNet was first proposed in 2016 and is the first to directly use point cloud data for deep learning.It takes raw point cloud data as input for classification and segmentation.</p><h3 id="PointNet-Architecture"><a href="#PointNet-Architecture" class="headerlink" title="PointNet Architecture"></a>PointNet Architecture</h3><figure>    <img src="/img/blogs/230705-pointnet1+2/1.png" alt="Fig.2 Overrall of PointNet">    <figcaption>Fig.2 Overrall of PointNet</figcaption></figure><blockquote><p>The classification network takes n points as input, applies input and feature transformations, and then aggregates point features by max pooling. The output is classification scores for k classes. The segmentation network is an extension to the classification net. It concatenates global and local features and outputs per point scores. “mlp” stands for multi-layer perceptron, numbers in bracket are layer sizes. Batchnorm is used for all layers with ReLU. Dropout layers are used for the last mlp in classification net.</p></blockquote><p>By analyzing the architecture of PointNet above, we can divide PointNet into 2 stages: input &amp; feature transform and certain task output. Noticeably, for each point cloud object, the number of points is usually different, so we need to sample a fixed number of points at first.Here we use a method called Farthest Point Sampling (FPS), which iteratively samples the farthest point and performs distance updating. </p><p>Given input points ${x_1, x_2, …, x_n}$, FPS choose a subset of points ${x_{i_1} , x_{i_2} , …, x_{i_m}}$, such that $x_{i_j}$ is the most distant point (in metric distance) from the set ${x_{i_1} , x_{i_2} , …, x_{i_{j-1}}}$ with regard to the rest points. Compared with random sampling, it has better coverage of the entire point set given the same number of centroids. FPS sampling strategy generates receptive fields in a data dependent manner.</p><p>Here’s the code of FPS:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">farthest_point_sample</span>(<span class="hljs-params">point, npoint</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Input:</span><br><span class="hljs-string">        xyz: pointcloud data, [N, D]</span><br><span class="hljs-string">        npoint: number of samples</span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">        centroids: sampled pointcloud index, [npoint, D]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    N, D = point.shape<br>    xyz = point[:,:<span class="hljs-number">3</span>]<br>    centroids = np.zeros((npoint,))<br>    distance = np.ones((N,)) * <span class="hljs-number">1e10</span><br>    farthest = np.random.randint(<span class="hljs-number">0</span>, N)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(npoint):<br>        centroids[i] = farthest<br>        centroid = xyz[farthest, :]<br>        dist = np.<span class="hljs-built_in">sum</span>((xyz - centroid) ** <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>)<br>        mask = dist &lt; distance<br>        distance[mask] = dist[mask]<br>        farthest = np.argmax(distance, -<span class="hljs-number">1</span>)<br>    point = point[centroids.astype(np.int32)]<br>    <span class="hljs-keyword">return</span> point<br></code></pre></td></tr></table></figure><h3 id="Task-output"><a href="#Task-output" class="headerlink" title="Task output"></a>Task output</h3><p>In order to analyze how to deal with permutation invariance more intuitively, we first discuss. As show in Fig.2, classfication head takes the output of feature transform with shape <code>(n, 64)</code> and the feature dimension is increased to 1024 by a mlp. After that, a max pool is excuted on dimension of number of points and we get global feature for each point with the length of 1024. Due to characteristics of max pool, the feature is permutation invariant. For segmantation, global feature is repeated by n times and concatenated with local feature (output of feature transform) to get point-wise feature.</p><p>Here’s the main code of this stage:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PointNetEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, global_feat=<span class="hljs-literal">True</span>, feature_transform=<span class="hljs-literal">False</span>, channel=<span class="hljs-number">3</span></span>):<br>        <span class="hljs-built_in">super</span>(PointNetEncoder, self).__init__()<br>        self.stn = STN3d(channel)<br>        self.conv1 = torch.nn.Conv1d(channel, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>)<br>        self.conv2 = torch.nn.Conv1d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>)<br>        self.conv3 = torch.nn.Conv1d(<span class="hljs-number">128</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">1</span>)<br>        self.bn1 = nn.BatchNorm1d(<span class="hljs-number">64</span>)<br>        self.bn2 = nn.BatchNorm1d(<span class="hljs-number">128</span>)<br>        self.bn3 = nn.BatchNorm1d(<span class="hljs-number">1024</span>)<br>        self.global_feat = global_feat<br>        self.feature_transform = feature_transform<br>        <span class="hljs-keyword">if</span> self.feature_transform:<br>            self.fstn = STNkd(k=<span class="hljs-number">64</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        B, D, N = x.size()<br>        trans = self.stn(x)<br>        x = x.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> D &gt; <span class="hljs-number">3</span>:<br>            feature = x[:, :, <span class="hljs-number">3</span>:]<br>            x = x[:, :, :<span class="hljs-number">3</span>]<br>        x = torch.bmm(x, trans)<br>        <span class="hljs-keyword">if</span> D &gt; <span class="hljs-number">3</span>:<br>            x = torch.cat([x, feature], dim=<span class="hljs-number">2</span>)<br>        x = x.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        x = F.relu(self.bn1(self.conv1(x)))<br><br>        <span class="hljs-keyword">if</span> self.feature_transform:<br>            trans_feat = self.fstn(x)<br>            x = x.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>            x = torch.bmm(x, trans_feat)<br>            x = x.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            trans_feat = <span class="hljs-literal">None</span><br><br>        pointfeat = x<br>        x = F.relu(self.bn2(self.conv2(x)))<br>        x = self.bn3(self.conv3(x))<br>        x = torch.<span class="hljs-built_in">max</span>(x, <span class="hljs-number">2</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>)<br>        <span class="hljs-keyword">if</span> self.global_feat:<br>            <span class="hljs-keyword">return</span> x, trans, trans_feat<br>        <span class="hljs-keyword">else</span>:<br>            x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, N)<br>            <span class="hljs-keyword">return</span> torch.cat([x, pointfeat], <span class="hljs-number">1</span>), trans, trans_feat<br></code></pre></td></tr></table></figure><p>Where stn will be explained in next section. </p><h3 id="Input-amp-feature-transform"><a href="#Input-amp-feature-transform" class="headerlink" title="Input &amp; feature transform"></a>Input &amp; feature transform</h3><p>For input transform, the PointNet model takes input with shape of <code>(n, 3)</code> and use a learnable T-Net to get an affine transformation matrix, then multiply the matrix and point cloud data. This stage is mainly to align the points to deal with the transformation invariance.</p><p>Here’s the code of T-Net (STN3d is similar):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">STNkd</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, k=<span class="hljs-number">64</span></span>):<br>        <span class="hljs-built_in">super</span>(STNkd, self).__init__()<br>        self.conv1 = torch.nn.Conv1d(k, <span class="hljs-number">64</span>, <span class="hljs-number">1</span>)<br>        self.conv2 = torch.nn.Conv1d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">1</span>)<br>        self.conv3 = torch.nn.Conv1d(<span class="hljs-number">128</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">1</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>)<br>        self.fc3 = nn.Linear(<span class="hljs-number">256</span>, k * k)<br>        self.relu = nn.ReLU()<br><br>        self.bn1 = nn.BatchNorm1d(<span class="hljs-number">64</span>)<br>        self.bn2 = nn.BatchNorm1d(<span class="hljs-number">128</span>)<br>        self.bn3 = nn.BatchNorm1d(<span class="hljs-number">1024</span>)<br>        self.bn4 = nn.BatchNorm1d(<span class="hljs-number">512</span>)<br>        self.bn5 = nn.BatchNorm1d(<span class="hljs-number">256</span>)<br><br>        self.k = k<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        batchsize = x.size()[<span class="hljs-number">0</span>]<br>        x = F.relu(self.bn1(self.conv1(x)))<br>        x = F.relu(self.bn2(self.conv2(x)))<br>        x = F.relu(self.bn3(self.conv3(x)))<br>        x = torch.<span class="hljs-built_in">max</span>(x, <span class="hljs-number">2</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>)<br><br>        x = F.relu(self.bn4(self.fc1(x)))<br>        x = F.relu(self.bn5(self.fc2(x)))<br>        x = self.fc3(x)<br><br>        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(<span class="hljs-number">1</span>, self.k * self.k).repeat(<br>            batchsize, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> x.is_cuda:<br>            iden = iden.cuda()<br>        x = x + iden<br>        x = x.view(-<span class="hljs-number">1</span>, self.k, self.k)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>It is not difficult to find that T-Net also obtains permutation invariant affine transformation matrix through max pool. Transformation matrix in the feature space has much higher dimension than the input transform matrix, which greatly increases the difficulty of optimization, so the authors add a regularization term $L_{reg}&#x3D;||I - AA^T||^2_F$ to make the transformation matrix A of the feature space as close to the orthogonal matrix as possible.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span><a href="https://arxiv.org/abs/1612.00593">C. Qi et al, “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation”, 2017</a><a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span><a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch">Pytorch_Pointnet_Pointnet2</a><a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Papers</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Computer Vision</tag>
      
      <tag>Point Cloud</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello, Hexo</title>
    <link href="/2023/06/30/hello-world/"/>
    <url>/2023/06/30/hello-world/</url>
    
    <content type="html"><![CDATA[<h3 id="This-is-my-new-blog-The-previous-one-has-been-outdated"><a href="#This-is-my-new-blog-The-previous-one-has-been-outdated" class="headerlink" title="This is my new blog. The previous one has been outdated."></a>This is my new blog. The previous one has been outdated.</h3>]]></content>
    
    
    <categories>
      
      <category>Others</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
